[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Online experiments",
    "section": "",
    "text": "Welcome\nThe aim of this book is to collect all I know about online experiments. I do this mainly for personal reference because I forget stuff. But if you find the notes helpful, find any errors, or have any suggestions, please get in touch by writing to fa.gunzinger@gmail.com.\nMy aim is to capture the material I need as concisely as I can. There are many excellent resources out there that explain concepts more elaborately (e.g. here, here).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Why causal inference?\nThe purpose:\nComparison to other types of data analysis\nCausal inference vs prediction",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#why-causal-inference",
    "href": "chapters/introduction.html#why-causal-inference",
    "title": "1  Introduction",
    "section": "",
    "text": "Because many things we think work don’t (look at list of refuted claims from Kohavi here)\n\n\n\nCreate a valid counterfactual outcome - the outcome of the treated if they had not been treated - in order to eliminate selection bias when estimating the treatment effect.\nMilton Friedman thermostat example\n\n\n\nDescriptive analysis describes the data. We simply turn data into meaningfull summary measures presented in tables or – better, usually! – figures. The aim here is to simply describe the data as it is, highlighting aspects that are particularly interesting or relevant to the task at hand.\nPredictive analysis predicts unobserved metric values based on observed ones. We build a model that captures the data generating process of the metric we aim to predict based on training data we observe, so that we can then predict metric values we don’t observe.\nCausal inference makes statements about what would happen to outcomes if we changed the world in a particular way.\n\n\n\nPrediction is about finding the most likely outcome based on a set of (existing) covariates. Causal inference is about finding the effect of a change in a covariate on the outcome.\nThe difference is profound: when predicting, you take the features as given and predict outcomes based on them – you’re asking: “given existing features, what outcome can I expect?”. When you perform causal inference, you want to know what would happen if you were to change one of the covariates – you’re asking “if I were to change one covariate in a certain way, what outcome could I expect?”. (In prediction, different units have different covariate values. In causal inference, we ask what would happen if we changed covariate values for some or all units.)\nCausal inference is about manipulating covariates – to paraphrase Donald Rubin: there is no causality without manipulation.\nTechnically, what this really comes down to is that in prediction, you don’t care about selection bias, whereas in causal inference that’s the main thing you care about.\nThis also means that the role of goodness of fit is very different: for prediction, it’s obviously very important – if your model explains only a very small part of the variation in the outcome, it won’t be very good at predicting outcomes. In causal inference, goodness of fit doesn’t matter because your aim is not to predict, but to know how the outcome changes if you change a covariate. So, you can have very low goodness of fit (lots of things outside the model predicting outcomes), but if you can precisely estimate your treatment effect, that’s very valuable (you learn that regardless of all the many other factors that determine the outcome, changing a covariate in a certain way tends to change outcomes in a certain way.)\nIn a business context, they capture two sets of problems:\n\nPrediction is about making statements in domains where the future is expected to be similar to the past (i.e. revenue given sales)\nCausal inference is about answering “what if” questions (what happens to sales if we change X)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#why-experiments",
    "href": "chapters/introduction.html#why-experiments",
    "title": "1  Introduction",
    "section": "1.2 Why experiments",
    "text": "1.2 Why experiments\nWe have built a new feature for our platform and want to know whether the platform is the better or worse for it. How can we know?\nFirst, let’s make the question more precise. What we care about is a comparison of two states of the world. We want a comparison of the state of the world with and without our new feature. So, how do we do that?\nIdeally, we’d clone the world and release the feature in one world and not the other, keep everything else the same, and compare outcomes. We can’t do that. What can we do?\nWe could compare outcomes from before and after the release of the feature. If nothing has changed in the world but the release of our feature, then this gives us the same result as our ideal scenario above. But nothing never changes in the world. If we find that outcomes after the release differ from outcomes from before we can never know whether that is because of our feature or whether the observed difference would have occurred regardless – because of a change in season, a big piece of world news, a change in customer preferences, or any of many other possible reasons or a mix thereof. What else can we do?\nWe can show the feature to one group of customers and withhold it from another group and compare outcomes between these two groups. If nothing but exposure to the feature is different between the two groups then this approach, too, gives us the same answer as our infeasible ideal approach above. Whether or not there is a difference between the two groups of customers depends on how we select customers for those groups: do we put high-value customers in one group and occasional customers in another? Or customers from one region in one and customers from another region in the other group? All such manual approaches raise concerns – some more obvious than others. An experiment is the best tool we have to maximise our chance that the two groups are, indeed the same: the randomisation of treatment eliminates selection bias, and the presence of a control group provides a counterfactual of what would have happened without treatment.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#is-experimentation-worth-it-in-business",
    "href": "chapters/introduction.html#is-experimentation-worth-it-in-business",
    "title": "1  Introduction",
    "section": "1.3 Is experimentation worth it in business?",
    "text": "1.3 Is experimentation worth it in business?\n\nIn some business organisations there is a prevailing wisdom that “science” often in the form of A/B testing (which is the digital version of controlled experiments) makes making decisions harder by slowing them down, which wastes important time.\nOne of the main jobs of an experimentation specialist in an organisation is to help people understand why running experiments is crucial. In this space, I want to gather my evolving thoughts on this topic with the aim of crafting an increasingly convincing argument.\n\nRelevant thoughts:\n\nEpistemic argument about how we know what works: human behaviour is heterogeneous and largely unpredictable, so to know what works you need to test things, and the easiest (certainly for most tech companies) and most rigorous way to test things is to run experiments.\nExperiments do not slow decision making down, but are one of the most powerful tools we have to ensure that the decisions we make are good ones.\nDiscuss when not to experiment (for there clearly are cases where you either don’t have the time or where there really is no need).\nAndrew Gelman argues (in this blog post) that i) “to reduce, control, or adjust for bias and variation in measurements” and ii) “to systematically gather data on multiple cases” are the two most important parts of statistics. This is a good way of emphasising why running experiments in a standardised way are important even if you feel you have data about your feature: it ensures that you collect data from a representative sample of users and it ensures you are judging the success of your feature by pre-defined metrics defined in a standardised way (at least ideally!).\nHow do we define identity of executives or product managers: not around knowing what is true or what works or what customers want, but around coming up with good hypotheses, being willing to learn from data, and strong at iterating on features based on data and past experience.\n\nIs experimentation worth it?\n\nKing (2020) on how Pinterest was able to build a machine-learning system that is 20% better at detecting policy-violating content thanks to incremental testing that experimentation allows.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#interpretation",
    "href": "chapters/introduction.html#interpretation",
    "title": "1  Introduction",
    "section": "1.4 Interpretation",
    "text": "1.4 Interpretation\n\nWe often think of experiment as answering: “Does the intervention work?”, but what it really answers is “What is the average effect of the intervention with this specific implementation for this particular population at this time?”\nWhen we have multiple treatments or analyse subgroups separately, we need to test whether different estimates are significantly different from each other (can’t just look at whether some are significantly different from zero while some aren’t)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#conventions-used-throughout",
    "href": "chapters/introduction.html#conventions-used-throughout",
    "title": "1  Introduction",
    "section": "1.5 Conventions used throughout",
    "text": "1.5 Conventions used throughout\n\nI use users instead of the more generic units\nI discuss the case with one treatment and one control group.\n\n\n\n\n\nKing, Jeremy. 2020. “The Power of These Techniques Is Only Getting Stronger.” Harvard Business Review. HARVARD BUSINESS SCHOOL PUBLISHING CORPORATION 300 NORTH BEACON STREET ….",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/statistics.html",
    "href": "chapters/statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "A/B tests are used throughout industry to make decisions that allocate millions of dollars and that affect millions of customers.\nMy goal is to provide a complete overview of the statistical theory on which all this decision-making rests.",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html",
    "href": "chapters/stats_of_online_experiments.html",
    "title": "1  The stats of online experiments",
    "section": "",
    "text": "1.1 Setup\nWe study a sample of \\(n\\) units, indexed by \\(i = 1, \\dots, n\\), to learn about the effect of a binary treatment on these units.1 The sample of units might be all visitors to an e-commerce app and the treatment a new UX feature. The treatment is “binary” because we only consider two treatment conditions: a unit either experiences the active treatment and is exposed to the new feature or experiences the control treatment and is exposed to the status-quo. We often refer to the two treatment conditions simply as “treatment” and “control”.\nEach unit has two potential outcomes: \\(Y_i(1)\\) is the outcome for unit \\(i\\) if they are in treatment and \\(Y_i(0)\\) is the outcome if they are in control. To simplify notation, we collect all unit-level potential outcomes in the \\(n \\times 1\\) vectors \\(\\mathbf{Y(1)}\\) and \\(\\mathbf{Y(0)}\\). These outcomes are “potential outcomes” because before the start of the experiment, each unit could be exposed to either treatment condition so that they can potentially experience either outcome. Once the experiment has started and units are assigned to treatment, only one of the two outcomes will be observed.\nThe causal effect of the treatment for unit \\(i\\) is the difference between the two potential outcomes:2 \\[\n\\tau_i = Y_i(1) - Y_i(0).\n\\] Because a unit can only ever be in either treatment or control, we can only ever observe one of the two potential outcomes, which means that directly observing unit-level treatment effects is impossible. This is the fundamental problem of causal inference (Holland 1986).\nAn experiment is one solution to the fundamental problem:3 randomly assigning units from a population to either treatment or control allows us to estimate average (unit-level) treatment effects. In the words of Holland (1986, 947):4\nHence, instead of trying to observe unit-level causal effects, the quantity of interest – the estimand – in an experiment is an average across a sample of units. In particular, we are usually interested in the effect of a universal policy, a comparison between a state of the world where everyone is exposed to the treatment and one where nobody is. While we can capture the difference between these two states of the world in many different ways, we typically focus on the difference in the averages of all these unit-level causal effects over the entire sample:\n\\[\n\\begin{align}\n\\tau\n= \\frac{1}{n}\\sum_{i=1}^n \\left(Y_i(1) - Y_i(0)\\right)\n= \\frac{1}{n}\\sum_{i=1}^n Y_i(1) - \\frac{1}{n}\\sum_{i=1}^nY_i(0).\n\\end{align}\n\\tag{1.1}\\]\nThis is the estimand, the statistical quantity we are trying to estimate in our experiment.\nRunning an experiment with our \\(n\\) units means that we randomly assign some units to treatment and some to control. We use the binary treatment indicator \\(W_i \\in \\{0, 1\\}\\) to indicate treatment exposure for unit \\(i\\) and write \\(W_i = 1\\) if they are in treatment and \\(W_i = 0\\) if they are in control. We collect all unit-level treatment indicators in the \\(n \\times 1\\) vector \\(\\mathbf{W} = (W_1, W_2, \\dots, W_n)'\\). At the end of the experiment, we have \\(n_t = \\sum_{i=1}^n W_i\\) units in treatment and the remaining \\(n_c = \\sum_{i=1}^n (1-W_i)\\) units in control. For each unit, we observe outcome \\(Y_i\\).\nTo estimate \\(\\tau\\), we use the observed difference in means between the treatment and control units:5\n\\[\n\\begin{align}\n\\hat{\\tau}^{\\text{dm}}\n=\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i - \\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\end{align}\n\\]\nThis is our estimator, the algorithm we use to produce estimates of the estimand.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#setup",
    "href": "chapters/stats_of_online_experiments.html#setup",
    "title": "1  The stats of online experiments",
    "section": "",
    "text": "“The important point is that [an experiment] replaces the impossible-to-observe causal effect of [a treatment] on a specific unit with the possible-to-estimate average causal effect of [the treatment] over a population of units.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#assignment-mechanism",
    "href": "chapters/stats_of_online_experiments.html#assignment-mechanism",
    "title": "1  The stats of online experiments",
    "section": "1.2 Assignment mechanism",
    "text": "1.2 Assignment mechanism\nThe procedure we use the allocate units to treatment conditions is the assignment mechanism. In online experiments, we typically assign units to treatment conditions dynamically as they visit our site and use an assignment mechanism where the assignment of each unit is determined by a process that is equivalent to a coin-toss, such that \\(P(W_i) = q\\), where \\(q \\in [0, 1]\\). Throughout, I’ll focus on the most common case where \\(q=\\frac{1}{2}\\), so that we have:\n\\[\nP(W_i = 1) = P(W_i = 0) = \\frac{1}{2}.\n\\] Because of their coin-toss-like nature assignments follow a Bernoulli distribution and the type of experiment is called a Bernoulli Randomised Experiment. Formally, we have: \\[\n\\begin{align}\nW_i &\\sim \\text{Bernoulli}(1/2) \\\\\n\\mathbb{E}[{W_i}] &= 1/2 \\\\\n\\end{align}\n\\] and \\[\n\\begin{align}\nn_t &\\sim \\text{Binomial}(n, 1/2) \\\\\n\\mathbb{E}[{n_t}] &= n(1/2).\n\\end{align}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#analysis",
    "href": "chapters/stats_of_online_experiments.html#analysis",
    "title": "1  The stats of online experiments",
    "section": "1.3 Analysis",
    "text": "1.3 Analysis\nThere are different approaches we could take to formally analyse our experiment.\nDecisions:\n\nWe could either take a superpopulation or fixed sample perspective. Because for all but the largest companies, most online experiments are eventually run on the entire population of interest, I focus on the latter. This means that the goal of our experiment is to estimate the average treatment effect of the treatment on our \\(n\\) units, rather than using the estimate for our \\(n\\) units to infer the average treatment effect on a larger population from which the \\(n\\) units are drawn. I thus use a fully design-based approach (see ?sec-experiment-setup for details)\nWe can analyse the Bernoulli Randomised Experiment treating \\(n_t\\) as a Binomial random variable or taking it as given. Because by the time of the analysis it is, in fact, given, I follow the latter approach. This approach also has the advantage of considerably simplifying the math.\n\nImplications:\n\nOur sample of \\(n\\) units and the associated potential outcomes \\(\\mathbf{Y(1)}\\) and \\(\\mathbf{Y(0)}\\) are fixed (because units are non-random but determined by sample I have). I refer to the potential outcomes collectively as \\(\\mathbf{Y(w)} = (\\mathbf{Y(1)}, \\mathbf{Y(0)})\\).\nOnce randomisation is complete the number of units in treatment and control, \\(n_t\\) and \\(n_c\\) are given. I refer to them collectively as \\(\\mathbf{n} = (n_t, n_c)\\).\nThe assignment mechanism is also such that units treatment assignment is independent of the treatment assignment of all other units.\n\nIn the next two sections we show that \\(\\hat{\\tau}^{\\text{dm}}\\) is an unbiased estimator of \\(\\tau\\) and calculate its variance. (My approach is based on Ding (2023). For an alternative, see Appendix 6.B. in Imbens and Rubin (2015).)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#unbiasedness",
    "href": "chapters/stats_of_online_experiments.html#unbiasedness",
    "title": "1  The stats of online experiments",
    "section": "1.4 Unbiasedness",
    "text": "1.4 Unbiasedness\nAn estimator is unbiased if its expected value equals the estimand. To show that the difference in means estimator is unbiased we thus have to show that:\n$$ \\[\\begin{align}\n\\mathbb{E}\\left[\n\\hat{\\tau}^{\\text{dm}}\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\n&=\n\\tau.\n\\end{align}\\] $$ Given our definitions above this is means showing that\n$$ \\[\\begin{align}\n\\mathbb{E}\\left[\n\\hat{\\tau}^{\\text{dm}}\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\n&=\n\\mathbb{E}\\left[\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i - \\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\\\\[5pt]\n\n&=\n\\mathbb{E}\\left[\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n- \\mathbb{E}\\left[\n\\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\\\\[5pt]\n\n&\n\\enspace\n\\overset{!}{=}\n\\enspace\n\\frac{1}{n}\\sum_{i=1}^nY_i(1) - \\frac{1}{n}\\sum_{i=1}^nY_i(0)\n\\\\[5pt]\n\n&=\n\\tau\n\\end{align}\\] $$\nThere are two pieces we need for this:\n\nLink observed to potential outcomes so that \\(Y_i = Y_i(W_i)\\). This requires the Stable Unit Treatment Value Assumption (SUTVA).\nLink treatment group averages to sample averages so that \\(\\mathbb{E}\\left[\\frac{1}{n_t}\\sum_{W_i=w}Y_i(w)\\right] = \\frac{1}{n}\\sum_{i=1}^{n}Y_i(w)\\). This requires randomisation.\n\nLet’s tackle them in turn.\n\n1.4.1 SUTVA links observed outcomes to potential outcomes\nWe want to learn something about unit-level differences in potential outcomes \\[\n\\tau_i = Y_i(1) - Y_i(0),\n\\] where a potential outcome is a unit-indexed function of the treatment level, with the treatment levels in our case simply being “treatment” and “control”, captured by the assignment indicator \\(W_i \\in {0, 1}\\).\nNow say we run an experiment and unit \\(i\\) is allocated to treatment. The \\(i\\)-th element of the assignment vector \\(\\mathbf{W}\\) of that experiment is thus \\(W_i = 1\\), and we refer to this assignment vector as \\(\\mathbf{W}^{(i=1)}\\).\nWe know we can’t observe both \\(Y_i(0)\\) and \\(Y_i(1)\\), but given that \\(i\\) is in treatment we can observe the potential outcome under treatment, \\(Y_i(1)\\), which will help us estimate \\(\\hat{\\tau}\\). So we need\n\\[\nY_i = Y_i(1).\n\\]\nWhat we directly observe, however, is \\[\nY_i = Y_i\\left(\\mathbf{W}^{(i=1)}\\right).\n\\] In words: the outcome we observe for unit \\(i\\) in our experiment is not the potential outcome for \\(i\\) under treatment, but the potential outcome for \\(i\\) under the specific assignment vector of our experiment, \\(\\mathbf{W}^{(i=1)}\\). What does this mean concretely? It means that the observed outcome is a function not only of \\(i\\)’s treatment assignment but of:\n\nThe assignment of all units in the experiment\nThe precise form of the assigned treatment level received by \\(i\\)\nThe way in which said assigned treatment level is administered to \\(i\\)\n\nWe need: \\[\nY_i = Y_i\\left(\\mathbf{W}^{(i=1)}\\right) \\overset{!}{=} Y_i(1).\n\\] The only way to make progress is to assume what we need: that potential outcomes for unit \\(i\\) are a function only of the treatment level unit \\(i\\) itself receives and independent of (i) treatment assignment of other units and (ii) the form and administration of the treatment level. (i) above is referred to as “no interference” in the literature, (ii) as “no hidden variations of treatment”.\nThat assumption is SUTVA, the Stable Unit Treatment Value Assumption. It ensures that the potential outcomes for each unit and each treatment level are well-defined functions of the unit index and the treatment level only – that for a given unit and treatment level, the potential outcome is well-defined and, thus, “stable” (Rubin 1980).\nNote that (ii) does not mean that a treatment level has to take the same form for all units, even though it is often misinterpreted to mean that. What we need is that \\(Y_i(W_i)\\) is a clearly defined function for all \\(i\\) and all possible treatment levels \\(W_i \\in \\{0, 1\\}\\). For this to be the case, it must be the case that there are no different forms of possible treatments, the potential outcome for each for is the same so that the differences are irrelevant, or that the experienced form is random so that the expected outcome across all units remains stable.\nIn the context of our e-commerce app experiment, the non-interference part of SUTVA means that potential outcomes for all customers are independent of treatment assignment of all other customers – very much including family members and friends and the no-hidden-variation part means that there their precise experience is pinned down by their mobile device and remains stable over time: there are no accidental server-side bugs that creates different background colours and the it is either clear whether a customer uses an Android or iOS app (or the experience is identical). But, again, SUTVA does not require that the active treatment looks exactly the same on iOS and Android apps just that for each unit in our experiment, their experience if they are part of the active treatment is pinned down.\nWhy is this important? We want to know what happened if we rolled out our policy to everyone compared to if we didn’t roll it out to anyone. To have any hope of estimating this we can’t have treatment level’s vary over time or depending on circumstances, but need them to be pinned down for each unit. (In the context of Tech, this would mean that the experience of a feature for a given user is pinned down by, say, the size of their phone screen and the app version they use, which, by and large, is plausible.)\nSUTVA is a strong assumption and can be violated in a number of ways. I’ll discuss these, together with solutions, in ?sec-threats-to-validity.\nIf SUTVA holds we have:\n\\[\nY_i = Y_i(W_i) = \\begin{cases}\n   Y_i(1) & \\text{if } W_i = 1 \\\\\n   Y_i(0)       & \\text{if } W_i = 0,\n  \\end{cases}\n\\] or, more compactly: \\[\nY_i = W_iY_i(1) + (1 - W_i)Y_i(0).\n\\]\nThis is the link between observed and potential outcomes we need, and makes clear that we observe \\(Y_i(1)\\) for units in treatment and \\(Y_i(0)\\) for units in control.\nIn our unbiasedness proof, we now have $$ \\[\\begin{align}\n\\mathbb{E}\\left[\n\\hat{\\tau}^{\\text{dm}}\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\n&=\n\\mathbb{E}\\left[\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i - \\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\\\\[5pt]\n\n&=\n\\mathbb{E}\\left[\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n- \\mathbb{E}\\left[\n\\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\\\\[5pt]\n\n&\\text{SUTVA}\n\\\\[5pt]\n\n&=\n\\mathbb{E}\\left[\\frac{1}{n_t}\\sum_{W_i=1}Y_i(1)\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\\right]\n- \\mathbb{E}\\left[\\frac{1}{n_c}\\sum_{W_i=0}Y_i(0)\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\\right]\n\\\\[5pt]\n\n&\n\\enspace\n\\overset{!}{=}\n\\enspace\n\\frac{1}{n}\\sum_{i=1}^nY_i(1) - \\frac{1}{n}\\sum_{i=1}^nY_i(0)\n\\\\[5pt]\n\n&=\n\\tau\n\\end{align}\\] $$\nWhat remains is to show that \\(\\mathbb{E}\\left[\\frac{1}{n_t}\\sum_{W_i=w}Y_i(w)\\right] = \\frac{1}{n}\\sum_{i=1}^{n}Y_i(w)\\). This requires randomisation.\n\n\n1.4.2 Randomisation links treatment groups to the population\nSteps:\n\nWe use the definition of \\(W_i\\) to write \\(\\sum_{W_i=1}Y_i(1)\\) as \\(\\sum_{i=1}^{n} W_iY_i(1)\\) and similarly for control units.\nWe use the linearity of \\(\\mathbb{E}\\) to move \\(\\mathbb{E}\\) inside the summation, where the only random element is \\(W_i\\).\nUse Lemma 1 (randomisation) to replace expectations with expected values.\n\n$$ \\[\\begin{align}\n\\mathbb{E}\\left[\n\\hat{\\tau}^{\\text{dm}}\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\n&=\n\\mathbb{E}\\left[\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i - \\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\\\\[5pt]\n\n&=\n\\mathbb{E}\\left[\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n- \\mathbb{E}\\left[\n\\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\\\\[5pt]\n\n&\\text{SUTVA}\n\\\\[5pt]\n\n&=\n\\mathbb{E}\\left[\\frac{1}{n_t}\\sum_{W_i=1}Y_i(1)\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\\right]\n- \\mathbb{E}\\left[\\frac{1}{n_c}\\sum_{W_i=0}Y_i(0)\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\\right]\n\\\\[5pt]\n\n&=\n\\mathbb{E}\\left[\n\\frac{1}{n_t}\\sum_{i=1}^{n}W_iY_i(1)\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n-\n\\mathbb{E}\\left[\n\\frac{1}{n_c}\\sum_{i=1}^{n}(1-W_i)Y_i(0)\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\\\\[5pt]\n\n&=\n\\frac{1}{n_t}\\sum_{i=1}^{n}\n\\mathbb{E}[W_i\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}]\nY_i(1)\n-\n\\frac{1}{n_c}\\sum_{i=1}^{n}\n\\mathbb{E}[1-W_i\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}]\nY_i(0)\n\\\\[5pt]\n\n&\\text{Randomisation (Lemma 1)}\n\\\\[5pt]\n\n&=\n\\frac{1}{n_t}\\sum_{i=1}^{n}\n\\left(\\frac{n_t}{n}\\right)\nY_i(1)\n-\n\\frac{1}{n_c}\\sum_{i=1}^{n}\n\\left(\\frac{n_c}{n}\\right)\nY_i(0)\n\\\\[5pt]\n\n&=\n\\frac{1}{n}\\sum_{i=1}^nY_i(1) - \\frac{1}{n}\\sum_{i=1}^nY_i(0)\n\\\\[5pt]\n\n&=\n\\tau\n\\end{align}\\] $$",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#variance",
    "href": "chapters/stats_of_online_experiments.html#variance",
    "title": "1  The stats of online experiments",
    "section": "1.5 Variance",
    "text": "1.5 Variance\nFor the variance calculation below we need a few more definitions. We can define sample means and variances of the potential outcomes as:\n\\[\n\\begin{align}\n\\overline{Y}(1) = \\frac{1}{n}\\sum_{i=1}^n Y_i(1),\n\\qquad\nS_1^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(Y_i(1) - \\overline{Y}(1)\\right)^2\n\\\\[5pt]\n\\overline{Y}(0) = \\frac{1}{n}\\sum_{i=1}^n Y_i(0),\n\\qquad\nS_0^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(Y_i(0) - \\overline{Y}(0)\\right)^2\n\\\\[5pt]\n\\end{align}\n\\]\nWe have already defined the sample average treatment effect in Equation 1.1. I rewrite it here for convenience and expand the definition using the above expressions:\n\\[\n\\begin{align}\n\\tau\n&= \\frac{1}{n}\\sum_{i=1}^n \\tau_i\n\\\\[5pt]&= \\frac{1}{n}\\sum_{i=1}^n \\left(Y_i(1) - Y_i(0)\\right)\n\\\\[5pt]&= \\frac{1}{n}\\sum_{i=1}^n Y_i(1) - \\frac{1}{n}\\sum_{i=1}^nY_i(0)\n\\\\[5pt]&= \\overline{Y}(1) - \\overline{Y}(0).\n\\end{align}\n\\]\nThe variance of the individual-level causal effects is:\n\\[\n\\begin{align}\nS_{\\tau_i}^2\n&= \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(Y_i(1) - Y_i(0)\n- \\left(\\overline{Y}(1) - \\overline{Y}(0)\\right)\\right)^2\n\\\\[5pt]\n&= \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(\\tau_i - \\tau\\right)^2 \\\\[5pt]\n\\end{align}\n\\] The covariance of potential outcomes is: \\[\n\\begin{align}\nS_{0, 1} &= \\frac{1}{n-1}\\sum_{i=1}^{n}\n\\left(Y_i(1) - \\overline{Y}(1)\\right)\n\\left(Y_i(0) - \\overline{Y}(0)\\right)\n\\end{align}\n\\]\nAll lemmas referred to below are here.\nWe can then calculate the variance as (I do not explicitly condition on \\(\\mathbf{n}\\) and \\(\\mathbf{Y(w)}\\) here to keep the notation lighter):\n$$ \\[\\begin{align}\n\n\\mathbb{V}\\left(\n\\hat{\\tau}^{\\text{dm}}\n\\right)\n\n&=\n\\mathbb{V}\\left(\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i - \\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\right)\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\n\\frac{1}{n_t}\\sum_{i=1}^n W_iY_i - \\frac{1}{n_c}\\sum_{i=1}^n (1-W_i)Y_i\n\\right)\n\\\\[5pt]\n\n&\\text{SUTVA}\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\n\\frac{1}{n_t}\\sum_{i=1}^n W_iY_i(1) - \\frac{1}{n_c}\\sum_{i=1}^n (1-W_i)Y_i(0)\n\\right)\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\n\\frac{1}{n_t}\\sum_{i=1}^n W_iY_i(1)\n- \\frac{1}{n_c}\\sum_{i=1}^n Y_i(0)\n+ \\frac{1}{n_c}\\sum_{i=1}^n W_iY_i(0)\n\\right)\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\n\\sum_{i=1}^n W_i\\frac{Y_i(1)}{n_t}\n- \\sum_{i=1}^n \\frac{Y_i(0)}{n_c}\n+ \\sum_{i=1}^n W_i\\frac{Y_i(0)}{n_c}\n\\right)\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\n\\sum_{i=1}^n W_i \\left(\\frac{Y_i(1)}{n_t} + \\frac{Y_i(0)}{n_c}\\right)\n- \\sum_{i=1}^n \\frac{Y_i(0)}{n_c}\n\\right)\n\\\\[5pt]\n\n&\\text{Dropping constant term}\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\n\\sum_{i=1}^n W_i \\left(\\frac{Y_i(1)}{n_t} + \\frac{Y_i(0)}{n_c}\\right)\n\\right)\n\\\\[5pt]\n\n&\\text{Demeaning (leaves variance unchanged)}\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\\sum_{i=1}^n W_i \\left(\\frac{Y_i(1)}{n_t} + \\frac{Y_i(0)}{n_c} - \\left(\\frac{\\overline{Y}(1)}{n_t} - \\frac{\\overline{Y}(0)}{n_c}\\right)\n\\right)\\right)\n&\\text{}\n\\\\[5pt]\n\n&\\text{Using shorthands } Y_i^+ = Y_i(1)/n_t + Y_i(0)/n_c \\text{ and } \\overline{Y}^+ = \\overline{Y}(1)/n_t - \\overline{Y}(0)/n_c\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\\sum_{i=1}^n W_i \\left(Y_i^+ - \\overline{Y}^+\n\\right)\\right)\n&\\text{}\n\\\\[5pt]\n\n&\\text{Rewriting variance in terms of covariance}\n\\\\[5pt]\n\n&=\n\\text{Cov}\\left(\n\\sum_{i=1}^n W_i \\left(Y_i^+ - \\overline{Y}^+\\right),\n\\sum_{j=1}^n W_j \\left(Y_j^+ - \\overline{Y}^+\\right)\n\\right)\n&\\text{}\n\\\\[5pt]\n\n&=\n\\sum_{i=1}^n \\sum_{j=1}^n\n\\text{Cov}\\left(\nW_i \\left(Y_i^+ - \\overline{Y}^+\\right),\nW_j \\left(Y_j^+ - \\overline{Y}^+\\right)\n\\right)\n&\\text{}\n\\\\[5pt]\n\n&=\n\\sum_{i=1}^n \\sum_{j=1}^n\n\\text{Cov}\\left(W_i, W_j \\right)\n\\left(Y_i^+ - \\overline{Y}^+\\right)\n\\left(Y_j^+ - \\overline{Y}^+\\right)\n&\\text{}\n\\\\[5pt]\n\n&=\n\\sum_{i=1}^n\n\\mathbb{V}\\left(W_i^2\\right)\n\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n+\n\\sum_{i=1}^n \\sum_{j \\neq i}\n\\text{Cov}\\left(W_i, W_j \\right)\n\\left(Y_i^+ - \\overline{Y}^+\\right)\n\\left(Y_j^+ - \\overline{Y}^+\\right)\n&\\text{}\n\\\\[5pt]\n\n&\\text{Lemma 2}\n\\\\[5pt]\n\n&=\n\\sum_{i=1}^n\n\\mathbb{V}\\left(W_i\\right)\n\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n+\n\\sum_{i=1}^n \\sum_{j \\neq i}\n\\text{Cov}\\left(W_i, W_j \\right)\n\\left(Y_i^+ - \\overline{Y}^+\\right)\n\\left(Y_j^+ - \\overline{Y}^+\\right)\n&\\text{}\n\\\\[5pt]\n\n&\\text{Lemma 3}\n\\\\[5pt]\n\n&=\n\\sum_{i=1}^{n}\\left(\\frac{n_tn_c}{n^2}\\right)\n\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n- \\sum_{i=1}^{n}\\sum_{j \\neq i}\\left(\\frac{n_tn_c}{n^2(n-1)}\\right)\n\\left(Y_i^+ - \\overline{Y}^+\\right)\\left(Y_j^+ - \\overline{Y}^+\\right)\n\\\\[5pt]\n\n&=\n\\left(\\frac{n_tn_c}{n^2}\\right)\n\\sum_{i=1}^{n}\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n- \\left(\\frac{n_tn_c}{n^2(n-1)}\\right)\\sum_{i=1}^{n}\\sum_{j \\neq i}\n\\left(Y_i^+ - \\overline{Y}^+\\right)\\left(Y_j^+ - \\overline{Y}^+\\right)\n\\\\[5pt]\n\n&\\text{Lemma 4}\n\\\\[5pt]\n\n&=\n\\left(\\frac{n_tn_c}{n^2}\\right)\n\\sum_{i=1}^{n}\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n+ \\left(\\frac{n_tn_c}{n^2(n-1)}\\right)\\sum_{i=1}^{n}\n\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n\\\\[5pt]\n\n&=\n\\left(\\frac{n_tn_c}{n^2} + \\frac{n_tn_c}{n^2(n-1)}\\right)\n\\sum_{i=1}^{n}\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n&\\\\[5pt]\n\n&=\n\\frac{n_tn_c(n-1) + n_tn_c}{n^2(n-1)}\n\\sum_{i=1}^{n}\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n&\\\\[5pt]\n\n&=\n\\frac{nn_tn_c - n_tn_c + n_tn_c}{n^2(n-1)}\n\\sum_{i=1}^{n}\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n&\\\\[5pt]\n\n&=\n\\frac{n_tn_c}{n(n-1)}\n\\sum_{i=1}^{n}\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n&\\\\[5pt]\n\n&\\text{Reverting to full notation and expanding square term}\n\\\\[5pt]\n\n&=\n\\frac{n_tn_c}{n(n-1)}\n\\sum_{i=1}^{n}\\left(\\frac{Y_i(1)}{n_t} + \\frac{Y_i(0)}{n_c}\n- \\frac{\\overline{Y}(1)}{n_t} - \\frac{\\overline{Y}(0)}{n_c}\\right)^2\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{n_tn_c}{n(n-1)}\n\\sum_{i=1}^{n}\\left(\n\\left(\\frac{Y_i(1)}{n_t} - \\frac{\\overline{Y}(1)}{n_t}\\right)\n+ \\left(\\frac{Y_i(0)}{n_c} - \\frac{\\overline{Y}(0)}{n_c}\\right)\n\\right)^2\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{n_tn_c}{n(n-1)}\n\\sum_{i=1}^{n}\\left(\n\\frac{1}{n_t}\\left(Y_i(1) - \\overline{Y}(1)\\right)\n+ \\frac{1}{n_c}\\left(Y_i(0) - \\overline{Y}(0)\\right)\n\\right)^2\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{n_tn_c}{n(n-1)}\\left[\n\\sum_{i=1}^{n}\\left(\n\\frac{1}{n_t^2}\\left(Y_i(1) - \\overline{Y}(1)\\right)^2\n+ \\frac{1}{n_c^2}\\left(Y_i(0) - \\overline{Y}(0)\\right)^2\n+ \\frac{2}{n_t n_c}\\left(Y_i(1) - \\overline{Y}(1)\\right)\\left(Y_i(0) - \\overline{Y}(0)\\right)\n\\right)\n\\right]\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{n_tn_c}{n(n-1)}\\left[\n\\frac{1}{n_t^2}\\sum_{i=1}^{n}\\left(Y_i(1) - \\overline{Y}(1)\\right)^2\n+ \\frac{1}{n_c^2}\\sum_{i=1}^{n}\\left(Y_i(0) - \\overline{Y}(0)\\right)^2\n+ \\frac{2}{n_t n_c}\\sum_{i=1}^{n}\\left(Y_i(1) - \\overline{Y}(1)\\right)\\left(Y_i(0) - \\overline{Y}(0)\\right)\n\\right]\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{n_c}{n n_t}\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(Y_i(1) - \\overline{Y}(1)\\right)^2\n+ \\frac{n_t}{n n_c}\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(Y_i(0) - \\overline{Y}(0)\\right)^2\n+ \\frac{2}{n}\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(Y_i(1) - \\overline{Y}(1)\\right)\\left(Y_i(0) - \\overline{Y}(0)\\right)\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{n_c}{n n_t}S_1^2\n+ \\frac{n_t}{n n_c}S_0^2\n+ \\frac{1}{n}2S_{0,1}\n&\\text{}\n\\\\[5pt]\n\n&\\text{Lemma 5}\n\\\\[5pt]\n\n&=\n\\frac{n_c}{n n_t}S_1^2\n+ \\frac{n_t}{n n_c}S_0^2\n+ \\frac{1}{n}\\left(S_1^2 + S_0^2 - S_{\\tau_i}^2\\right)\n&\\text{}\n\\\\[5pt]\n\n&=\n\\left(\\frac{n_c}{n n_t} + \\frac{1}{n}\\right)S_1^2\n+ \\left(\\frac{n_t}{n n_c} + \\frac{1}{n}\\right) S_0^2\n- \\frac{S_{\\tau_i}^2}{n}\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{n_c + n_t}{n n_t} S_1^2\n+ \\frac{n_t + n_c}{n n_c} S_0^2\n- \\frac{S_{\\tau_i}^2}{n}\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{S_1^2}{n_t}\n+ \\frac{S_0^2}{n_c}\n- \\frac{S_{\\tau_i}^2}{n}\n&\\text{}\n\\\\[5pt]\n\\end{align}\\] $${#eq-var}\nThis is the sampling variance of \\(\\hat{\\tau}^{\\text{dm}}\\). It’s a theoretical quantity we cannot directly observe. However, we can observe treatment group means:\n\\[\n\\begin{align}\n\\overline{Y}_t = \\frac{1}{n_t}\\sum_{i=1}^n W_iY_i\n\\qquad\n\\overline{Y}_c = \\frac{1}{n_c}\\sum_{i=1}^n (1-W_i)Y_i\n\\end{align}\n\\] and treatment group variances:\n\\[\n\\begin{align}\ns_t^2 = \\frac{1}{n_t-1}\\sum_{i=1}^{n}W_i\\left(Y_i - \\overline{Y}_t\\right)^2\n\\qquad\ns_c^2 = \\frac{1}{n_c-1}\\sum_{i=1}^{n}(1-W_i)\\left(Y_i - \\overline{Y}_c\\right)^2.\n\\end{align}\n\\] It can be shown that the observed treatment group variances \\(s_t^2\\) and \\(s_c^2\\) are unbiased estimators of the sample variances \\(S_1^2\\) and \\(S_0^2\\) (see, for instance, Appendix A in Chapter 6 of Imbens and Rubin (2015)). The last term in ?eq-var, \\(S_{\\tau_i}^2\\), is the variance of unit-level treatment effects, which is impossible to observe.\nAs a result, the most widely used estimator in practice is: \\[\n\\hat{\\mathbb{V}}\n= \\frac{s_t^2}{n_t} + \\frac{s_c^2}{n_c}.\n\\] In our context, the main advantages of this estimator are:\n\nIf treatment effects are constant across units, then this is an unbiased estimator of the true sampling variance since in this case, \\(S^2_{\\tau_i} = 0\\).\nIf treatment effects are not constant, then this is a conservative estimator of the sampling variance (since \\(S_{\\tau_i}^2\\) is non-negative).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#standard-error",
    "href": "chapters/stats_of_online_experiments.html#standard-error",
    "title": "1  The stats of online experiments",
    "section": "1.6 Standard error",
    "text": "1.6 Standard error\nThe standard error of an estimator is simply the square root of its sampling variance. From ?eq-var we thus have:\n\\[\n\\widehat{SE}\n= \\sqrt{\\frac{s_t^2}{n_t} + \\frac{s_c^2}{n_c}}.\n\\tag{1.2}\\]\nBecause in online experiments sample sizes are large and treatment effects are usually small, it is sometimes convenient to assume equal sample sizes, so that the sample size for each variant is \\(n_t = n_c = n_v\\), and equal variances, so that \\(s_t^2 = s_c^2 = s^2\\). The common variance \\(s^2\\) is estimated by “pooling” the treatment group variances to create a degrees-of-freedom-weighted estimator of the form: \\[\ns^2 = \\frac{(n_t - 1) s_t^2 + (n_c - 1) s_c^2}{n_t + n_c - 2}.\n\\] Substituting in Equation 1.2 we then have: \\[\n\\widehat{SE}^{\\text{equal}}\n= \\sqrt{\\frac{s^2}{n_v} + \\frac{s^2}{n_v}}\n= \\sqrt{\\frac{2s^2}{n_v}}.\n\\tag{1.3}\\]\nFinally, for the purpose of experiment design it is sometimes useful to express the standard error in terms of the proportion of units allocated to the treatment group. Hence, instead of assuming equal sample sizes, we use \\(p\\) to denote that proportion and \\(n\\) to denote total sample size, while maintaining the assumption of equal variance. Again substituting in Equation 1.2 we can then write: \\[\n\\widehat{SE}^{\\text{prop}}\n= \\sqrt{\\frac{s^2}{pn} + \\frac{s^2}{(1-p)n}}\n= \\sqrt{\\frac{s^2}{np(1-p)}}.\n\\tag{1.4}\\]\nFor \\(p=0.5\\), this formulation is equivalent to Equation 1.3 as expected.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#qa",
    "href": "chapters/stats_of_online_experiments.html#qa",
    "title": "1  The stats of online experiments",
    "section": "1.7 Q&A",
    "text": "1.7 Q&A\n\n1.7.0.0.1 Question 1\nWhy do we need potential outcomes at all? Can’t we interpret the difference from a simple comparison of averages as the causal effect?\n\n\n1.7.0.0.2 Question 2\nWhy do randomised trials not require the excludability assumption in order to lead to unbiased results?\n\n\n1.7.0.0.3 Answer 1\nWhy potential outcomes? - Clarifies what precisely we are trying to estimate: average individual treatment effect - Makes explicit the assumptions we need to make to do so: - SUTVA: What we need is to be able to write Y-i = WY1 + (1-W)Y0. For this we need i) independence from other’s assignment, and ii) clearly defined meaning of Wi =1 and Wi = 0, because if they are not clearly defined then Y1/Y0 might not be stable. SUTVA handles both of these. - Randomisation: to make sure that EY0 for W=1 equals EY0 W=0 Material - ding2023first footnote 2 in chapter 4 and\n\nWhat is definition of causal effect in suggested comparison?\nWhat is source of randomisation?\nDiscuss textbook iid approach and why it’s not a good model for our purpose.\nShow that in practice, variance is the same\n\nIn the classic two-sample problem, observations in the treatment group {y1s} and control group {y0s} are assumed to be IID draws from two separate distributions. Treatment observations are assumed to be IID draws from a distribution with mean \\(\\mu_t\\) and variance \\(\\sigma_t^2\\) and similar for control, and the variance of the difference in means estimator is given by:\n\\[\n\\mathbb{V}(\\hat{\\tau}) = \\frac{\\sigma_t^2}{n_t} + \\frac{\\sigma_c^2}{n_c}.\n\\] That is, there is no third term for the variance of the individual-level potential outcomes.\nIn contrast, Rubin points out that for proper causal inference, {y1, y0} pairs are from the same distribution but we observe only one item of the pair.\nDifferences: - Sampling based vs randomisation based variation: makes sense given that in IID case we are assumed to sample from population, whereas in FS case we are assumed to have all units, but randomise which item of the PO pair is observed. - Hence: the variance in IID is taken over the randomness of the outcomes because uncertainty is sampling based, whereas in the potential outcomes framework, where potential outcomes are fixed, the variance is taken over the randomisation distribution. - As a result: there is no correlation between two groups in IID case (covar = 0) and hence no third term, whereas in FS case there is – why precisely? Because there is correlation between y1s and y2s – if there isn’t, then the third term vanishes. See ding derivation. However, ultimately it’s because there is heterogeneity in individual-level treatment effects. Why is that? Is that the same as PO correlation at individual level? - Weird, though, that Ding lemmas seem to be based on IID case!\n\n\n1.7.0.0.4 Answer 2\n…",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#references",
    "href": "chapters/stats_of_online_experiments.html#references",
    "title": "1  The stats of online experiments",
    "section": "1.8 References",
    "text": "1.8 References\n\n\n\n\nDing, Peng. 2023. “A First Course in Causal Inference.” https://arxiv.org/abs/2305.18793.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association 81 (396): 945–60.\n\n\nImbens, Guido W, and Donald B Rubin. 2015. Causal Inference in Statistics, Social, and Biomedical Sciences. Cambridge University Press.\n\n\nRubin, Donald B. 1980. “Randomization Analysis of Experimental Data: The Fisher Randomization Test Comment.” Journal of the American Statistical Association 75 (371): 591–93.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#footnotes",
    "href": "chapters/stats_of_online_experiments.html#footnotes",
    "title": "1  The stats of online experiments",
    "section": "",
    "text": "Our \\(n\\) units are not a sample of a larger population that we hope to learn about but the entire population of units of interest. We thus use a finite sample rather than a super-population approach. For a discussion on the difference between these approaches see ?sec-experiment-setup.↩︎\nIn principle, the unit-level level causal effect can be any comparison between the potential outcomes, such as the difference \\(Y_i(1) - Y_i(0)\\) or the ratio \\(Y_i(1)/Y_i(0)\\). In online experiments, we usually focus on the difference.↩︎\nHolland (1986) discusses two solutions to the Fundamental Problem: one is the statistical solution, which relies on estimating average treatment effects across a large population of units while the other is the scientific solution, which uses homogeneity or invariance assumptions. The scientific solution works as follows: say we have one measurement of a units outcome under treatment from today and another measurement of their outcome under control from yesterday. If we are prepared to assume that control measurements are homogenous and invariant to time – that yesterday’s control measurement equals the control measurement we would have taken today – then we can calculate the individual level causal effect by comparing the two measurements taken at different points in time. Our assumption is untestable, of course, but in lab experiments it is sometimes possible to make a strong case that it is plausible. It is also the approach we informally use in daily life, whenever we conclude that taking Paracetamol helps against headaches or that going to sleep early makes us feel better the next morning.↩︎\nI have taken a slight shortcut here by treating experiments as being synonymous with the statistical solution (see previous footnote) even though observational studies can serve the same purpose (albeit with additional assumptions). I do this because my focus here is on experiments. See, for instance, Imbens and Rubin (2015) for an extensive discussion of experimental and observational approaches.↩︎\nTo denote the sum over all units in a given treatment group \\(w\\) I use \\(\\sum_{W_i=w}\\) as a shorthand for \\(\\sum_{i:W_i=w}\\) to keep the notation compact.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/hypothesis_testing.html",
    "href": "chapters/hypothesis_testing.html",
    "title": "3  Hypothesis testing",
    "section": "",
    "text": "3.1 Types of errors\nWith the above proceedure, there are two types of mistakes we can make:\nProvide detail of Neyman approach/insight (cannot control these for single experiment, but can control in the long-run over many experiments). Implications: how to interpret (and not interpret test results).\nThe complements of these two errors are:\nThe false discovery rate",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/hypothesis_testing.html#types-of-errors",
    "href": "chapters/hypothesis_testing.html#types-of-errors",
    "title": "3  Hypothesis testing",
    "section": "",
    "text": "We can reject \\(H_O\\) when it is true. This is called a Type I error and leads to a false positive. The probability of this type of error is denoted \\(\\alpha\\).\nWe can fail to reject \\(H_0\\) when it is false. This is called a Type II error and leads to a false negative. The probability of this type of error is denoted \\(\\beta\\).\n\n\n\n\nThe confidence level, the probability that we do not reject \\(H_O\\) if it is false – the probebility of a true negative – given by \\(1 - \\alpha\\).\nPower, the probability that we do reject \\(H_O\\) if it is false – the probability of a true positive – given by \\(1 - \\beta\\).\n\n\n\nThe false positive rate is \\(P(significant result | H0 true)\\)\nThe false discovery rate is \\(P(H0 true | significant result)\\)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/hypothesis_testing.html#hypothesis-testing",
    "href": "chapters/hypothesis_testing.html#hypothesis-testing",
    "title": "3  Hypothesis testing",
    "section": "3.2 Hypothesis testing",
    "text": "3.2 Hypothesis testing\nTo test whether the observed treatment effect is significantly different from zero, we test:\n\\[\n\\begin{align}\n&H_0: \\tau = 0 \\\\[5 pt]\n&H_A: \\tau \\neq 0\n\\end{align}\n\\]\nWe calculate the test-statistic\n\\[\nT =\n\\frac{\\hat{\\tau}^{\\text{dm}}}\n{SE\\left(\\hat{\\tau}^{\\text{dm}}\\right)} \\sim t_{(N_t + N_c - 2)},\n\\]\nwhere \\(N_t + N_c - 2\\) is number of degrees of freedom.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/hypothesis_testing.html#justification-for-use-of-testing-distribution",
    "href": "chapters/hypothesis_testing.html#justification-for-use-of-testing-distribution",
    "title": "3  Hypothesis testing",
    "section": "3.3 Justification for use of testing distribution",
    "text": "3.3 Justification for use of testing distribution\nWhy can we use t/z distribution?\nTo integrate: Note that the test statistic follows a t-distribution because we have to estimate the variance (that is, if we replace the true variance with its estimate when standardising a normal variable, the result follows a Student’s t-distribution). So, this has nothing to do with the CLT. However, for the test statistic to follow a student distribution, the numerator has to follow a normal distribution. Often, though, the underlying data is not normal, so that its approximately normal only for large enough samples, due to the CLT. At the same time, the t-distribution also converges to normal as the sample size increases. Hence, one we have a sample size large enough to justify using the t-distribution, we might as well use a z-test. As pointed out in Chapter 9 in Rice (2006), the test statistic above only follows a t-distribution if we use the pooled variance, but for large sample sizes, the distribution is still approximately t or normal.\n\nSee ding2023first section 4.2 and imbens2015causal section 6.6.1 and 6.6.2 for justification for testing approach\nSee ding2023first pdf page 72 for proof and derivation\nSee Rice 7.3.3\nSee Rice 6.2 on why this follows t distribution\nFor complete treatment and derivation of sampling distributions (incl. discussion of all the approximations and sample corrections), see Rice sampling chapter and my ipad notes.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/hypothesis_testing.html#understanding-p-values",
    "href": "chapters/hypothesis_testing.html#understanding-p-values",
    "title": "3  Hypothesis testing",
    "section": "3.4 Understanding P-values",
    "text": "3.4 Understanding P-values\n\n3.4.1 History of the P-value\n\nThere are two schools of thought in statistical significance testing.\nR.A. Fisher thought of the p-value as a useful metric to determine how surprising a given set of data was.\nJerzy Neyman and Egon Pearson realised that while it is not possible to eliminate false positives and negatives, it is possible to set up a process that guarantees that false positives occur only at a pre-defined rate, which they called \\(\\alpha\\).\nUnlike in Fisher’s approach, the p-value in the Neyman-Pearson approach doesn’t tell us anything about the strength of the evidence in any particular experiment besides whether or not to reject the null hypothesis, but guarantees that in the long run (over the course of many experiments), our false positive rate is not larger than \\(\\alpha\\). :::\n\n\n\n3.4.2 The base-rate fallacy\nSuppose you test 100 features out of which 10 have a true effect. If you have 80 percent power and use a 5 percent significance level. You can expect to get a significant result for 8 of the 10 working features, but you also have a false positive rate of 5%, so you’ll also get a significant result for about 5 non-working features. You thus end up with 13 treatments that appear to work. But we know that only 8 of these 13, or 62% of them work, while the remaining 5 are false discoveries. Thus, you have a false discovery rate of 38%.\nYou commit the base-rate fallacy when you confuse your p-value with your false discovery rate. This would lead you to make statements such as “My p-value is 0.002, so there is only a 2 in a 1000 chance that I found my result by chance!”\nThis confuses\n\\[\nP(significant result | H_0),\n\\]\nthe p-value, with\n\\[\nP(H_A | significant result)\n\\]",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/hypothesis_testing.html#limitations-of-hypothesis-testing",
    "href": "chapters/hypothesis_testing.html#limitations-of-hypothesis-testing",
    "title": "3  Hypothesis testing",
    "section": "3.5 Limitations of hypothesis testing",
    "text": "3.5 Limitations of hypothesis testing\n\np-values do not tell you anything about whether the result has any practical significance.\nAny intervention usually has some effect, you will always find a significant result with enough data. The question is whether the range of plausible effect sizes (captured by the confidence interval) is relevant.\nArbitrary cutoff\nNo appreciation for variation of coefficient – focus on ci instead (see Romer (2020), Imbens (2021))\nMultiple hypothesis testing (actual) – report and apply MHT-correction\nMultiple hypothesis testing (potential Gelman post)\nMHT: family-wise error rate vs false discovery rate",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/hypothesis_testing.html#confidence-intervals",
    "href": "chapters/hypothesis_testing.html#confidence-intervals",
    "title": "3  Hypothesis testing",
    "section": "3.6 Confidence intervals",
    "text": "3.6 Confidence intervals\n\nRely on confidence intervals: they provide the same information as a p-value (if they include the value of the null hypothesis then we cannot reject it) but the width of the interval also provides additional information on the uncertainty of the effect and its practical significance. (See In Praise of Confidence Intervals by David Romer and Statistical Significance, p-Values, and the Reporting of Uncertainty by Guido Imbens for more).",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/hypothesis_testing.html#testing-and-ci-duality",
    "href": "chapters/hypothesis_testing.html#testing-and-ci-duality",
    "title": "3  Hypothesis testing",
    "section": "3.7 Testing and CI duality",
    "text": "3.7 Testing and CI duality",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/hypothesis_testing.html#one-sides-vs-two-sided-tests",
    "href": "chapters/hypothesis_testing.html#one-sides-vs-two-sided-tests",
    "title": "3  Hypothesis testing",
    "section": "3.8 One-sides vs two-sided tests",
    "text": "3.8 One-sides vs two-sided tests\n\nInstacard uses one-sided because they focus on improvements (hesterberg2024power section 2)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/hypothesis_testing.html#multivariant-tests",
    "href": "chapters/hypothesis_testing.html#multivariant-tests",
    "title": "3  Hypothesis testing",
    "section": "3.9 Multivariant tests",
    "text": "3.9 Multivariant tests",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/hypothesis_testing.html#multiple-hypothesis-correction",
    "href": "chapters/hypothesis_testing.html#multiple-hypothesis-correction",
    "title": "3  Hypothesis testing",
    "section": "3.10 Multiple hypothesis correction",
    "text": "3.10 Multiple hypothesis correction\n\nSee BIT blue book 4.3 for nice explanation of BH MHT correction",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/hypothesis_testing.html#qa",
    "href": "chapters/hypothesis_testing.html#qa",
    "title": "3  Hypothesis testing",
    "section": "3.11 Q&A",
    "text": "3.11 Q&A\nQuestions\n\n\n\n\nA student is asked 12 true-or-false questions and gets 3 of them wrong. What is the probability that the student guessed randomly?\nThe student above would actually have been asked questions until they got 3 wrong. What is the probability that they guessed randomly?\n\nc)What do you conclude from a) and b)?\n\n\n\nConsider the outcome of a large sample size (e.g. 10K subjects) A/B test that yielded tiny (e.g. odds ratio of 1.0008) but statistically significant (e.g. p &lt; 0.001) results. Would you deploy the change to production. Why or why not?\n\n\n\nA researcher runs an experiment, gets a p-value of 0.033, and concludes that his false positive rate is 3.3 percent. Is that correct? Why or why not?\n\n\n\nWhy don’t we include MDE in our hypothesis statement (i.e. “a will increase b by at least MDE for target population”)? Wouldn’t that be a good idea in a business context where we strongly care about an MDE? Don’t we just not do that because academics, who developed the methods, don’t usually care about an effect being of a certain size but only about learning what the effect is?\n\n\n\nThe 95% CI for control and treatment overlap. Does this imply treatment is not significant?\nAnswers\n1\n\nCalculate p-value for getting no more than observed number of errors\n\np = stats.binom.cdf(k=3, n=12, p=0.5)\nprint(f\"p-value is: {p:.3f}\")\n2\n3\nNo. An individual experiment doesn’t have a false positive rate. The false positive rate is determined by the proceedure used for experiments over the long run. For example, if you consistently use a p-value of 0.05, then you are guaranteed to have a false positive rate of 5 percent in the long run. This is the main insight underlying the Neyman-Pearson hypothesis testing framework.\n\n\n\nIncluding the MDE’s in the hypothesis would be incorrect. Why? Because what we formulate is the alternative hypothesis. Without altering the null hypothesis, which traditionally states the effect is zero, our two hypotheses would not be complete (H0 testing effect different from 0, HA asserting that effect is at least MDE means rejecting H0 would not imply support of HA). So why not adapt H0, too? Because the hypothesis is the thing we test when we calculate our test statistic (duh!), and so if included the MDE in the hypothesis formulation, then we’d have to include it in the calculation of the test statistic (subtract it from the observed difference), which would be incorrect. Why? The question confused the MDE with the effect size under the Null hypothesis, which are conceptually different things: the MDE is the smallest effect size we want to detect – we are interested in effects that are as large or larger than the MDES, we’re not testing whether the observed difference is significantly different from it, which is what the null hypothesis value is.\n\n\n\n\nImbens, Guido W. 2021. “Statistical Significance, p-Values, and the Reporting of Uncertainty.” Journal of Economic Perspectives 35 (3): 157–74.\n\n\nRice, John A. 2006. Mathematical Statistics and Data Analysis. Cengage Learning.\n\n\nRomer, David. 2020. “In Praise of Confidence Intervals.” In AEA Papers and Proceedings, 110:55–60.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "chapters/power.html",
    "href": "chapters/power.html",
    "title": "2  Power",
    "section": "",
    "text": "2.1 Required sample size\nThe required sample size is determined by four factors:\nIn the context of online experiments, we usually fix the significance level and desired power, calculate the estimate the outcome variable’s standard deviation from historical data, fix the minimal detectable effect, and then calculate required sample size. Given these inputs, and assuming equal sample sizes and variances for treatment and control variants, that required sample size per variant is given by: \\[\n\\begin{align}\nn_v = 2(z_{\\alpha/2} + z_{1 - \\beta})^2\\frac{s^2}{\\Delta^2},\n\\end{align}\n\\tag{2.1}\\]\nThe power formula can be intimidating and confusing, all the more so since there are different and sometimes incorrect versions presented in different articles. Here, I want to derive the formula to demystify it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#required-sample-size",
    "href": "chapters/power.html#required-sample-size",
    "title": "2  Power",
    "section": "",
    "text": "The probability of making a Type I error, denoted by \\(\\alpha\\), corresponds to the significance level of the test and has an associated with the upper-tail critical value \\(z_{\\alpha/2}\\) in a two-sided test.\nThe probability of making a Type II error, denoted by \\(\\beta\\), determines the power of the test, \\(1-\\beta\\), and has associated critical value given by \\(z_{1 - \\beta}\\).\nThe standard deviation of the outcome variable, \\(s\\).\nThe minimal detectable effect size, \\(\\Delta\\).\n\n\n\n\n2.1.1 Bloom approach\n\nBloom (1995) introduces the concept of MDE to measure and compare power and provides a useful heuristic approach to perform sample size calculations.\n\n\n\n\ntitle\n\n\n\nIn above figure, which is taken from Duflo, Glennerster, and Kremer (2007), the left hand curve is the sampling distribution of the estimator under \\(H_0\\), where the true effect size is 0, and the right hand curve its sampling distribution under \\(H_A\\), where the true effect size is \\(\\Delta\\). Because in online experiments sample sizes are usually large, these sampling distributions are well approximated by a standard normal distribution.\nFor a given significance level \\(\\alpha\\), the critical value \\(z_{a/2}\\) in a two-sided test is the point in the \\(H_0\\) distribution that has \\(\\alpha/2\\) of the probability mass to its right. We can also think of that critical critical value as a distance between the center of the distribution and the critical value.\nFor a given level of power, \\(1-\\beta\\), the critical value \\(z_{1-\\beta}\\) is the point in the \\(H_A\\) distribution that has \\(1-\\beta\\) of the probability mass to its right. It, too, can be thought of as a distance.\n\nlh curve …this is sampling dist of tee, know shape from sampling theory reject h0 if value larger than za rhs is sampling distr under ha what is zk? now derive bloom formula…\nBloom (1995) introduces the notion of the MDE as a useful way to quantify power. In the process, he also uses an intuitive way to derive the power formula based on an illustration of a typical hypothesis-testing scenario.\n\n\n\n\n\n\nFigure 2.1: Source: Duflo, Glennerster, and Kremer (2007), based on Bloom (1995).\n\n\n\nLet’s start by understanding Figure 2.1, which visualises the setup of a one-sided hypothesis test where the true effect equals 0 under the null hypothesis and some positive constant \\(\\te\\) under the alternative hypothesis. Note that the curves are not the standard normal distribution, but the sampling distribution of our estimator \\(\\tee\\). This means that the standard deviation of the curves is given by the standard error of \\(\\tee\\), which is \\(\\se\\). Under the assumption of a homogenous treatment effect, the standard error is identical under \\(\\hn\\) and \\(\\ha\\), which is why the two curves have the same shape (see ?sec-experiment-stats for details).\nthe distribution will be the same under both the null and the alternative hypothesis, with the center of each distribution given by our hypothesised value of \\(\\te\\) – zero under \\(\\hn\\) and a positive constant under \\(\\ha\\).\nWe reject \\(\\hn\\) if \\(\\tee\\) is to the right of the critical value \\(\\za\\). Also, for a given level of power \\(\\beta\\),\nLargely based on (duflo2007randomization?)\nPower basics\n\\[\nn = \\frac{(f(\\alpha) + f(\\beta))}{\\text{Sample allocation}}\\frac{\\sigma}{\\delta}\n\\]\n\nIn the simplest possible, we randomly draw a sample of size \\(N\\) from an identical population, so that our observations can be assumed to be i.i.d, and we allocate a fraction \\(P\\) of our sample to treatment. We can then estiamte the treatment effect using the OLS regression\n\n\\[ y = \\alpha + \\beta T + \\epsilon\\]\n\nwhere the standard error of \\(\\beta\\) is given by \\(\\sqrt{\\frac{1}{P(1-P)}\\frac{\\sigma^2}{N}}\\).\nstd error derivation (from standard variance result of two independent samples, using population fractions):\n\n\\[\nstd = \\sqrt{\\frac{\\sigma^2}{N_t} + \\frac{\\sigma^2}{N_c}} = \\sqrt{\\frac{\\sigma^2}{PN} + \\frac{\\sigma^2}{(1-P)N}} = ... = \\sqrt{\\frac{1}{P(1-P)}\\frac{\\sigma^2}{N}}\n\\]\n\nThe distribution on the left hand side below shows the distribution of our effect size estimator \\(\\hat{\\beta}\\) if the null hypothesis is true.\nWe reject the null hypothesis if the estimated effect size is larger than the critical value \\(t_{\\alpha}\\), determined by the significance level \\(\\alpha\\). Hence, for this to happen we need \\(\\hat{\\beta} &gt; t_{\\alpha} * SE(\\hat{\\beta})\\) (follows from rearranging the t-test formula).\nOn the right is the distribution of \\(\\hat{\\beta}\\) if the true effect size is \\(\\beta\\).\nThe power of the test for a true effect size of \\(\\beta\\) is the area under this curve that falls to the right of \\(t_{\\alpha}\\). This is the probability that we reject the null hypothesis given that it is false.\nHence, to attain a power of \\(\\kappa\\) it must be that \\(\\beta &gt; (t_a + t_{1-\\kappa}) * SE(\\hat{\\beta})\\), where \\(t_{1-\\kappa}\\) is the value from a t-distribution that has \\(1-\\kappa\\) of its probability mass to the left (for \\(\\kappa = 0.8\\), \\(t_{1-\\kappa} = 0.84\\)).\nThis means that the minimum detectable effect (\\(\\delta\\)) is given by:\n\n\\[ \\delta = (t_a + tq_{1-\\kappa}) * \\sqrt{\\frac{1}{P(1-P)}\\frac{\\sigma^2}{N}} \\]\n\nRearranding for the minimum required sample size we get:\n\n\\[ N =  \\frac{(t_a + t_{1-\\kappa})^2}{P(1-P)}\\left(\\frac{\\sigma}{\\delta}\\right)^2 \\]\n\nSo that the required sample size is inversely proportional to the minimal effect size we wish to detect. This makes sense, it means that the smaller an effect we want to detect, the larger the samle size we need. In particular, given that \\(N \\propto \\delta^{-2}\\), to detect an effect of half the size we need a sample four times the size.\nSE(\\(\\beta\\)) also includes measurement error, so this is also a determinant of power.\n\n\n\n2.1.2 Two-equations approach\n\n\n2.1.3 First-principles approach\nPower is the probability that we reject the null hypothesis if there exists a true effect of size \\(\\Delta\\).\nWe thus have: \\[\n\\begin{align}\n&H_0: \\tau = 0 \\\\[5 pt]\n&H_A: \\tau = \\Delta.\n\\end{align}\n\\]\nWe test the null hypothesis by constructing the test statistic \\[\nZ =\n\\frac{\\hat{\\tau}^{\\text{dm}}}\n{\\widehat{SE}},\n\\]\nand reject \\(H_0\\) if if falls into the rejection region beyond the critical value \\(z_{\\alpha/2}\\). Because the standard normal distribution is symmetric, for a two-sided test we thus reject \\(Z\\) if $$ \\[\\begin{align}\n|Z| &&gt; z_{\\alpha/2} \\\\[5pt]\n\n\\left|\\frac{\\hat{\\tau}^{\\text{dm}}}{\\widehat{SE}}\\right|\n&&gt; z_{\\alpha/2} \\\\[5pt]\n\n\\left|\\hat{\\tau}^{\\text{dm}}\\right|\n&&gt; z_{\\alpha/2}\\widehat{SE}.\n\\end{align}\\] $$\nThe power \\(1-\\beta\\) of the test given that \\(\\tau = \\Delta\\) is the probability that the test statistic \\(Z\\) falls into the rejection region, which is: \\[\n1 - \\beta = P\\left[\n\\left|\\hat{\\tau}^{\\text{dm}}\\right|\n&gt; z_{\\alpha/2}\\widehat{SE}\n\\&gt;\\middle|\\&gt; H_A\n\\right].\n\\]\nThe test statistic falling into the lower or upper rejection region are mutually exclusive events, so the above is equal to \\[\n1 - \\beta\n= P\\left[\n\\hat{\\tau}^{\\text{dm}}\n&gt; z_{\\alpha/2}\\widehat{SE}\\&gt;\\middle|\\&gt; H_A\n\\right]\n+ P\\left[\n\\hat{\\tau}^{\\text{dm}}\n&lt; -z_{\\alpha/2}\\widehat{SE}\\&gt;\\middle|\\&gt; H_A\n\\right]\n\\]\nWe can calculate these probabilities by standardising, which gives us:\n$$ \\[\\begin{align}\n1 - \\beta\n&= P\\left[\n\\frac{\\hat{\\tau}^{\\text{dm}} - \\Delta}{\\widehat{SE}}\n&gt;\n\\frac{z_{\\alpha/2}\\widehat{SE} - \\Delta}{\\widehat{SE}}\n\\right]\n\n+ P\\left[\n\\frac{\\hat{\\tau}^{\\text{dm}} - \\Delta}{\\widehat{SE}}\n&lt;\n\\frac{-z_{\\alpha/2}\\widehat{SE} - \\Delta}{\\widehat{SE}}\n\\right]\n\n\\\\[5pt]\n\n&=\nP\\left[Z &gt; \\frac{z_{\\alpha/2}\\widehat{SE} - \\Delta}{\\widehat{SE}}\n\\right]\n\n+ P\\left[Z &lt; \\frac{-z_{\\alpha/2}\\widehat{SE} - \\Delta}{\\widehat{SE}}\n\\right]\n\n\\\\[5pt]\n\n&=\nP\\left[Z &gt; z_{\\alpha/2} - \\frac{\\Delta}{\\widehat{SE}}\n\\right]\n\n+ P\\left[Z &lt; - z_{\\alpha/2} - \\frac{\\Delta}{\\widehat{SE}}\n\\right].\n\\end{align}\\] $$\nUsing the standard normal CDF, \\(\\Phi(z)\\), we get:\n\\[\n\\begin{align}\n1 - \\beta\n=1 - \\Phi\\left(z_{\\alpha/2} - \\frac{\\Delta}{\\widehat{SE}}\\right)\n+ \\Phi\\left(- z_{\\alpha/2} - \\frac{\\Delta}{\\widehat{SE}}\\right).\n\\end{align}\n\\]\nThe probability that we reject the null hypothesis for the wrong reason, because the test statistic falls below the lower critical value for a true positive effect or above the upper critical value for a true negative effect – sometimes called a Type III error –, is very small. Hence, as the true effect size deviates from zero, one of the two terms in the expression above becomes vanishingly small and can be ignored. For the rest of this chapter, I assume we have a true positive effect and omit the second of the two terms above. We thus have:\n\\[\n\\begin{align}\n1 - \\beta\n=\n1 - \\Phi\\left(z_{\\alpha/2} - \\frac{\\Delta}{\\widehat{SE}}\\right)\n\\end{align}\n\\]\nUsing the symmetry of the standard normal distribution, which implies that \\(1 - \\Phi(k) = \\Phi(-k)\\), we can simplify this to\n\\[\n\\begin{align}\n1 - \\beta\n=\n\\Phi\\left(\\frac{\\Delta}{\\widehat{SE}} - z_{\\alpha/2}\\right).\n\\end{align}\n\\tag{2.2}\\]\nNext, remember that \\(\\Phi(z)\\) takes z-values and returns probabilities (the probability that a standard normal variable is less than a given z value), so its inverse, \\(\\Phi^{-1}(p)\\), takes probabilities and returns z-values (the \\(z\\) value with \\(p\\) probability mass to its left). Hence, \\(\\Phi^{-1}(1-\\beta)\\) refers to the upper-tail critical value of the standard normal distribution that has \\(1-\\beta\\) probability mass to its right, and which we defined above as \\(z_{1-\\beta}\\). Using this, we get:\n$$ \\[\\begin{align}\n\n\\Phi^{-1}(1 - \\beta)\n&=\n\\Phi^{-1}\\left(\n\\Phi\\left(\\frac{\\Delta}{\\widehat{SE}} - z_{\\alpha/2}\\right)\n\\right) \\\\[5pt]\n\nz_{1-\\beta}\n&=\n\\frac{\\Delta}{\\widehat{SE}} - z_{\\alpha/2} \\\\[5pt]\n\n\\Delta\n&= \\widehat{SE}\\left(z_{\\alpha/2} + z_{1-\\beta}\\right).\n\\end{align}\\] $${#eq-mde}\nThis last line is in itself useful because it shows how the MDE is determined by the standard error and our choice of Type I and Type II probabilities.\nDepending on the context, we can plug in any of the standard error versions we defined earlier. To arrive at the above version, we use Equation 1.3, which gives us:\n$$ \\[\\begin{align}\n\n\\Delta\n&= \\widehat{SE}\\left(z_{\\alpha/2} + z_{1-\\beta}\\right) \\\\[5pt]\n\n\\Delta\n&= \\sqrt{\\frac{2s^2}{n_v}}\\left(z_{\\alpha/2} + z_{1-\\beta}\\right) \\\\[5pt]\n\n\\Delta^2\n&= \\frac{2s^2}{n_v}\\left(z_{\\alpha/2} + z_{1-\\beta}\\right)^2 \\\\[5pt]\n\nn_v\n&= 2\\left(z_{\\alpha/2} + z_{1-\\beta}\\right)^2\\frac{s^2}{\\Delta^2}\n\\end{align}\\] $$\nIf, instead of using Equation 1.3 we use the standard error expressed in terms of sample proportions from Equation 1.4, we get: \\[\n\\begin{align}\nn &= \\frac{(z_{\\alpha/2} + z_{1-\\beta})^2}{p(1-p)} \\frac{s^2}{\\Delta^2},\n\\end{align}\n\\] where the left-hand side, \\(n\\) now refers to the total sample size in the experiment rather than the sample size per variant.\nFinally, if we do not assume equal variance then we have: $$ \\[\\begin{align}\n\n\\Delta\n&= \\widehat{SE}\\left(z_{\\alpha/2} + z_{1-\\beta}\\right) \\\\[5pt]\n\n\\Delta\n&= = \\sqrt{\\frac{s_t^2}{n_t} + \\frac{s_c^2}{n_c}}\\left(z_{\\alpha/2} + z_{1-\\beta}\\right) \\\\[5pt]\n\n\\Delta^2\n&= \\frac{s_t^2}{n_t} + \\frac{s_c^2}{n_c}\\left(z_{\\alpha/2} + z_{1-\\beta}\\right)^2 \\\\[5pt]\n\nn_v\n&= 2\\left(z_{\\alpha/2} + z_{1-\\beta}\\right)^2\\frac{s^2}{\\Delta^2}\n\\end{align}\\] $$\n\n\n2.1.4 Starting from Type I and Type II error conditions\nUse List, Sadoff, and Wagner (2011)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#relative-effects",
    "href": "chapters/power.html#relative-effects",
    "title": "2  Power",
    "section": "2.2 Relative effects",
    "text": "2.2 Relative effects\nSee zhou2023all ## Correlated data\nSee zhou2023all, hesterberg2024power",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#effective-sample-size-of-test",
    "href": "chapters/power.html#effective-sample-size-of-test",
    "title": "2  Power",
    "section": "2.3 Effective sample size of test",
    "text": "2.3 Effective sample size of test\n\n2.3.1 Effective Sample Size in a Two-Sample Test (Equal Variances Assumed)\nWhen comparing two groups — treatment (size \\(N_T\\)) and control (size \\(N_C\\)) — and assuming equal variances, the effective sample size for estimating the variance of the difference in means is given by the harmonic mean:\n\\[\nN_{\\text{eff}} = \\frac{1}{\\frac{1}{N_T} + \\frac{1}{N_C}}\n\\]\nThis arises because the variance of the difference in means is:\n\\[\n\\text{Var}(\\bar{Y}_T - \\bar{Y}_C) = \\sigma^2 \\left( \\frac{1}{N_T} + \\frac{1}{N_C} \\right)\n\\]\nIf we treat this as equivalent to the variance under a single sample of size \\(N_{\\text{eff}}\\), then:\n\\[\n\\text{Var}(\\text{difference}) = \\frac{2\\sigma^2}{N_{\\text{eff}}}\n\\]\nMatching both sides gives the harmonic mean as the effective sample size.\n\n\n2.3.2 Interpretation\n\nThe harmonic mean weights smaller group sizes more heavily.\nIt reflects the information content for estimating differences — imbalanced samples reduce power.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#rule-of-thumb",
    "href": "chapters/power.html#rule-of-thumb",
    "title": "2  Power",
    "section": "2.4 Rule of thumb",
    "text": "2.4 Rule of thumb\nBlog post on 16 or 32 power confusion: - Reliably looking posts who get it wrong: (https://towardsdatascience.com/probing-into-minimum-sample-size-formula-derivation-and-usage-8db9a556280b — starts with the wrong std error with N for total instead of variant sample size), there is also Kohavi book or paper that gets it wrong\n\nThere is another way to express the variance, which has led to massive confusion.\nI’m pretty sure its the 1/N vs 1/(N/2) error that accounts for the wrong result, and nobody seems to derive this from first principles to check.\nIs original wrong? Check in book – access through WBS.\n\nPopular experiment textbooks and countless sources on the internet often refer to the rule-of thumb for the total sample size calculation that is given by:\n\\[\n\\N \\approx \\frac{32\\vpe}{\\tee^2}.\n\\]\nUsing formula ?eq-sample-size we can see that the rule of thumb straightforwardly results from using the default parameters.\nAssuming equal sample size, so that \\(P=0.5\\) gives us\n\\[\nN = 4 (z_{1 - \\beta} + z_{\\alpha/2})^2\\left(\\frac{\\sev}{\\te}\\right)^2.\n\\]\nSetting the false positive rate to 5% and the false negative rate at 20% for a two-sided hypothesis test, as we commonly do, we get\n\\[\n\\begin{align}\nN &= 4 (0.84 + 1.96)^2\\left(\\frac{\\sev}{\\te}\\right)^2 \\\\\n&\\approx \\left(\\frac{32\\sev}{\\te}\\right)^2\n\\end{align}\n\\]\nGive also per variant, as this is more useful to calculate sample size for experiments with n arms.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#how-to-choose-key-parameters",
    "href": "chapters/power.html#how-to-choose-key-parameters",
    "title": "2  Power",
    "section": "2.5 How to choose key parameters",
    "text": "2.5 How to choose key parameters\n\n2.5.1 MDE\n\nWhat are you balancing here? The size of the effect you are able to identify and the time it takes to do it.\nAll else equal, the smaller a change you want to be able to detect, the longer it will take for the experiment to run because you need more sample size.\nThe relevant question to ask here is “what counts as a practically relevant change?”\nTo answer that, consider:\n\nMaturity of service (the more mature, the smaller a change can be expected)\nSize of service (the larger, the smaller a change still generates a lot of revenue)\nCost of change that need ot be covered\n\nCost of fully building out feature for launch (can be 0 when fully built out for experiment or high if we use painted door)\nCost of maintaining new code (new code has higher bugs, may increase code complexity and maintenance)\nOther costs: e.g. does CPU utilization increase?\n\n\n\n\n\n2.5.2 Significance level\n\nWhat are you balancing here? The probabilities of making a type I and type II error.\nThe higher significance level, the less likely we are to implement useless features (to make a Type I error) but the more likely we are to no implement useful features (to make a Type II error).\nHence, gotta balance cost of implementing useless feature and cost of not implementing useful feature.\nThings that play into this:\n\nHow long will feature be in effect (less long lowers risk of implementing)?\nHow widely will it be deployed (less widely lowers risk of implementing)?\nHow many users will see it / where in the funnel is it (later in funnel lowers risk of implementation)\n\nWhat to do in practice:\n\nStart from baseline values (\\(alpha = 0.05\\))\nAdjust depending on balance of risks\n\n\n\n\n2.5.3 Power\n\nWhat are you balancing here? The risk of making a Type II error and the time you have to wait for your results.\nAll else equal, the higher a level or power you want, the longer you’ll have to run the experiment to accumulate the requried sample size.\nFactors to consider:\n\nHow costly is it to not implement a useful feature.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#measuring-power",
    "href": "chapters/power.html#measuring-power",
    "title": "2  Power",
    "section": "2.6 Measuring power",
    "text": "2.6 Measuring power\n\nCohen (1977) proposes estimated effect size / standard deviation of outcome. This is useful to compare effects across studies and domains.\nBloom (1985) proposes MDE, useful for within study/domain comparisons. More directly interpretable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#what-determines-power",
    "href": "chapters/power.html#what-determines-power",
    "title": "2  Power",
    "section": "2.7 What determines power",
    "text": "2.7 What determines power\n\nSignificance level\nEffect size\nStandard error\n\nSample size\nVariant allocation proportion\nMetric variance",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#how-to-increase-power",
    "href": "chapters/power.html#how-to-increase-power",
    "title": "2  Power",
    "section": "2.8 How to increase power",
    "text": "2.8 How to increase power\n\nfor framing on how to increase power, Integrate larsen2023statistical section 2\nPower can be increased trivially by lowering the significance level, which we often don’t want to do, or by increasing sample size, which we’re often trying to avoid.\nIncrease effect size\n\nEnsure that only users who are exposed to the change are in the data to avoid dilution of the effect\n\nOptimally allocate variance proportions\n\nUsually equal for highest power\nShow why with many treatment variants, higher share in control is better\n\nReduce metric variance\n\nChoose metric with low variance\n\nIndicator variables\nAvoid count variables which have have increasing variance as experiment duration progresses\n\nUse variance reduction technique\nTrim outliers\nOnly include triggered users\n\nUse a one-sided test\nEffect of one-sided testing on required sample size.\nIn general: \\[\n  N =  \\frac{(t_a + t_{1-\\kappa})^2}{P(1-P)}\\left(\\frac{\\sigma}{\\delta}\\right)^2\n  \\]\nFor \\(\\alpha = 0.05\\), we have \\(t_{\\alpha}^{ts} = 1.96\\) and \\(t_{\\alpha}^{os} = 1.65\\), while for \\(\\kappa = 0.8\\) we have \\(t_{1 - \\kappa} = 0.84\\). Hence: \\[\n  \\frac{N^{os}}{N^{ts}} = \\frac{ (1.64 + 0.84)^2}{(1.96 + 0.84)^2} = \\frac{6.2}{7.84} = 0.79\n  \\]\nHence, for given levels of power and significance, a one-sided test requires about 21 percent fewer observations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#problems-with-low-power",
    "href": "chapters/power.html#problems-with-low-power",
    "title": "2  Power",
    "section": "2.9 Problems with low power",
    "text": "2.9 Problems with low power\n\nTruth inflation: underpowered studies only find a significant effect it the effect size is larger than the true effect size, leading to inflated claims of effect sizes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#power-in-online-experiments",
    "href": "chapters/power.html#power-in-online-experiments",
    "title": "2  Power",
    "section": "2.10 Power in online experiments",
    "text": "2.10 Power in online experiments\n\nKohavi et al. (2014) point out (in rule 7) that while general advice suggets that the CLT provides a good approximation for n larger than 30, the large skew in online metrics often requires many moer users. They recomment 355 * (skewness coefficient)^2.\n\nThe theory for power calculation was developed for metrics with fixed values such as hight or weight.\n\n(During an experiment, the treatment will still change the metric values, but in practice we often make the sensible assumption that the treatment effect will be small, so that the variance between treatment and control are the same. This, in turn, then justifies use of pre-experiment data under the assumption that pre-experiment and experiment data will be very similar.)\nIn online experiments, we often experiment with metrics that are only defined for a specific period of time (e.g. conversion during a 1-month period starting on 15 March 2024).\nThis makes power calculation more complicated.\nWhen calculating required sample size (and/or experiment duration) for an experiment we want to make sure that a Z-test performed at the end of the experiment with the required number of unique units has a certain pre-specified level of power.\nTo calculate that required sample size we input a baseline metric value and the metric standard deviation.\nWith metrics defined only for specific periods, how to calculate these two values is not straightforward.\nLet’s see what we generally do when a metric value is fixed.\nActually, writing this and thinking of an example for the above makes me think that this might be an issue inherent to causal inference analysis.\nAn example where we don’t have the problem is if we want to compare the height of Londoners to the height of Berliners. Here, we’d do the following:\n\nDraw a random sample of Londoners and measure mean and variance of their heights.\nDo the same for a random sample of Berliners.\nPerform a Z-test and calculate its power.\n\nWriting the above makes me realise that the issue is inherent in ex-ante power calculations:\n\nEven in the Londoners and Berliners height example above we’d run into the same problem if we wanted to calculate, before taking any samples, how many samples we’d have to take to have our Z-test be adequately powered.\n\nThe problem arises once we rearrange the power formula from\n\n\\[\n1 - \\beta = f(\\sigma^2, \\delta, P, z_\\alpha, z_{1-\\beta})\n\\] to \\[\nN = \\frac{z_\\alpha + z_{1-\\beta}}{P(1-P)}\\frac{\\sigma^2}{\\delta^2}\n\\] - Because we would use the first version at the time we perform the analysis when we have all the required inputs, whereas we perform the second one before the analysis when we have to estimate \\(\\sigma\\) and \\(\\delta\\).\n\nThe core of the problem is that for many online experiment metrics baseline metric mean and variance change depending on (1) the size of the sample they are calculated from and (2) the period of time and period location they are calculated based on.\n\nis always the case, even in the Londoners and Berliners example above. It’s inherent in performing power calculations.\n\n\nis an additional complications in many online experiments. The two components are period length and period location (i.e. do we measure period of length \\(t\\) in January of September).\n\nOutside of periods that are non-representative because of seasonality reasons, ignoring period location should usually not be a big problem.\nHowever, period duration might make a difference.\nSo, the core problem is that in online experiments, in addition to approximating the sample we calculate metrics based on we also have to approximate the time period.\nHow big a difference does calculating means and stds based on different time periods make? The difference can be substantial. The below table shows means and std for order visit conversion for a set of UK users based on different period lengths.\n\n![[order-conversion-visit-different-periods.png|300]]\n\nRequired sample size is directly proportional to the sample variance, which means that using the variance based on one week instead of 1 month of data would increase required sample size by a factor of \\(\\frac{0.36^2}{0.31^2} = 1.35\\).\nHow hard is it to decently estimate an appropriate time-period? There are two parts we have to estimate: required number of unique units, and how long it takes to gather data from these many units.\nThe required number of units is determined by:\n\nMetric value mean (for experiment-period-length long period)\nMetric value variance (for experiment-period-length long period)\n\nTo amount of time it takes us to gather data for the required number of units depends on traffic to the precise point of the user-funnel/app where the bucketing for the experiment takes place.\n\nSolution: - To ensure that analysis is correctly powered, calculate power every day and stop once adequately powered. - If you want ex-ante guidance, use sensible approximations.\n\nFirst best: to know when analysis is sufficiently powered, calculate power daily and stop experiment when required level of power reached. This ensures that we have both (1) sample we use for analysis and (2) period used for analysis.\nSecond best, if we performed power using data from, say, the first 7-days of the experiment period, we would have a subset of (1) and could intelligently estimate (2) because the observed traffic would take into account the bucketing point and we could estimate future traffic based on it (e.g. we can estimate unique user visits based on unique visits during first week, with different traffic being the result of different bucketing points, but we can estimate path for all bucketing points).\nThird best, if we want to perform power calculation before the experiment starts (i.e. because we want to provide duration estimate during experiment config), we could use data from recent history (e.g. calculated monthly), and calculated separately for each metric, market, and based on other relevant dimensions such as different time period. Though, here, taking into account bucketing points might be challenging and hard to scale. So think about good approximations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#best-practices",
    "href": "chapters/power.html#best-practices",
    "title": "2  Power",
    "section": "2.11 Best practices",
    "text": "2.11 Best practices\n\nWhen aiming to estimate a precise effect size rather than just being interested in statistical significance, use assurance instead of power: instead of choosing a sample size to attain a given level of power, choose sample size so that confidence interval will be suitably narrow 99 percent of the time (Sample-Size Planning for More Accurate Statistical Power: A Method Adjusting Sample Effect Sizes for Publication Bias and Uncertainty and Understanding the new statistics.)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#experiment-duration",
    "href": "chapters/power.html#experiment-duration",
    "title": "2  Power",
    "section": "2.12 Experiment duration",
    "text": "2.12 Experiment duration\n\nWe usually care about power because it determines experiment runtime.\nThere we walk about how to translate required N into runtime.\nSimon Johnson – Four Customer Characteristics That Should Change Your Experiment Runtime",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#useful-resources",
    "href": "chapters/power.html#useful-resources",
    "title": "2  Power",
    "section": "2.13 Useful resources",
    "text": "2.13 Useful resources\n\nLarsen et al. (2023) for general overview\nZhou, Lu, and Shallah (2023) for comprehensive overview of how to calculate power\nBojinov, Simchi-Levi, and Zhao (2023), section 5, for simulation results for switchbacks and generally good approach to simulation to emulate\nReich et al. (2012) power calcs for cluster-randomised experiments\nPower Analysis for Experiments with Clustered Data, Ratio Metrics, and Regression for Covariate Adjustment\nStatsig sample size calculation formula",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#qa",
    "href": "chapters/power.html#qa",
    "title": "2  Power",
    "section": "2.14 Q&A",
    "text": "2.14 Q&A\nQuestions:\n\nLonger experiment duration generally increases power. Can you think of a scenario where this is not the case?\nAn online shopping site ranks products according to their average rating. Why might this be suboptimal? What could the site do instead?\n\nAnswers:\n\nWhen using a cumulative metric such as number of likes, the variance of which will increase the longer the experiment runs, which will increase the standard error of our treatment effect estimate and lower our power. Remember that \\(SE(\\hat{\\tau}) = \\sqrt{\\frac{1}{P(1-P)}\\frac{\\sigma^2}{N}}\\). So, whether this happens depends on what happens to \\(\\frac{\\sigma^2}{N}\\), as experiment duration increases. A decrease in power is plausible – likely, even! – because \\(N\\) will increase in a concave fashion over the course of the experiment duration (some users keep coming back), while \\(\\sigma^2\\) is likely to grow faster than linearly, which causes the ratio to increase and power to decrease.\nThe approach is suboptimal because products with few ratings will have much more variance than products with many ratings, and their average rating is thus less reliable. The problem is akin to small US states having the highest and lowest rates of kidney cancer, or small schools having highest and lowest average pupil performance. Fundamentally, it’s a problem of low power – the sample size is too low to reliably detect a true effect. The solution is to use a shrinkage method: use a weighted average of the product average rating and some global product rating, with the weight of the product average rating being proportional to the number of ratings. This way, products with few ratings will be average, while products with many ratings will reflect their own rating.\n\n\n\n\n\nBloom, Howard S. 1995. “Minimum Detectable Effects: A Simple Way to Report the Statistical Power of Experimental Designs.” Evaluation Review 19 (5): 547–56.\n\n\nBojinov, Iavor, David Simchi-Levi, and Jinglong Zhao. 2023. “Design and Analysis of Switchback Experiments.” Management Science 69 (7): 3759–77.\n\n\nDuflo, Esther, Rachel Glennerster, and Michael Kremer. 2007. “Using Randomization in Development Economics Research: A Toolkit.” Handbook of Development Economics 4: 3895–3962.\n\n\nKohavi, Ron, Alex Deng, Roger Longbotham, and Ya Xu. 2014. “Seven Rules of Thumb for Web Site Experimenters.” In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1857–66.\n\n\nLarsen, Nicholas, Jonathan Stallrich, Srijan Sengupta, Alex Deng, Ron Kohavi, and Nathaniel T Stevens. 2023. “Statistical Challenges in Online Controlled Experiments: A Review of a/b Testing Methodology.” The American Statistician, 1–15.\n\n\nList, John A, Sally Sadoff, and Mathis Wagner. 2011. “So You Want to Run an Experiment, Now What? Some Simple Rules of Thumb for Optimal Experimental Design.” Experimental Economics 14: 439–57.\n\n\nReich, Nicholas G, Jessica A Myers, Daniel Obeng, Aaron M Milstone, and Trish M Perl. 2012. “Empirical Power and Sample Size Calculations for Cluster-Randomized and Cluster-Randomized Crossover Studies.” PloS One 7 (4): e35564.\n\n\nZhou, Jing, Jiannan Lu, and Anas Shallah. 2023. “All about Sample-Size Calculations for a/b Testing: Novel Extensions and Practical Guide.” arXiv Preprint arXiv:2305.16459.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/practice.html",
    "href": "chapters/practice.html",
    "title": "Practice",
    "section": "",
    "text": "This section covers practical aspects of successfully running A/B-tests at scale.\nThere a number of very good guide that cover this topic in one way or another.\n\nkohavi2009controlled\nbojinov2022online\nhbs2018booking\ntreybig2022experimentation",
    "crumbs": [
      "Practice"
    ]
  },
  {
    "objectID": "chapters/threats_to_validity.html",
    "href": "chapters/threats_to_validity.html",
    "title": "5  Threats to validity",
    "section": "",
    "text": "5.0.1 Selection bias\nCommon types of selection bias in data science:\nWays to guard against selection bias:",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Threats to validity</span>"
    ]
  },
  {
    "objectID": "chapters/threats_to_validity.html#threats-to-reliability",
    "href": "chapters/threats_to_validity.html#threats-to-reliability",
    "title": "5  Threats to validity",
    "section": "5.1 Threats to reliability",
    "text": "5.1 Threats to reliability",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Threats to validity</span>"
    ]
  },
  {
    "objectID": "chapters/threats_to_validity.html#threats-to-internal-validity",
    "href": "chapters/threats_to_validity.html#threats-to-internal-validity",
    "title": "5  Threats to validity",
    "section": "5.2 Threats to internal validity",
    "text": "5.2 Threats to internal validity\n\nInternal validity refers to the ability of a study to measure the causal effect within the study population.\n\n\n5.2.1 Misc effects (some of them relevant only for field experiments)\n\nHawthorne effect: the treatment group works harder than normal.\nJohn Henry effect: the comparison group competes with the treatment group.\nResentment and demoralising effect: not getting the treatment changes behaviour negatively.\nDemand effect: treatment units more likely to do what they think is wanted from them.\nAnticipation effect: control group changes behaviour in anticipation of future treatment.\nSurvey effect: being surveyed changes behaviour.\n\n\n\n5.2.2 Interference\n\nBasically, all violations to SUTVA\nInterference can happen due to\n\nNetwork effects\nCannibalisation of resources in marketplaces\nShared resources (i.e. treatment slowing down site for everyone)\n\n\n\n\n5.2.3 Interaction effects\n\nUsers can be simultaneously part of multiple experiments, so that what we measure for reach of them is really the effect of the interaction of all of them. This means that, if only some features are implemented, the results after roll out could be different from those observed during the experiment period.\nHowever, with large sample sizes, this should not generally be a problem because effects of different experiments average out between treatment and control group.\nWhile the above may be true statistically, interaction effects can still lead to extremely poor user experiences (blue background interacted with blue font), which is why mature platforms aim to avoid them.\n\n\n\n5.2.4 Non-representative users\nPossible scenarios:\n\nOur marketing department launches an add campaign and attracts a lot of unusual users to the site temporarily.\nA competitor does the same and temporarily takes a ways users from our site.\nHeavy-user bias: heavy users are more likely to be bucketed in an experiment, biasing the results relative to the overall effect of a feature. Depending on the context, this can be an issue.\nSolution: run experiments for longer (thought this comes with opportunity costs, and will increase cookie churn)\n\n\n\n5.2.5 Survivor bias\n\nThis is really just a version of the above: if you select only users that have used the product for some time, your sample is not representative of all users. The classic demonstration of survivor bias is Abraham Wald’s insight in WWII that you want to put extra armour where returning plans got hit the least, since it’s presumable the planes that got hit there that didn’t make it back.\n\n\n\n\n5.2.6 Novelty and learning effects\n\nChallenge: behaviour might change abruptly and temporarily in response to a new feature (novelty or “burn in” effect) or it might take a while for behaviour fully adapt to a new feature (learning effects). In both cases, the results from a relatively short experiment will not provide a representative picture of the long-run effects of a feature.\nExamples: Increasing number of adds shows on Google led to increase in add revenue initially but then decrease of clicks in the long term because it increased add blindness Hohnhold, O’Brien, and Tang (2015)\nSolutions:\n\nMeasure long-term effects (by running experiments for longer)\nHave a “holdout” group of users that isn’t exposed to any changes for a pre-set period of time (a month, a quarter), to measure long-term effects\nEstimate dynamic treatment effects to see the evolution of the treatment effect\n\n\n\n\n5.2.7 Sample ratio mismatch\n\n\n5.2.8 Attrition\nAttrition is when, for some reason, we cannot collect endline data on some units.\nIt’s a problem because when it’s systematic rather than random, treatment and control group are no longer comparable, which is a threat to internal validity. (Even if the same number of people drop out, if they are different type, the problem is the same.) Also, it reduces sample size and thus power.\nWe can limit it if we promise access to the program to everyone (phase-in design), change the level of randomisation, and improve data collection.\nIn our analysis we should 1. Report the extent of attrition, 2. Check for differential attrition (between groups, and within groups based on observable characteristics), and 3. Determine the range of estimates given attrition using a selection model or bounds.\nOne method is to use Heckman’s selection model: we look at the characteristics of those who attrite, assume that they have the same outcomes as those with the same characteristics for which we have data, and then fill in their outcome variables accordingly.\nAnother method is to use bounds. There are two types.\nManski-Horowitz bounds: Lower bound: replace all missing values in treatment with the least favourable outcome value from the observed sample and replace all missing values in control with most favourable value in the observed sample. Upper bound created in a reverse way. Unless attrition is low and the outcome variable is tightly bounded, this tends to lead to very large bounds.\nLee bounds: We treat the estimate from the available data as an upper bound, and construct the lower bound by trimming from the sample with less attrition the observations that most contribute to the treatment effect.\n\n\n5.2.9 ## Imperfect compliance\nPartial complience\n\nPartial compliance occurs if, for some reason, some people in the treatment group are not treated or some people in the control group are.\nIt’s problematic because it reduces the difference in treatment exposure between treatment and control group (in the extreme, if they are equal, we learn nothing), and because they might make treatment takeup non-random.\nA few things can help to limit the problem: make takeup easy and/or incentivise it, randomise at a higher level, and provide a treatment to everyone.\nWe can adjust our analysis by calculating LATE, either using the Wald estimator (ITT / difference in take-up between treatment and control groups) or 2SLS where we instrument the behaviour we want to encourage with the treatment dummy (and possibly other covariates).\nDo not drop non-compliers or re-assign them to the control group – compliers and non-compliers are different so that dropping or reclassifying non-compliers would re-introduce self-selection into out two samples, which defeats the whole point of the randomisation.\n\nDefiers\n\nThey are the opposite of compliers: they either take up the treatment because they were assigned to control or the other way around.\nThey might occur in an encouragement design if they overestimated the benefit of treatment and got discouraged by the information provided in the treatment.\nThey might make us significantly misinterpret the true effect (RRE page 303, or, better, MHE p 156).\nWe can deal with them only if they form an identifiable subgroup, in which case we can estimate the treatment effect on defiers and compliers separately and calculate an average treatment effect.\n\nIn an experiment with one-sided non-compliance what does IV estimate 1. if there are no always-takers and 2. if there are not never-takers?\n\nIn general, IV estimates LATE, the effect on compliers, and the treated consist of compliers and always-takers, while the non-treated consist of compliers and never-takers.\nIf there are no always-takers, the population of compliers is the same as the population of the treated, so that LATE = ATET.\nIf there are no never-takers, the population of compliers is the same as that of the untreated, so LATE = average treatment effect of the untreated.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Threats to validity</span>"
    ]
  },
  {
    "objectID": "chapters/threats_to_validity.html#treats-to-external-validity",
    "href": "chapters/threats_to_validity.html#treats-to-external-validity",
    "title": "5  Threats to validity",
    "section": "5.3 Treats to external validity",
    "text": "5.3 Treats to external validity\n\nExternal validity is concerned with whether the results from a study generalise to other contexts.\n\n\n5.3.1 Budget effects in Ads\n\nOn an adds platform, a treatment might perform very well during an experiment in that it makes marketers launch more adds. But once scaled up may do less well because the increased traffic might exhaust marketer’s budgets, leading them to reduce adds launched.\n\n\n\n5.3.2 Feedback loops from personalisation\n\nTreatments might behave differently during experimentation and once they are scaled up if the performance of a feature is a function of the size of the audience it is exposed to (an example could be a recommendation algorithm, which performs better and better as it is being used more).\n\n\n\n5.3.3 Day-of-week effects\n\nSee below\n\n\n\n5.3.4 Seasonality\n\nSeasonality comes in many forms: day of week effects, week of year effects, season effects, holiday effects, etc.\nThe challenge is that, potentially, user behaviour might differ on certain days or over certain time periods either because we get different users or because users change their behaviour.\nWhether it is really a problem depends on the context. One aspect that is often forgotten here is that seasonality, first and foremost, is about a shift in levels – activity on LinkedIn might go down during the summer months. What we usually want to measure, however, is the difference between treatment and control units. Hence, if you don’t have reason to believe that the effect of the treatment is different during a particular season (e.g. because you think it’s additive), then seasonality might not be a problem for you.\nHaving said that, it’s actually quite likely that with either different users or different behaviour by the same users, users might react differently to featore on different days. So it really is a thread to external validity, and we thus should usually care about it.\nSolution: design your experiment so as to take seasonality into account. E.g. run your experiment for at least one week to account for day of week effects (that’s generally a good idea), don’t run crucial experiments during the holiday season or on major holidays or discard data from such periods, etc.\nWhat to take into account depends on your context. So understand the relevant seasonality for you (if you’re a travel app, consider seasonality of travel demand, if you’re an e-commerce site, consider seasonality of shopping behaviour)\n\n\n\n5.3.5 Differences in time-to-action between users\n\nSome users may engage with a new features immediately, others might take a while and then react differently to it.\nWhen running experiments for a very short time, we might thus get a biased picture of the overall effect of the feature.\nWith this one, one could argue that it’s a threat to internal validity, too. But that depends on what whether we set out to measure the short or long-term causal effect in our study\n\n\n\n5.3.6 Consent\n\nIf consent for experiment participation is required, then people who give consent may be different from those who don’t.\n\n\n\n5.3.7 Heterogeneous treatment effects\n\nConsent above is a specific manifestation of the underlying problem: heterogeneous treatment effects. Often, it is plausible that participants of an experiment are different from non-participants, since – in real life – sampling into the experiment population doesn’t happen at random. So, clearly defining who the full population of interest is, what population we can make inferences about from our experiment population, and understanding hte difference between the two is crucial.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Threats to validity</span>"
    ]
  },
  {
    "objectID": "chapters/threats_to_validity.html#resources",
    "href": "chapters/threats_to_validity.html#resources",
    "title": "5  Threats to validity",
    "section": "5.4 Resources",
    "text": "5.4 Resources\n\nForbes article on when not to trust your A/B tests\nDennis Meisner discussing threats to external validity\n\n\n\n\n\nHohnhold, Henning, Deirdre O’Brien, and Diane Tang. 2015. “Focusing on the Long-Term: It’s Good for Users and Business.” In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1849–58.\n\n\nKing, Gary, and Langche Zeng. 2006. “The Dangers of Extreme Counterfactuals.” Political Analysis 14 (2): 131–59.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Threats to validity</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_configuration.html",
    "href": "chapters/experiment_configuration.html",
    "title": "7  Experiment configuration",
    "section": "",
    "text": "7.1 Minimal detectable effect\nThe minimal detectable effect size (MDE) is the smallest effect size you are well-powered to detect: if the true effect size is larger than the MDE then your effective power is larger than the \\(1-\\beta\\) you set, if the true effect is smaller, your effective power is lower.\nWhat are you balancing here? The size of the effect you are able to identify and the time it takes to do it; all else equal, the smaller a change you want to be able to detect, the longer it will take for the experiment to run because you need more sample size.\nThe relevant question to ask here is “what counts as a practically relevant change?” To answer that, consider:",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Experiment configuration</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_configuration.html#minimal-detectable-effect",
    "href": "chapters/experiment_configuration.html#minimal-detectable-effect",
    "title": "7  Experiment configuration",
    "section": "",
    "text": "Maturity of service (the more mature, the smaller a change can be expected)\nSize of service (the larger, the smaller a change still generates a lot of revenue)\nCost of change that need ot be covered\n\nCost of fully building out feature for launch (can be 0 when fully built out for experiment or high if we use painted door)\nCost of maintaining new code (new code has higher bugs, may increase code complexity and maintenance)\nOther costs: e.g. does CPU utilization increase?",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Experiment configuration</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_configuration.html#significance-level",
    "href": "chapters/experiment_configuration.html#significance-level",
    "title": "7  Experiment configuration",
    "section": "7.2 Significance level",
    "text": "7.2 Significance level\nThe significance level, often denoted \\(\\alpha\\), is the probability of concluding that your feature works when it doesn’t.\nWhat are you balancing here? The probabilities of making a type I and type II error; the higher significance level, the less likely we are to implement useless features (to make a Type I error) but the more likely we are to no implement useful features (to make a Type II error).\nHence, gotta balance cost of implementing useless feature and cost of not implementing useful feature. Things that play into this:\n\nHow long will feature be in effect (less long lowers risk of implementing)?\nHow widely will it be deployed (less widely lowers risk of implementing)?\nHow many users will see it / where in the funnel is it (later in funnel lowers risk of implementation)\nWhat to do in practice:\n\nStart from baseline values (\\(alpha = 0.05\\))\nAdjust depending on balance of risks\n\n\n\n7.2.1 Power\n\nWhat are you balancing here? The risk of making a Type II error and the time you have to wait for your results.\nAll else equal, the higher a level or power you want, the longer you’ll have to run the experiment to accumulate the requried sample size.\nFactors to consider:\n\nHow costly is it to not implement a useful feature.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Experiment configuration</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_configuration.html#how-to-get-that-info-in-practice",
    "href": "chapters/experiment_configuration.html#how-to-get-that-info-in-practice",
    "title": "7  Experiment configuration",
    "section": "7.3 How to get that info in practice?",
    "text": "7.3 How to get that info in practice?\n\nThink about the Expected Value of Information: basically: investing a little bit of research (expert consultation, querying data) reduces uncertainty a lot while costing little.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Experiment configuration</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_configuration.html#traffic-percentage",
    "href": "chapters/experiment_configuration.html#traffic-percentage",
    "title": "7  Experiment configuration",
    "section": "7.4 Traffic percentage",
    "text": "7.4 Traffic percentage",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Experiment configuration</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_configuration.html#traffic-split",
    "href": "chapters/experiment_configuration.html#traffic-split",
    "title": "7  Experiment configuration",
    "section": "7.5 Traffic split",
    "text": "7.5 Traffic split",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Experiment configuration</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_configuration.html#ramp-up-proceedure",
    "href": "chapters/experiment_configuration.html#ramp-up-proceedure",
    "title": "7  Experiment configuration",
    "section": "7.6 Ramp-up proceedure",
    "text": "7.6 Ramp-up proceedure",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Experiment configuration</span>"
    ]
  },
  {
    "objectID": "chapters/triggered_analysis.html",
    "href": "chapters/triggered_analysis.html",
    "title": "9  Triggered analysis",
    "section": "",
    "text": "(inproceedings?){deng2015diluted, title={Diluted treatment effect estimation for trigger analysis in online controlled experiments}, author={Deng, Alex and Hu, Victor}, booktitle={Proceedings of the Eighth ACM International Conference on Web Search and Data Mining}, pages={349–358}, year={2015} }\n\nFeature coverage: proportion of traffic that triggers a feature (e.g. improvement in checkout page only seen by those sessions that go to checkout)\n(deng2015diluted?) recommend using triggered analysis for features with coverage of less than 20 percent\nSee also this post for interesting discussion of what happens when you add data – dilute further",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Triggered analysis</span>"
    ]
  },
  {
    "objectID": "chapters/simpsons_paradox.html",
    "href": "chapters/simpsons_paradox.html",
    "title": "10  Simpson’s paradox",
    "section": "",
    "text": "10.1 Q&A\nQuestions\nAnswers",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simpson's paradox</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html",
    "href": "chapters/metrics.html",
    "title": "11  Metrics",
    "section": "",
    "text": "11.1 Calculating metrics in online experiments\nElements: - Facts (Eppo), response variable (larsen2023statistical) – e.g. number of clicks per user - Metrics - e.g. average clicks per user\nSee - https://www.microsoft.com/en-us/research/articles/metric-computation-for-multiple-backends/ - Eppo docs - larsen2023statistical section 1.2",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#metric-types-in-online-experiments",
    "href": "chapters/metrics.html#metric-types-in-online-experiments",
    "title": "11  Metrics",
    "section": "11.2 Metric types in online experiments",
    "text": "11.2 Metric types in online experiments\nEppo’s terminology is useful here:\nA fact is a statistic collected at the level of the unit of randomisation, so usually a user. It could be, for instance, the number of orders or the number of sessions.\nA metric is defined at the variant level and is comprised of one or two facts, each together with an aggregation method, expressed in per user terms. There are simple and ratio metrics.\nA simple metric is a single fact and aggregation metric. For instance, the sum of the number of orders in the treatment group, defined as:\n\\[\n\\frac{\\sum_{i:W_i=1}o_i}{n_t}\n\\]\nA ratio metric is the ratio of two facts and their aggregation methods (still implicitly, expressed in per user terms, but the denominator common to the numerator and denominator of the ratio drops out). For instance, number of orders per session defined as:\n\\[\n\\frac{\\frac{\\sum_{i:W_i=1}o_i}{n_t}}{\\frac{\\sum_{i:W_i=1}s_i}{n_t}}\n= \\frac{\\sum_{i:W_i=1}o_i}{\\sum_{i:W_i=1}s_i}\n\\]\nAnother way to calculate ratio metrics is using the double-average approach, whereby we first calculate user-level averages and then take the average across all users, which gives us:\n\\[\n\\frac{\\sum_{i:W_i=1}\\frac{o_i}{s_i}}{n_t}\n\\] This approach gives equal weight to each user, which makes it more robust to outliers. But it is still a ratio metric.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#why-good-metrics-matter",
    "href": "chapters/metrics.html#why-good-metrics-matter",
    "title": "11  Metrics",
    "section": "11.3 Why good metrics matter",
    "text": "11.3 Why good metrics matter\n\nGood metrics ensure that everyone works towards the same goal in a way that is reliable, transparent, and provides accountability – they ensure coherence across the company.\nGood metrics increase the probability that our evaluations detect a change if there is one – they have high sensitivity.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#picking-metrics-for-an-experiment",
    "href": "chapters/metrics.html#picking-metrics-for-an-experiment",
    "title": "11  Metrics",
    "section": "11.4 Picking metrics for an experiment",
    "text": "11.4 Picking metrics for an experiment\n\n11.4.1 Jorden Lentze from Booking:\nPrimary metric = business metric Supporting metric = close to your change Both need to be significant before you can go fullon (=ship)\nEvery change you make should be aligned with your business objectives. Only focussing on things like “items added to cart” or “searches made?” will be bad for customers because you are optimising for metrics which are too easy to change and are not aligned with the goals of your customers (they did not come to your platform to add stuff to their shopping cart 😀 )\nFrom Christophe Perrin in the same discussion (Jorden agrees) I agree, the general rule should be to use a business metric as primary decision metric but I think this can be the right approach in some cases, I would say it really depends on the maturity of the product. I know some teams at booking stopped using the main business metrics as their primary metric. The reason was that the product they were experimenting on made it very hard to get meaningful learning (using the top level business metric) in a time which made sense to the business and the product life cycle. Instead they picked 1 or 2 metrics (related to user behaviours) which they knew were good proxy for the main business KPI. This allowed them to move much faster in validating ideas. Of course you need to keep validating those proxies and run blackout experiments every now and then to make sure you are actually moving the needle in the right direction overall. I think this can work well if you careful choose and stick to relevant proxy metrics, what you should not do is start picking different metrics for each experiments just because it’s closer to the change you are making.\n\n\n11.4.2 Sebastian Honores Espejo\n\nYour metric of success impacts the success of your product more than any feature you’ll ever ship!\n\n3 step process to define metrics\n\nDefine organisation-wide end goal (e.g. have as many customers as possible who are as loyal as possible – this is equivalent to maximising customer equity, which is the sum of customer lifetime values).\n\nThis metric is directly connected to the value the business creates, but has very long feedback cycles since it might take months or years until we can measure the full effect of a feature on it.\nCustomer equity has two components: number of customers and customer lifetime values. Hence, we can focus on two things:\n\nUnderstand what brings in new customers and how we can do more of that\nUnderstand what makes customers loyal and how we can do more of that\n\n\nDefine your focus\n\n\nThe company as a whole might want to both bring in more customers and make them more loyal\nBut as a product domain, you might have to focus on one of them (depending on maturity of product, …)\n\n\nDefine KPIs\n\nExample if your focus in on customer acquisition:\n\nCustomer acquisition has two steps: visits to the site and conversion. You could focus on either or both depending on your need.\n\nKPIs for visits: visits themselves, but also drivers such as: referrals, shares, net-promoter-scores NPS (“how likely are you to recommend product”)\nKPIs for conversion: conversion, CTR\n\n\nExample if you focus on customer lifetime value\n\nTwo components: number of value-generating transactions, and average value per value-generating transaction\n\nKPIs for number of value-generating transactions: conversion rate, ratings, NPS\nKPIs for avg value per vgt: order value, basket size, etc.\n\n\nGuidelines for finding good KPIs\n\nDoes it help the customer? (i.e. “revenue per metric” might solve company rather than customer problem, while basket size might indicate that customer finds more of what they were looking for).\nUnderstand tradeoffs and correlations (e.g. is referrals or shares more correlated to visits?)\nUse binary KPIs for experimentation whenever possible (higher power, forces you to better understand the problem – e.g. aim to reduce loading time to below relevant threshold rather than reducing avg loading time)",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#metric-taxonomy",
    "href": "chapters/metrics.html#metric-taxonomy",
    "title": "11  Metrics",
    "section": "11.5 Metric taxonomy",
    "text": "11.5 Metric taxonomy\n\nDifferent contributions in the literature and different companies use different ways to classify metrics. So so the same type of metrics will have different names and the same name will be used for different types of metrics across contexts. What matters is less the labels, but an understanding of the different functions metric can serve and how different types of metric relate to one another.\nKohavi, Tang, and Xu (2020) classify metrics into goal metrics, driver metrics, and guardrail metrics. I find this useful and use it as the basis of how I think about metrics, but also add supporting and debug metrics.\nWhile I talk about these metrics below mainly as defined at the company level, they can be defined at each level within an organisation.\nThe challenge is to make sure that definitions across metric types and organisation levels cohere. Figure 6.1 in Kohavi, Tang, and Xu (2020) is a useful way to visualise this: it shows a large arrow containing many small arrows inside. This can represent goal metrics (large arrow) and driver metrics (small arrows) or organisational metrics (large arrows) and team metrics (small arrows). In each case, we want to make sure that the direction of the small arrows is as aligned as possible with the large arrow.\n\nFrom Kohavi book (online source)  ### Goal metrics\n\nAlso “success metrics” or “North Star” metrics\nDirectly aligned with the organisation’s mission and represents how it creates value for its customers\nThey are a quantitative definition of what success looks like\nTend to be long-term oriented and slow-moving\nAn organisation usually has only very few or even just one\nExamples: “average monthly purchases” for Amazon, MAP for Meta\n\n\n11.5.1 Driver metrics\n\nAlso “surrogate metrics”, “predictive metrics”, “proxy metrics”\nCapture the movement of factors contributing to the organisation’s goal\nThey are a quantitative representation of what drives success\nThey tend to be short-term oriented and fast-moving\nWill change over time as the service matures and as it becomes more closely aligned/correlated with the North Star\nDuan, Ba, and Zhang (2021) discuss lots of useful considerations for cases where we run experiments based on surrogate metrics\nExample for Amazon buyer focused team: number of high-quality sellers that join platform per month.\n\n\n\n11.5.2 Guardrail metrics\n\nKohavi, Tang, and Xu (2020) divide guardrails into two types: those protecting the business and those ensuring internal validity of experiment results\nThe main guardrail to ensure internal validity is smaple ratio mismatch (SRM). Others are discussed in chapter 21 in Kohavi, Tang, and Xu (2020)\nGuardrails that protect the business ensure that improving one part of the platform don’t come at the cost of quality/experience/something else – they basically try to guard against unintended consequences (an example would be site latency, but could also be cross-pillar effects such as when a change to customer experience affects restaurant metrics)\nDeng and Shi (2016) argue that the main feature of a good guardrail metric should be directionality, so that we can be sure that if we get a signal, it points in the right direction in terms of user experience (in contrast to debug metrics, which should have good sensitivity)\nAmazon example: average number of purchases per day (to check that influx of sellers doesn’t lead to paralysis for buysers)\n\n\n\n11.5.3 Supporting metrics\n\nIndicators that the primary or NS metric are moving in the right direction (particularly useful as leading indicators)\nAmazon example: emails sent to high-quality sellers, emails opened, etc.\n\n\n\n11.5.4 Debug metrics\n\nDeng and Shi (2016) mention debug metrics as a way to get additional informaiton about the movement of our primary metrics.\nThey can be useful when showing the individual components of combo metrics, or the numerator and denominator of ratio metrics.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#what-makes-a-good-metric",
    "href": "chapters/metrics.html#what-makes-a-good-metric",
    "title": "11  Metrics",
    "section": "11.6 What makes a good Metric",
    "text": "11.6 What makes a good Metric\n\nWhat makes a good metric in general varies of the type of matric we’re talking about.\nKohavi, Tang, and Xu (2020) argues that a good goal or North Star metric is simple and stable. A good driver metrics, especially one which we would use for experimentation (is there ever an argument to be made for a driver metric that you wouldn’t want to use for experimentation?), we have a few more criteria.\nA good starting point to think about characteristics of a good metric for experimentation is the STEDII framework from Microsoft’s experimentation platform. But I find it quite incomplete, and augment it with other measures mentioned in Kohavi, Tang, and Xu (2020).\n\nKey:\n\nMeaningful: does it reflect the goal of the company/product (check is direction of change aligned with change in quality)\nMeasurable: not everything is measurable (e.g. post-purchase satisfaction)\nAttributable: we must be able to attribute changes in the metrics to experiment variants (not possible with data from third party data proviers)\nSensitive and timely: ensures we can detect a change in a timely manner (check that metric variance is low, and that we have historically observed change in the metric)\nTrustworthy: is the metric reliable for what we want to measure (is data collection reliable? is it not gameable1? change in metric is driven by chance in customer behaviour – e.g. change in NPS could be driven by question appearing in a different place on website)\nInterpretable and actionable: do we know what a change in the metric means? can we easily interpret it?\nDirectionality: does the measure consistently go in the same direction for a change that means the same?\n\nFurther concerns:\n\nEfficient: can we use the metric at scale? (check cost of metric use)\nDebuggable: can we investigate anomalies (can we decompose metric?)\nInclusive and fair: do we know blindspots and limitations? do we have segments to check impact of most vulnerable segments?",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#developing-and-evaluating-good-metrics",
    "href": "chapters/metrics.html#developing-and-evaluating-good-metrics",
    "title": "11  Metrics",
    "section": "11.7 Developing and evaluating good metrics",
    "text": "11.7 Developing and evaluating good metrics\n\nSee Richardson et al. (2023) for a great paper on how to develop ideal proxy metrics (i.e. metrics that aren’t directly the North Star). Duan, Ba, and Zhang (2021) also seems promising.\nDeng and Shi (2016) suggest a three-step process to develop user-behaviour-driven metrics:\n\nFormulate a hypothesis based on a simple model of user behaviour (e.g. social network users who like, comment, and share more have a better user-experience)\nConduct user studies to create labelled data against which the original model can be tested (e.g. conduct user surveys and test whether users who like, comment, and share more report a better experience)\nDesign online metrics based on insights from step 2 and assess their directionality and sensitivity as discussed below (e.g. create a metric called “meaningful engagement” and test whether it has high directionality and sensitivity in past experiments)\n\nIn doing this, remember to 1) focus on behaviour patterns that are observed for most users, 2) collect labelled data from various sources to mitigate bias (surveys, lab studies, annotated logged data), and 3) use transparent models so that we can define and track debug metrics (e.g. create an online metric using a decision tree, rather than a complex ML model).\nDeng and Shi (2016) recommend using two criteria to evaluate the quality of a metric: directionality and sensitivity. Directionality requires that a move of the metric in one direction consistently captures the direction of the user experience. In practice, the direction of a metric, such as queries per user can be ambiguous. Sensitivity requires that the metric picks up changes in the user experience such that we can identify it as part of an experiment. We can think of them as the direction and the size of a vector: the more directly it points towards the North Star, and the closer it gets, the better. These two criteria also allow us to qualitatively compare different metrics and decide which one(s) to use as our OEC.\nDeng and Shi (2016) propose two ways to evaluate directionality and sensitivity. First, we can use a validation corpus: a collection of prior experiments for which we know the effect on user experience, and which we can then use to test sensitivity and directionality of our metrics. The second, if no validation corpus is available, is degeneration experiments, whereby we deliberately degenerate the user experience in a way that is acceptable and doesn’t harm long-term user experience, and then measure directionality and sensitivity of the metrics.\nDeng and Shi (2016) point out that ratio metrics (CTR or Success Query Rate) are often good candidates for OECs because they are bounded and have high sensitivity. However, they need to be interpreted carefully because a change can result from a change in the numerator, the denominator, of both, out of which only the first gives a clear signal. Hence, when relying on ratio metrics, they recommend two things: 1) rely on debug metrics to separately track numerator and denominator, 2) only rely on ratio metrics with a stable denominator. For example: Bing used Session Success Rate instead or Query Success Rate even though QSR was more sensitive because the denominator of SSR was more stable (QSR was used as a debug metric).\nFor cases where no single metric fits all scenarios, Deng and Shi (2016) recommend using a combo metric. To create such a metric, it is important to 1) understand the direction and interpretation of each metric and be aware of the scenarios when it fails to provide an accurate signal, 2) have metrics that cover all scenarios.\nOverall process\n\nDefine Goals for feature (e.g. improve efficiency)\nDefine Signals (fewer undos or erases)\nDefine Metrics (average number of undos per session)",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#combining-metrics-into-a-signal-creating-an-oec",
    "href": "chapters/metrics.html#combining-metrics-into-a-signal-creating-an-oec",
    "title": "11  Metrics",
    "section": "11.8 Combining metrics into a signal / creating an OEC",
    "text": "11.8 Combining metrics into a signal / creating an OEC\n\nWe often consider multiple key metrics, and have a mental model of the trade-offs we are willing to accept (i.e. how much churn are we willing to accept as long as other users compensate for loss)?\nAn OEC is a way to formally state these trade-offs by creating a single metric as a weighted average of all key metrics.\nPossible approaches\n\nNormalise each metrics to between 0 and 1 and assign a weight to each",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#how-to-select-metrics",
    "href": "chapters/metrics.html#how-to-select-metrics",
    "title": "11  Metrics",
    "section": "11.9 How to select metrics",
    "text": "11.9 How to select metrics\n\nNSM – what is essence of company? How can we capture it?\nDriver metrics\n\nIn general, want product/service metrics that capture essence/goal/value proposition of product and drive company mission/NSM\nE.g. for mature product, might wanna focus on engagement rather than activation/growth\nIs there one single NSM for product/service (i.e. #of txns for Marketplace)\nQuestions to think about\n\nHow does the product work, exactly? (Do we know if transaction happens?)\nWhat is essence of product? (Give everyone opportunity to sell things and help locals find what they need)\nWhat are its goals? (Product pretty mature, so maybe engagement rather than growth)\nWhat would success look like? (We facilitate a lot of transactions)\nHow do users get value from it? (Sellers sell things quickly, buyers find things quickly)\n\n\nSupport and guardrails trickier. Use AAAERRR Framework\nTo ensure coherence across all workstreams in a company, metrics used at all levels have to contribute to the same overall goal, which is captured by the company’s North Star.\nPrimary metric something that directly captures what you wanna improve? Guardrails general health metrics you don’t want to go down (e.g. revenue, conversion)? Based on Kohavi anecdote below\nBojinov, Chen, and Liu (2020) mention that LinkedIn has four company wide success metrics and many product specific ones. So, presumably we’d use company-wide ones as guardrails. Question is, what are good product-specific metrics? Good in the sense that they have a positive impact on business-wide metrics? Can use causal inference (e.g. IV) to test effect (see section 2.1 in Bojinov, Chen, and Liu (2020))\nHave an OEC that directly captuers what you want to measure. Selecting the wrong metric can lead to misleading results. (kohavi2012trusworthy?) provide a memorable example from an experiment at Bing: the experiment increased revenue by user because search results were poorer, leading users to make more searches and lead them to click on more adds. This is good in the short-term. But in the long term, users will surely get frustrated by the poorer search results. A better metric would have been one that directly captures the quality of the search results, such as sessions per user. Lesson: have an OEC that directly captures the thing you want to improve, and use higher level-metrics such as revenue as guardrails.\nNormalise metrics by sample size. E.g. user revenue per user rather than variant-level revenue (since the latter is dependent on number of users, which can vary between variants).\nAdam D’Angelo on metrics here\n\nWhat to measure? Focus on users who are getting value today\n\nActive users (active if getting value)\nRevenue (implies they get value)\nTransactions (for marketplace)\n\nIn general, if you have two groups, focus on metric that unifies them (i.e. for marketplace, transactions captures value for sellers and buyers)",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#good-use-of-guardrail-metrics",
    "href": "chapters/metrics.html#good-use-of-guardrail-metrics",
    "title": "11  Metrics",
    "section": "11.10 Good use of guardrail metrics",
    "text": "11.10 Good use of guardrail metrics\n\nBased on Airbnb’s Experiment Guardrail system as discussed in this post\nGoal is to have an automated process that ensures that no change as a negative impact on another part of the product without there having been an explicit discussion about it\nSelect guardrails for each experiemen. Based on types of guardrails above, and find balance between protecting all teams and moving fast – remember, as pointed out in the post, if you have 50 guardrail metrics and alert any significant degradation, then you have at least one false alert in 92% percent of experiments – given that \\(1 - (1-0.05)^{50} = 0.92\\).\nFor each metric, have three types of guardrals: impact guardrail (catch experiments with high negative impact on metric), power guardrail (ensure impact guardrail has adequate significance and power levels) and stat. sig guardrail (catch even small impacts on key metrics if statsig)\nExperiments that raise a flag are being discussed among all stakeholders to make launch decision considering all trade-offs\n\n\n11.10.1 How to think about risks of a given set of metrics?\n\nThink about what I’ve focuse on (i.e. engagement for a mature product)\nThe risk is then that we neglect stages earlier in the funnel (e.g. acquisition in areas where the product doesn’t do as well) and later in the funnel (i.e. we might wanna think about monetisation)",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#frameworks-for-creating-driver-metrics",
    "href": "chapters/metrics.html#frameworks-for-creating-driver-metrics",
    "title": "11  Metrics",
    "section": "11.11 Frameworks for creating driver metrics",
    "text": "11.11 Frameworks for creating driver metrics\nDriver metrics represent quantitative measures of the factors that drive an organisation’s success. The following frameworks help us think about those drivers of success.\n\n11.11.1 Pirate metrics (AARRR)\nDeveloped by Dave McClure, the pirate metrics (summary here, slides here) classify driver metrics into:\n\nAcquisition (the user comes to our site from various channels)\nActivation (has a good experience on the first visit)\nRetention (comes back to the site)\nReferral (likes the site enough to recommend it to others)\nRevenue (engages in a revenue generating behaviour)\n\nThe below Table provides a useful example of possible conversion metrics and associated conversion rates.\n\n\n\nAARRR example conversion metrics. Source: 500hats.typepad.com/500blogs/2007/09/startup-metrics.html\n\n\n\n\n11.11.2 AAAERRR\nAn extension of the pirate metrics.\n\nAwareness (how many aware of product)\nAcquisition (how many use product)\nActivation (how many are realizing value of product – e.g. 10 friends in 7 days on FB / stored at least 1 file on a device on Dropbox)\nEngagement (breath and frequencey of engagement)\nRevenue (how many are paying for product)\nRetention/renewal (how many are coming back)\nReferral (how many are becoming advocates)\n\n\n\n11.11.3 PULSE\n\nPage views\nUptime\nLatency\nSeven-day active users\nEarnings\n\n\n\n11.11.4 HEART\n\nDeveloped by Rodden, Hutchinson, and Fu (2010), the authors address shortcomings of the PULSE framework.\nKey challenge in CHI is creation of user-experience metrics based on large-scale data.\nTraditional PULSE metrics (Page views, Uptime, Latency, Seven-day active users, Earnings) are useful and related to user-experience, but limited because they are indirect and can be ambiguous (are more page views a sign of an increase in engagement or in confusion?) and provide limited insight (seven-day active users shows user-base volume but nothing about product commitment).\nAuthors propose HEART metrics (Happiness, Engagement, Adoption, Retention, Task success) to complement traditional metrics and remedy their shortcomings.\nHappiness\n\nMeasured using bipolar scale in-product survey\nShows that users liked redesign of personalised homepage after initial dip (also shows value of dynamic treatment effects)\n\nEngagement\n\nE.g. number of visits per user per week\nHelped Gmail team see proportion of users who visited more than 5 times per week.\n\nAdoption and retention\n\nE.g. how many new accounts created (adoption), how many users from seven-day active users 3 weeks ago are still in that set (retention).\nHelped Google Finance team distinguish between new and recurring users during 2008 meltdown.\n\nTask success\n\nE.g. progress in optimal path (signup)\nHelped Google maps team see that users could adopt search to single-box so they could drop double-box version.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#common-metrics",
    "href": "chapters/metrics.html#common-metrics",
    "title": "11  Metrics",
    "section": "11.12 Common metrics",
    "text": "11.12 Common metrics\n\nConversion rate\nNumber of bookings\nEngagement\n\nLikes, shares, comments, reactions\nPage views\nClick-through rates (CTR)\nTime spent per user per day\n\nRetention\n\nDaily active users (DAU, useful to measure intensity of usage)\nWeekly active users (WAU)\nMonthly active users (MAU, user engages frequently)\nStickiness (DAU / MAU)\nChurn rate (percentage of users who stop using the service within a given period)\nRetention rate (1 - churn rate)\n28-day retention rate (% who still use it 28-days after\nTime spent on product\nSession frequency\n\nRevenue\n\nAverage revenue per user (ARPE)\nCustomer lifetime value (CLV)\n\n\nGuardrails\n\nProtecting the business\n\nRevenue\nCancellation rate\nCannibalisation of similar products in our ecosystem (i.e. WhatsApp impact on Messenger)\n\nProtecting internal validity of experiment\nUser experience metrics\n\nBounce rate (proportion of site visitors who leave after seeing only the first page)\nLatency\nNumber of messages flagged as spam/harmful\n\nStrategic priority metrics",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#other-metric-taxonomies",
    "href": "chapters/metrics.html#other-metric-taxonomies",
    "title": "11  Metrics",
    "section": "11.13 Other metric taxonomies",
    "text": "11.13 Other metric taxonomies\nBusiness report vs heuristic vs user-behaviour\n\nDeng and Shi (2016) use a different classification altogether:\n\nType 1: Business Report Driven Metrics: Business report driven metrics, like Revenue per User and Monthly Active Users, focus on long-term goals of online services. These metrics are vital for business assessments but are less actionable for short-term product improvements. For example, improving search results in a service like Bing might decrease short-term revenue per user, highlighting the need for longer-term experimentation to truly assess impacts.\nType 2: Simple Heuristic Based Metrics: Simple heuristic based metrics, such as Click-Through Rate and user activity counts, offer direct insights into user interaction with online services. While actionable, they can be misleading in terms of user experience and business goals. For instance, higher CTR due to misleading content can negatively impact user experience and, subsequently, the service’s market share. These metrics are suitable for early-stage services but may not align with real user experience improvements in more mature stages.\nType 3: User-Behavior-Driven Metrics: User-behavior-driven metrics, derived from user satisfaction and frustration models, aim to directly measure user experience and its impact on long-term service success. They are complex, involving detailed analysis of user behavior, like considering both clicks and dwell time for assessing search satisfaction. These metrics are sensitive and actionable for agile experiments, offering a more nuanced understanding of user interaction than simpler metrics.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#metric-sensitivity-decomposition",
    "href": "chapters/metrics.html#metric-sensitivity-decomposition",
    "title": "11  Metrics",
    "section": "11.14 Metric sensitivity decomposition",
    "text": "11.14 Metric sensitivity decomposition\n\nDeng and Shi (2016) point out that detecting a treatment effect has two components:\n\n\\[\nP(\\text{detecting treatment effect on the metric}) = P(H_1) \\times P(p \\leq \\alpha | H_1)\n\\]\n\n\\(P(H_1)\\) is moveability of the metric\n\\(P(p \\leq \\alpha | H_1)\\) is power\nThe authors point out that understanding which component produces a lack of sensitivity is crucial. Because if it’s movement probability, we might need a different metric, whereas with a lack of power, variance reduction might help.\nExamples for metrics with low movement probability might be “number of sessions per user” for a search engine, since daily search needs are limited, and changing user engagement is difficult in the short-term. An example for a metric with low power is “Revenue per user”, due to very high variance.\nSee also Richardson et al. (2023) for a more rigorous exposition of the point.\nThink about this more. I’m not convinced we don’t just care about power.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#metrics-vs-goals",
    "href": "chapters/metrics.html#metrics-vs-goals",
    "title": "11  Metrics",
    "section": "11.15 Metrics vs goals",
    "text": "11.15 Metrics vs goals\n\nEvery metric has its limitations.\nMeta’s goal is to have lots of users. But how do you measure this? As Alex Schultz explains here, the industry has moved from registered users to confirmed registered users to active confirmed registered users to monthly active users",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#levels-at-which-to-calculate-metrics",
    "href": "chapters/metrics.html#levels-at-which-to-calculate-metrics",
    "title": "11  Metrics",
    "section": "11.16 Levels at which to calculate metrics",
    "text": "11.16 Levels at which to calculate metrics\n\n“Conversion rate” can be calculated at a numa number of different ways:\n\nAt the variant level:\n\n\\[\nCR_{variant\\_level} = \\frac{\\text{Successful sessions across variant}}{\\text{Total sessions across variant}}\n\\]\nUser level: \\[\nCR_{User\\_level} = \\sum_{i=1}^{N_v}frac{\\text{Successful sessions across user}}{\\text{Total sessions across user}}\n\\]\nAdvantage of user level is that they are more robust to outliers, since each user has weight 1, instead of weight being proportional to their number of sessions (both total and successful).",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#resources",
    "href": "chapters/metrics.html#resources",
    "title": "11  Metrics",
    "section": "11.17 Resources",
    "text": "11.17 Resources\n\nVery good LinkedIn discussion on metric selection\n\n30min\n\n\n\n\nBojinov, Iavor, Albert Chen, and Min Liu. 2020. “The Importance of Being Causal.” Harvard Data Science Review 2 (3): 6.\n\n\nDeng, Alex, and Xiaolin Shi. 2016. “Data-Driven Metric Development for Online Controlled Experiments: Seven Lessons Learned.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 77–86.\n\n\nDuan, Weitao, Shan Ba, and Chunzhe Zhang. 2021. “Online Experimentation with Surrogate Metrics: Guidelines and a Case Study.” In Proceedings of the 14th ACM International Conference on Web Search and Data Mining, 193–201.\n\n\nKohavi, Ron, Diane Tang, and Ya Xu. 2020. Trustworthy Online Controlled Experiments: A Practical Guide to a/b Testing. Cambridge University Press.\n\n\nRichardson, Lee, Alessandro Zito, Dylan Greaves, and Jacopo Soriano. 2023. “Pareto Optimal Proxy Metrics.” arXiv Preprint arXiv:2307.01000.\n\n\nRodden, Kerry, Hilary Hutchinson, and Xin Fu. 2010. “Measuring the User Experience on a Large Scale: User-Centered Metrics for Web Applications.” In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 2395–98.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/metrics.html#footnotes",
    "href": "chapters/metrics.html#footnotes",
    "title": "11  Metrics",
    "section": "",
    "text": "A metric that isn’t gameable defies Goodhart’s law, which is what we want.↩︎",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_designs.html",
    "href": "chapters/experiment_designs.html",
    "title": "12  Experiment designs",
    "section": "",
    "text": "12.1 Designs",
    "crumbs": [
      "Designs",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Experiment designs</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_designs.html#designs",
    "href": "chapters/experiment_designs.html#designs",
    "title": "12  Experiment designs",
    "section": "",
    "text": "Check cox2000theory book. It’s the bible.\nSee BIT blue book section 5\nathey2017econometrics for rigorous discussion of basic designs\n\n\n12.1.1 Assignment mechanisms\nNotes from Imbens and Rubin (2015):\nA randomised experiment is an assignment mechanism that\n\nis probabilistic (each unit has positive probability of being assigned to treatment and control), and\nhas a known functional form that is controlled by the researcher (e.g. p(treatment) = 1/2).\n\nA classical randomised experiment is a randomised experiment with an assignment mechanism that is\n\nIndividualistic (assignment probabilities for each unit is independent of information on\nUnconfounded (assignment probabilities are independent of potential outcomes)\n\n\n\n12.1.2 Bernoulli randomised experiments\n\nAssigning each unit to treatment using coin-flip\n\n\n\n12.1.3 Completely randomised experiments\n\nEnsuring exactly \\(N_t\\) units are in treatment and rest in control\n\n\n\n12.1.4 Stratified randomised experiments\n\nCompared to completely randomised experiment, improve efficiency of ATE estimator by disallowing uninformative treatment allocations\n\nStratification is a treatment allocation method in which the pool of eligible units is first divided into strata (or groups or blocks) so as to ensure balance along certain variables. Random assignment is then performed within each group.\nEffect: Random allocation of treatment ensures that control and treatment group will be similar in expectation, stratification ensures that along certain dimensions they will be in practice. If we stratify by gender and language, for instance, we make sure that both control and treatment group have the same proportion of women who speak a given language.\nAdvantages: 1.) It can increase efficiency of the estimator by lowering residual variance (intuitively, it has the opposite effect of clustering: it makes samples more representative of the overall population, which means that repeated samples will be more similar, which means that the standard error of the estimator is smaller). 2.) It allows analysis by subgroups.\nDisadvantages: it can lead to very different sample sizes in different strata.\nWhen to stratify? - We should stratify if we want to: - increase power - achieve balance - analyse data by subgroups\n…and what variables to use? Use variables that - are highly correlated with the outcome variable - are discrete (can be discretised) - are the subgroups of interest\nHow to adjust analysis?\n\nCalculate treatment effect and variance separately for each subgroup using Neyman approach\nThen calculate overall TE and Var using stratum-weighted averages\n\n\n\n12.1.5 Matching\nHow to adjust analysis? Matching is an extreme form of stratifying, and so the approach is the same: we include a dummy for each (but one) matched pair. (If we have matched on test scores, we include a dummy for all but one possible test scores).\nDescription - Preprocess observational data such that it resembles more closely data that would have resulted from a perfectly blocked (and possibly randomised) experiment, and thus break the relationship between outcomes and pre-treatment controls.\n\nOverall goal is to create a dataset that mimics data from a randomised experiment as closely as possibe. Hence, to create data where treatment and control groups are as similar as possible on all observable covariates so that the only difference between them is the treatment.\nAs described in Stuart (2010), a matching anaysis has two stages.\nDesign stage (with matching in mind, but holds more generally for synthetic control, regression discontinuity, diff in diff):\n\nDefine “similarity” between treatment and control group\nSelect similar control group based on above definition\nValidate similarity between treatment and control\n\nAnalysis stage\n\nAnalyse the data\n\n\nAssumptions Data requirement Comparison group Considerations\n\nBalance checks: successful matching removes the relationship between outcomes and pre-treatment controls that’s often inherent in non-experimental data. To check whetehr this succeeded, we need to check that the distribution of pre-treatment controls is the same within matched treatment and control units.\nAnalysis: using difference in mean test is only appropriate if we performed exact matching, which is often not feasible. For all other forms of matching, use model with controls to estimate treatment effects.\n\nExamples - Opportunity sizing making field mandatory\nSoftware - Stuart et al. (2011)\nMatching vs synthetic controls vs regression\n\nBased on excellent discussion in introduction to Abadie and L’Hour (2021).\nSynthetic control vs nearest-neighbour matching:\n\nSimilarities: Weights for control units are positive, sum to one, and are often sparse. This allows for interpretability of the weights.\nDifferences: synthetic control doesn’t impose a fixed number of control units, and, instead of using a simple average of the control units with equal weights, it creates a control unit using a weighted average of all control units such as to minimise the discrepancy between treated and control units in the values of the matching variables.\n\n\n\n\n12.1.6 Paired randomised experiments\nPaired randomisation is an extreme form of stratification where the size of each stratum equals the number of treatment cells.\nIf we have three cells (two treatments and one control) and we pair based on test scores, we pick groups of three students, starting with the three highest scoring ones, and randomly assign one of them to each cell.\nThe aim is to increase power and achieve balance, and it’s particularly useful if we have small samples.\nThe danger is that if we have attrition and one of the three students drops out, we lose all three observations (OLS will drop all observations with the relevant dummy because there is a missing value). One remedy is to have strata of size double or triple the cell number.\n\n\n12.1.7 Clustered randomised experiments\n\nClustered randomised experiments are not designed to improve efficiency over CRE, but instead to address concerns with interactions at the unit level.\nLike in stratified design, we start by partitioning of covariate space (e.g. partition by city name). We call the resulting groups clusters instead of strata. But unlike a stratification design where units within a strata are then randomly assigned to treatment and control, we randomise at the level of the cluster, assigning all units within a cluster to the same variant.\nSo, we have a CRE/BRE at the level of the clusters\n\nAnalysis:\n\nChoice of estimand:\n\nOverall population ATE\nUnweighted average of cluster-level ATEs\n\n\n\n\n12.1.8 Switchback designs\n\nAlso called time-series experiments.\n\nNotes on bojinov2020design\n\nAB tests have two main challenges in practice: dealing with interaction effects, and estimating heterogeneous treatment effects.\nSwitchback experiments sequentially assign units to a random treatment, measure the response, and repeat the proceedure for a fixed period of time.\nThe approach can thus deal with both limitations above: it limits interference, and it can estimate individual-level causal effects, thus providing the ability to estimate heterogeneous treatment effects.\nInstead of making assumptions on the outcome model under interference, switchback experiments require assumptions on the duration of carryover effects, the duration for which the effects of one treatment during a particular period affects outcomes in subsequent periods (the authors call this carryover duration the “order of carryover effects”).\nThus far, most switchback approaches have assumed away carryover effects\nPaper provides solution to optimal switchback design in the presence of carryover effects.\nCarryover effects create bias, leading to a different treatment control comparison if switch happens hourly vs weekly\nTradeoff: fewer switching points (lower bias) vs more switching points (lower variance).\nAuthors provide computational approach to optimal design\nThere are two main areas of application: to deal with interference in a network (either in the context of network or marketplace interference) and to deal with situations where we have a limited number of units in the experiment and where we expect heterogeneous treatment effects.\nThere are three main challenges when running switchback experiments:\n\nATE estimators from switchback experiments have high variance because the precision is a function of the total number of assignments.\nOne has to deal with the existence of carryover effects.\nSuper-population inference requires unrealistic assumptions.\n\nThe paper provides solutions for all of them:\n\nIt provides an optimal design approach that reduces variance\nIt assumes carryover effects and shows that estimation and inference is valid both when they are correctly and incorrectly specified, though in the latter case estimation variance is higher. The authors also provide a method for practitioners to measure the duration of carryover effects.\nIt takes a purely design-based perspective on inference by assuming that outcomes are unknown but fixed, which means that findings are wholly non-parametric and robust to model misspecification (akin to the approach of Fisher’s exact P-value approach)\n\nAssumptions:\n\nAssumption 1 (Non-anticipating Potential Outcomes).\nFor any \\(t \\in [T]\\), \\(w_{1:t} \\in \\{0,1\\}^t\\), and for any \\(w', w'' \\in \\{0,1\\}^{T-t}\\), \\[ Y_t(w_{1:t}, w'_{t+1:T}) = Y_t(w_{1:t}, w''_{t+1:T})\\]\n\nThis says that potential outcomes don’t depend on future assignments. Given that we control the assignment mechanisms, this holds by design (i.e. units can’t adapt their behaviour in a given period based on future assignments because these assignments are random).\n\nAssumption 2 (m-Carryover Effects).\nThere exists a fixed and given m, such that for any \\(t \\in \\{m+1, m+2, \\ldots, T\\}\\), \\(w \\in \\{0,1\\}^{T-t+m+1}\\), and for any \\(w', w'' \\in \\{0,1\\}^{t-m-1}\\), \\[ Y_t(w'_{1:t-m-1}, w_{t-m:T}) = Y_t(w''_{1:t-m-1}, w{t-m:T})\\]\n\nThis says that outcomes at time \\(t\\) are independent of assignenments more than \\(m\\) periods in the past.\nTogether, the two assumption imply that for any \\(t \\in \\{m + 1, \\ldots, T\\}\\) and any two assignment paths \\(w, w' \\in \\{0, 1\\}^{m+1}\\), whenever \\(w = w'\\) this leads to:\n\n\\[\nY_t(w_{1:T})=Y_t(w'_{1:T})\n\\]\n\nWhich is a rigorous way of waying that all that matters do determine potential outcomes at time \\(t\\) is the assignment history of the previous \\(m\\) periods.\nBojinov, Simchi-Levi, and Zhao (2023)\nBojinov and Shephard (2019)\nExperiment Rigor for Switchback Experiment Analysis\nAnalyzing Switchback Experiments by Cluster Robust Standard Error to Prevent False Positive Results\n\n\n\n12.1.9 Step-wedge / phase-in randomised experiments\n\nSimilar to switchback, but can only switch from control to treatment (see bajari2023experimental)\nMight allow us to estimate long-term effects. If we focus on a specific age-group or cohort. Because then certain people will never receive the treatment and can thus act as a long-run control group. (Example: if we focus on kids in their last year of school and phase-in periods are a year, then once we get to the third school, two cohorts will have graduated and will never receive the program.)\n\n\n\n12.1.10 Split-plot design\n\nSee bajari2023experimental\n\n\n\n12.1.11 Interleaving\n\nNetflix users interleaving in first stage to quickly filter out most promising recommendation algorithms that can then be tested in classic A/B test.\nInterleaving works as follows:\n\nWe have two algorithms, A and B, each with a ranked list of recommendations\nInstead of showing some users the list from A and others that of B, we create interleaved lists using team-drafting algorithm:\nWe randomly select from which algo we take the first item, for subsequent items, each algo alternates in contributing its highest ranked item that isn’t yet in the interleaved list\nEach user is then shown such a list at each visit\nAdvantage: we massively increase effective sample size, and we get a direct within-unit comparison between A and B each time\n\nResults for Netflix\n\nFind that interleaving in first stage reduces required sample size by &gt;100x\nPreference from interleaving correlates strongly (R .95) with A/B test results\n\n\nSources:\n\nInnovating Faster on Personalization Algorithms at Netflix Using Interleaving\nInterleaving at Airbnb\nLarge-scale validation and analysis of interleaved search evaluation\n\n\n\n12.1.12 Multiple randomisation design\n\n\n12.1.13 Pigeonhole design\n\nDesign to improve covariance balance in sequentially randomised units in online experiments\nUp to 10% variance reduction over completely randomised design in simulations based on Yahoo! data\nzhao2024pigeonhole\n\n\n\n12.1.14 Multi-arm bandits\n\n\n12.1.15 Wait-list design\n\n\n12.1.16 Factorial design\n\n\n12.1.17 Encouragement design\n\nIt measures the effect of those who took up the program because of the encouragement but would not have otherwise (LATE). So it’s particularly helpful if we are interested in the effect of encouragement on marginal participants.\nThe encouragement must not encourage some and discourage others (monotonicity assumption).",
    "crumbs": [
      "Designs",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Experiment designs</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_designs.html#general-considerations",
    "href": "chapters/experiment_designs.html#general-considerations",
    "title": "12  Experiment designs",
    "section": "12.2 General considerations",
    "text": "12.2 General considerations\n\n12.2.1 Between vs within designs\nWhat’s the difference between a “between-subjects design” and a “within-subjects design” and what are their advantages? What is a “mixed-design”?\n\nA between design compares the treatment group to a control group, a within design compares the treatment group to itself before the treatment, so that each participant acts as their own control group.\nThe strength of a between design is that it protects against confounding factors, that of a within design that it improves precision (because we have baseline data).\nA mixed design is where we have a control group and baseline data. If we use that baseline, the estimator is then a difference-in-difference estimator, which is statistically more efficient.\n\n\n\n12.2.2 Simple A/B test vs factorial designs:\nBased on kohavi2007practical - Factorial designs allow for testing interactions between treatment variables. - In practice, interactions are rarer than people assume, and negative interactions can be avoided by being aware of the issue and not running obviously conflicting tests overlapping.\nGeneral advice - Run single-treatment tests to test and compare new designs - Run factorial designs when you believe that multiple factors interact strongly\n\n\n12.2.3 Multiple arms vs multiple experiments\nSituation - You have 3 ideas to solve a single problem. So each user will only see one of the solutions. Treatments are thus independent (cannot have an interaction effect because they can’t interact). Recommendation\n- Run a single test with multiple arms Reason - Direct comparison of variants (use multiple-pairwise-testing and correct for MHT) - Less overhead - dMore power (one instead of multiple control groups).\n\n\n12.2.4 Choosing unit of randomisation\nFactors to consider: - Spillovers - The level of treatment administration - The level of measurement (randomisation level needs to be the same or higher) - Power - Attrition and compliance (within-group randomisation may lead to resentment among participants and staff) - Feasibility (what is easiest, cheapest, politically possible, …)\nQuestion: - You randomise at customer level. For analysis, you do the following: you calculate metrics at a restaurant level, then calculate variant level averages from the restaurant-level averages. Is there a problem? What is it? What assumptions are being violated?\n\n\n12.2.5 Dimensions along which we can vary randomisation\nAspect: - access - access around cutoff - timing - encouragement\nLevel: - individual - group\n\n\n12.2.6 Choosing number of units per arm\n\nShow that equal split maximises power if N is fixed (use power formula and show that power is max for p = 1-p )\nIf we have many treatment groups, then more units should be allocated to control to estimate its effect more precisely (add formula)\nif sample variances differ, the ratio of the sample sizes should equal the ratio of the standard deviations.",
    "crumbs": [
      "Designs",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Experiment designs</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_designs.html#q-a",
    "href": "chapters/experiment_designs.html#q-a",
    "title": "12  Experiment designs",
    "section": "12.3 Q & A",
    "text": "12.3 Q & A\n\n12.3.1 Questions\n1.\nA food delivery company randomises at the customer level, then calculates outcomes at the restaurant level for both treatment and control group (i.e. avg food price at restaurant for treatment group units is 25, for control group units it’s 22), then calculates the variant-level average based on the restaurant averages, and then calculates the ATE as the difference between the two variant-level averages. Is this a problem? What assumptions does it relate to? Does it violate any?\n\n\n12.3.2 Answers\n\n\n\n\nAbadie, Alberto, and Jérémy L’Hour. 2021. “A Penalized Synthetic Control Estimator for Disaggregated Data.” Journal of the American Statistical Association 116 (536): 1817–34.\n\n\nBojinov, Iavor, and Neil Shephard. 2019. “Time Series Experiments and Causal Estimands: Exact Randomization Tests and Trading.” Journal of the American Statistical Association 114 (528): 1665–82.\n\n\nBojinov, Iavor, David Simchi-Levi, and Jinglong Zhao. 2023. “Design and Analysis of Switchback Experiments.” Management Science 69 (7): 3759–77.\n\n\nImbens, Guido W, and Donald B Rubin. 2015. Causal Inference in Statistics, Social, and Biomedical Sciences. Cambridge University Press.\n\n\nStuart, Elizabeth A. 2010. “Matching Methods for Causal Inference: A Review and a Look Forward.” Statistical Science: A Review Journal of the Institute of Mathematical Statistics 25 (1): 1.\n\n\nStuart, Elizabeth A, Gary King, Kosuke Imai, and Daniel Ho. 2011. “Matchit: Nonparametric Preprocessing for Parametric Causal Inference.” Journal of Statistical Software.",
    "crumbs": [
      "Designs",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Experiment designs</span>"
    ]
  },
  {
    "objectID": "chapters/philosophy.html",
    "href": "chapters/philosophy.html",
    "title": "Philosophy",
    "section": "",
    "text": "Discussion of epistemology, philosophy of causality, arguments around limits of experiments, etc.\ndawid2000causal - Argues that potential outcomes is not a useful perspective for causal inference. - Looks interesting in itself, and comments from just about every top statistician (Cox, Rubin, Casella) are super interesting",
    "crumbs": [
      "Philosophy"
    ]
  },
  {
    "objectID": "chapters/limitations_criticism.html",
    "href": "chapters/limitations_criticism.html",
    "title": "13  Limitations and criticisms",
    "section": "",
    "text": "13.1 Limitations of experiments\nPotentially: - Costly - Time consuming - Ethical concerns - Only retrospective\nNeed to ask the right questions - yeh2018parachute\n(article?){yeh2018parachute, title={Parachute use to prevent death and major trauma when jumping from aircraft: rand omized controlled trial}, author={Yeh, Robert W and Valsdottir, Linda R and Yeh, Michael W and Shen, Changyu and Kramer, Daniel B and Strom, Jordan B and Secemsky, Eric A and Healy, Joanne L and Domeier, Robert M and Kazi, Dhruv S and others}, journal={bmj}, volume={363}, year={2018}, publisher={British Medical Journal Publishing Group} }",
    "crumbs": [
      "Philosophy",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Limitations and criticisms</span>"
    ]
  },
  {
    "objectID": "chapters/limitations_criticism.html#rct-critique-of-deaton-and-cartwright",
    "href": "chapters/limitations_criticism.html#rct-critique-of-deaton-and-cartwright",
    "title": "13  Limitations and criticisms",
    "section": "13.2 RCT critique of Deaton and Cartwright",
    "text": "13.2 RCT critique of Deaton and Cartwright\n\nRead and understand critique of deaton2018understanding, read also Imbens’ response (imbens2018understanding) and overall understand debate here and develop my own thoughts.\nHow does this relate to tech?\nSummary from Stevenson (2024) (see paper for more details and comprehensive list of sources)\n\nThey cannot be used to study the past.\nThey can only estimate the effect of treatments that can be manipulated (“no causation without manipulation” Rubin).\nGuidance for policy limited by compliance issues, environmental dependence, and equilibrium effects.\nThere are ethical concerns.",
    "crumbs": [
      "Philosophy",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Limitations and criticisms</span>"
    ]
  },
  {
    "objectID": "chapters/limitations_criticism.html#using-big-data-to-manipulate-consumers",
    "href": "chapters/limitations_criticism.html#using-big-data-to-manipulate-consumers",
    "title": "13  Limitations and criticisms",
    "section": "13.3 Using “big data” to manipulate consumers",
    "text": "13.3 Using “big data” to manipulate consumers\n\nThe main concern here is that large amounts of data about individuals is used to specifically target those individuals in ways that might be harmful to them or not align with their best interest.\nI think experiments that are part of the product development process – the way experiments are generally used in tech – are not part of this, since they don’t usually use any information about individuals (i.e. no covariates), focus on average outcomes and determine the experience of all users, rather than creating experiences for individual users.\nHaving said that, the degree to which the above is true depends on what metrics are being optimised for. The alignment problem seems more pronounced for social media platforms that aim to increase “time spent on site” than for e-commerce sites that aim to increase conversion (there are a few assumptions baked in here, such as that time on social media might not be beneficial and that conversion on e-commerce signifies a customer having found what they were looking for. But they both seem defensible)",
    "crumbs": [
      "Philosophy",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Limitations and criticisms</span>"
    ]
  },
  {
    "objectID": "chapters/limitations_criticism.html#running-ethical-online-experiments",
    "href": "chapters/limitations_criticism.html#running-ethical-online-experiments",
    "title": "13  Limitations and criticisms",
    "section": "13.4 Running ethical online experiments",
    "text": "13.4 Running ethical online experiments\nSome useful principles to consider, based on reading chapter 9 in Kohavi, Tang, and Xu (2020)\n\nOnly test changes that Company policy would allow you to roll out to 100 percent of users i.e. for Meta, deliberately showing them negative content wouldn’t qualify. Personally I’m not sure I agree. I think in the Meta example, it would be useful to understand the effect of exposure to negative content. I’m not gonna think about this deeply now, but I think adapting the rule to not showing levels of content users couldn’t organically be exposed to on the platform might be more useful. In the Meta example, this would allow for studying the effect of negative content in a systematic way without making the experience worse for treatment users than it actually is for some users (and could well be for treatment users, too). The reason for my willingness to entertain to go further is that there is a potentially large benefit to understanding harm. Yes, there might be some cost in the very short term (and you’d obviously want ot bound that cost somehow, as I proposed above, in order to guard against slippery slopes), but if the insights gained allow you to prevent large harm indefinitely later, then that’s worth considering. Two other thoughts: motivation clearly matters here. And: the possibility of a slippery slope is not generally an argument to not do something – often, as here – there are quite natural and objective ways to draw a line on how far one would be willing to go.\nAim for equipoise: this is a situation where, ex-ante, there is no grounds to favour one variant over another. This is the normal case. (The term is borrowed from clinical trial, where clinical equipoise is the assumption in an RCT that no drug is ex-ante better than another).\nSome worthwhile experiments violate equipoise: increasing latency, disabling feature, showing more adds, all aim to help us collect data which can be useful to make tradeoffs later on, which, ultimately, can benefit users.\nBeware of behavioural experiments and deception.\nPresumptive consent: ask a small subset of users whether they would be okay participating and if they do, assume that the sentiment would generalise.\nDifferent from clinical trials, subjects in online trials usually have the opportunity to switch service (for sth like FB, this might be difficult).",
    "crumbs": [
      "Philosophy",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Limitations and criticisms</span>"
    ]
  },
  {
    "objectID": "chapters/limitations_criticism.html#salganik-2018s-three-principles-of-ethical-design",
    "href": "chapters/limitations_criticism.html#salganik-2018s-three-principles-of-ethical-design",
    "title": "13  Limitations and criticisms",
    "section": "13.5 Salganik (2018)’s three principles of ethical design?",
    "text": "13.5 Salganik (2018)’s three principles of ethical design?\n\nReplace: use quasi-experiments whenever possible.\nRefine: make interventions as harmless as possible.\nReduce: use the minimum necessary number of participants (use power analysis to make sure you’re not over-powered).\nWhy all this? Because we can never know whether an intervention is not harmful to at least some participants.\n\n\n\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association 81 (396): 945–60.\n\n\nKohavi, Ron, Diane Tang, and Ya Xu. 2020. Trustworthy Online Controlled Experiments: A Practical Guide to a/b Testing. Cambridge University Press.\n\n\nStevenson, Megan T. 2024. “Cause, Effect, and the Structure of the Social World.” CrimRxiv.",
    "crumbs": [
      "Philosophy",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Limitations and criticisms</span>"
    ]
  },
  {
    "objectID": "chapters/approaches_to_causal_inference.html",
    "href": "chapters/approaches_to_causal_inference.html",
    "title": "14  Approaches to causal inference",
    "section": "",
    "text": "14.1 Neyman-Rubin causal model\nFrom holland1986statistics\nTheir role is somewhat different, however: the first one is axiomatic: it’s the starting point for how we think about causal effects and intimately linked to the notion that causal effects are always relative to a different state (see holland1986statistics notes, as well as Rubin interview). The second is a corollary from the first if we are unwilling to take the scientific solution (in Holland’s words) to the Fundamental Problem: it’s the insight that leads to the statistical solution. The third is a corollary of the second: to make the statistical solution work, the assignment mechanism is central.",
    "crumbs": [
      "Philosophy",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Approaches to causal inference</span>"
    ]
  },
  {
    "objectID": "chapters/approaches_to_causal_inference.html#neyman-rubin-causal-model",
    "href": "chapters/approaches_to_causal_inference.html#neyman-rubin-causal-model",
    "title": "14  Approaches to causal inference",
    "section": "",
    "text": "The framework has three key features:\n\nCausal effects are associated with potential outcomes\nStudying causal effects required multiple units\nCentral role of the assignment mechanism",
    "crumbs": [
      "Philosophy",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Approaches to causal inference</span>"
    ]
  },
  {
    "objectID": "chapters/approaches_to_causal_inference.html#perl",
    "href": "chapters/approaches_to_causal_inference.html#perl",
    "title": "14  Approaches to causal inference",
    "section": "14.2 Perl",
    "text": "14.2 Perl\nDirected Acyclic Graphs\n\nA confounder is a variable that simultaneously affects the treatment indicator and the outcome (the same as an omitted variable).\nA collider is a variable that is simultaneously affected by the treatment indicator and the outcome.\nA backdoor path is a path from the treatment indicator to the outcome via a confounder.\nThere are two ways to close a backdoor path:\n\nControl for the confounder if it is available\nHave a collider on the backdoor path\n\nAn analysis design meets the backdoor criterion if all backdoor paths are closed, in which case we have isolated a causal effect.",
    "crumbs": [
      "Philosophy",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Approaches to causal inference</span>"
    ]
  },
  {
    "objectID": "chapters/approaches_to_causal_inference.html#thought-experiments",
    "href": "chapters/approaches_to_causal_inference.html#thought-experiments",
    "title": "14  Approaches to causal inference",
    "section": "14.3 Thought experiments",
    "text": "14.3 Thought experiments\n\nSee Heckman and Pinto (2023)\n\n\n\n\n\nHeckman, James J, and Rodrigo Pinto. 2023. “Econometric Causality: The Central Role of Thought Experiments.” National Bureau of Economic Research.",
    "crumbs": [
      "Philosophy",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Approaches to causal inference</span>"
    ]
  },
  {
    "objectID": "chapters/history.html",
    "href": "chapters/history.html",
    "title": "15  History",
    "section": "",
    "text": "Integrage: - Great-looking history of experimentation from Kohavi here\n\nholland1986statistics for extensive discussion on philosophical approaches to causality",
    "crumbs": [
      "Philosophy",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>History</span>"
    ]
  },
  {
    "objectID": "chapters/platforms.html",
    "href": "chapters/platforms.html",
    "title": "Platforms",
    "section": "",
    "text": "Implementations and examples of experiment platforms in industry.",
    "crumbs": [
      "Platforms"
    ]
  },
  {
    "objectID": "chapters/data_architecture.html",
    "href": "chapters/data_architecture.html",
    "title": "16  Data architecture",
    "section": "",
    "text": "Data Mesh architecture intro here",
    "crumbs": [
      "Platforms",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data architecture</span>"
    ]
  },
  {
    "objectID": "chapters/platform_examples.html",
    "href": "chapters/platform_examples.html",
    "title": "17  Platform examples",
    "section": "",
    "text": "17.1 Spotify",
    "crumbs": [
      "Platforms",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Platform examples</span>"
    ]
  },
  {
    "objectID": "chapters/platform_examples.html#spotify",
    "href": "chapters/platform_examples.html#spotify",
    "title": "17  Platform examples",
    "section": "",
    "text": "Confidence distinguishes between rollouts and A/B tests: rollouts help to roll changes out safely and track only guardrail metrics and allow for changes in audience traffic, while A/B-tests are a tool for product development and track success and guardrail metrics but don’t allow for changes in audience traffic. (Blog post here)\nBlog that introduces Fixed-Power designs (presumably used in Confidence) here\nPost on choosing a sequential testing framework here",
    "crumbs": [
      "Platforms",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Platform examples</span>"
    ]
  },
  {
    "objectID": "chapters/platform_examples.html#airbnb",
    "href": "chapters/platform_examples.html#airbnb",
    "title": "17  Platform examples",
    "section": "17.2 Airbnb",
    "text": "17.2 Airbnb\nExperimentation specific articles\n\nExperiments at Airbnb (2014). Provides the rationale for why Airbnb uses experimentation and how they solve a number of critical issues (stopping early, duration, etc.)\nExperiment reporting Framework (2014). Introduces ERF, the company’s early experimentation tool.\nScaling Airbnb’s experimentation platform (2017). Explains how ERF evolved to operate at increasing scale.\n4 principles for making experimentation count (2017). Discusses the key principles that Airbnb relies on to build a culture of experimentation.\n\nRelated articles\n\nData quality at Airbnb\nDemocratising data at Airbnb (2017). Discusses Dataportal, a way for people to find and understand metrics across the company\nScaling knowledge at Airbnb (2016). Provides a nice overview with links to main components of Airbnb’s data infrastructure, and then introduces Data University initiative, a series of courses to educate staff on data use.\nHow Airbnb achieved metric consistency at scale (2021). Describes Airbnb’s metric platform Minerva, which is used across all functions of the business (finance, analytics, experimentation) to ensure that everyone works with and looks at the same data.",
    "crumbs": [
      "Platforms",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Platform examples</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_setup.html",
    "href": "chapters/experiment_setup.html",
    "title": "18  Experiment setup",
    "section": "",
    "text": "18.1 Assignment mechanism",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Experiment setup</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_setup.html#assignment-mechanism",
    "href": "chapters/experiment_setup.html#assignment-mechanism",
    "title": "18  Experiment setup",
    "section": "",
    "text": "18.1.1 General discussion\n\nSee [[notes_on_imbens2015causal]] for background on assignment mechanism\n\n\n\n18.1.2 BRE\nBRE in online experiments, but CRE used in literature for analysis. Apart from those, many others.\n\nSecond, regardless of which of these two perspectives we adopt, we decide how to allocate the \\(n\\) units in the experiment sample into a treatment and control group. The procedure for performing this allocation is called the assignment mechanism, and there are a number of different such mechanisms.\n\nTreatment assignment for units in the experiment sample then works in the same way:\n\nWe again create a unique – but different – hash string for each unit (e.g. &lt;user_id&gt;&lt;experiment_id&gt;&lt;market&gt; with the first bit flipped).\n\nWhy another hash? Suppose we used the same hash strings instead, and that we sampled units with hash values that fall into the bottom 10% of possible hash values. What would happen if we now allocated all units with hash values in the bottom 50% of possible values to treatment and all others to control? How many units would be in the control group? (None! Since the hash values of all units in the sample would fall into the bottom 10% of possible hash values so that all units would be allocated to treatment.) A simple way to create a different hash value is to flip the first bit of the hash string. Because of the avalanche effect, this would result in a completely different hash value.\n\nWe use the hash algorithm to generate a hash value\nIf we wanted to allocate units equally to a control and treatment group, we could allocate all units with hash values within the bottom 50% of possible values to the treatment group and all others to the control group.\n\n\nHashed sampling and treatment assignment resembles sampling without replacement in that we can only include a given user in the experiment once, and resembles sampling with replacement in that sampling and treatment assignment are independent across units – sampling any one unit does not alter the chance of being sampled for any other unit.\nIn online experiments, as least, this is not an assumption if we properly test the randomisation proceedure (see discussion of SRM in Chapter 5)\nIn online experiments, the assignment mechanism is usually a BRE. The assignment mechanism of a BRE is individualistic, probabilistic, and unconfounded. In the simplest case without stratification, it is also independent of covariates. In all cases, the assignment mechanism is fully under our control. For probability of treatment assignment \\(q\\), we thus have:\n\n\\[\nP(\\mathbf{W} | \\mathbf{X}, \\mathbf{Y}(1), \\mathbf{Y}(0)) = P(\\mathbf{W}) = q^{n_t} (1-q)^{n_c}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Experiment setup</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_setup.html#approaches-to-analysing-bres-sampling-vs-design-based",
    "href": "chapters/experiment_setup.html#approaches-to-analysing-bres-sampling-vs-design-based",
    "title": "18  Experiment setup",
    "section": "18.2 Approaches to analysing BREs – sampling vs design based",
    "text": "18.2 Approaches to analysing BREs – sampling vs design based\n\nSampling vs randomisation based – see [[stats_foundations#Modes of inference]]\nReading Wager, it seems there are two relevant factors for inference: he just conditions on n to get a CRE, and then there is the question of whether to take a finite sample or super-population perspective.\nThe additional assumption (discussed in population asymptotics) of random sampling from super population holds for online experiments.\nHe does use Bernoulli sampling, which is what I need for online experiments. I just don’f fully understand how his perspective fits into the Imbens Rubin book / Athey Imbens one.\nSee Ding book",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Experiment setup</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_setup.html#finite-sample-vs-superpopulation-analysis",
    "href": "chapters/experiment_setup.html#finite-sample-vs-superpopulation-analysis",
    "title": "18  Experiment setup",
    "section": "18.3 Finite sample vs superpopulation analysis",
    "text": "18.3 Finite sample vs superpopulation analysis\n\nFirst, we decide whether the \\(n\\) units in our experiment sample are the population of interest, or whether that population of interest is instead a larger super-population of size \\(N\\) of which the \\(n\\) units in the experiment sample are a random sample. Following Imbens and Rubin (2015), I refer to the former as the finite sample perspective and the later as the super population perspective.\nOnline experiments typically go through a ramp-up phase: they include only a small fraction of users at the start and all users at the end.\n\nBelow is from an old version – I don’t actually consider both. Just discuss difference here. Use material from imbens2015causal to point out that, ultimately, the difference is irrelevant in practice.\n\nStatistically, this means we have to consider two different cases. In the first case, during the early stages of an experiment, we use a sample of the entire user population to learn something about that population as a whole; in this case, the sample we work with and the sample we want to learn something about are different. In the second case, when the entire user population is part of the experiment, the sample we work with and the sample we want to learn something about are the same. Following Imbens and Rubin (2015), I refer to the first approach as the super-population perspective and the second case the finite sample perspective.\nWe have a super population of \\(N\\) users. In the example I’m going to use throughout, these are all of our iOS-app users in the UK.\nFrom this super population, we sample \\(n\\) users to be part of the experiment, and then allocate \\(n_t\\) users to treatment and \\(n_c\\) users to control.\nThe way we sample users into the experiment and allocate them to treatment has implications for our statistical analysis, so let’s have a look at the details.\nOnline experiments typically use the following sampling approach to determine whether a user is part of an experiment:\n\nCreate a hash string unique for each user in the experiment such as &lt;user_id&gt;&lt;experiment_id&gt;&lt;market&gt;.\n\nFeed the hash string into a hash algorithm (often MD5) and receive a hash value.\nUse the hash value to determine whether a user is part of the experiment. Say we allocate 10% of traffic to the experiment. We then include the user only if their hash value falls within the bottom (or top) 10% of possible hash values.\n\nWith this approach, the probability that a user is sampled into the experiment is independent of the sampling decisions of all other users.\nWe write \\(R_i = 1\\) if user \\(i\\) is part of the experiment sample and \\(R_i = 0\\) if they aren’t.\nIf we sample \\(n\\) users into the experiment, then each of our \\(N\\) users is part of the experiment experiment with probability \\(n/N\\).\nFor each user in the super population, being part of the experiment sample is a Bernoulli trial with \\(R_i \\sim \\text{Bernoulli}(n/N)\\) and, hence, \\(\\mathbb{E}[R_i] = n/N\\) and \\(\\mathbb{V}(R_i) = n/N(1-n/N)\\). I assume here that we know \\(n\\). The reason for doing so is twofold. First, it makes the math easier as it spares us from modeling \\(n\\) as a Binomial random variable. Second, by the time we analyse the data, we do know \\(n\\).\n\n\n18.3.0.1 Description\nFrom athey2017econometrics\nSuper population perspective:\n\nTraditionally, uncertainty in empirical analysis is viewed as arising from randomly drawing a sample of size \\(n\\) from an infinitely large super-population of size \\(N\\).\n(Infinite probably out of convenience so math is easier. Check if ever relevant or when I have time.)\nFor example, it we could measure the height of every single person in London there would be no uncertainty about the average height of that population.\n\nFinite sample perspective\n\nWhen we perform causal inference studies, however, then there are many contexts where the above perspective is odd because we have the entire population to work with so that it’s not clear what the super-population is.\nFor instance, an online experiment with an audience traffic of 100% has got the entire population of customers.\nIn such a setting, however, thinking of causal effects as the difference in individual potential outcomes allows us to interpret the randomness as coming from treatment allocation.\n\n\n\n18.3.0.2 Notes on super-population perspective\ntodo: - Is it worth considering SP case? As in, do we have to adjust the standard error? How much do SEs differ in practice?\n\nHow to go about this: 1) establish whether or not super-population approach is required, 2) is so, compare FS and SP standard errors to check whether it makes a difference. Write up regardless to have a good theoretical foundation. But if I’m lucky then using SP SEs makes a material difference in practice, in which case this would be interesting to publish and use at work.\n\nNotes:\n\nSo far, we have focused on the case where the \\(n\\) units in the experiment sample are the population of interest.\nOnline experiments typically go through a ramp-up phase: they include only a small fraction of users at the start and all users at the end.\nStatistically, this means we have to consider two different cases. In the first case, during the early stages of an experiment, we use a sample of the entire user population to learn something about that population as a whole; in this case, the sample we work with and the sample we want to learn something about are different. In the second case, when the entire user population is part of the experiment, the sample we work with and the sample we want to learn something about are the same. Following Imbens and Rubin (2015), I refer to the first approach as the super-population perspective and the second case the finite sample perspective.\nWe have a (super) population of \\(N\\) users. In the example I’m going to use throughout, these are all of our iOS-app users in the UK.\nFrom this super population, we sample \\(n\\) users to be part of the experiment, and then allocate \\(n_t\\) users to treatment and \\(n_c\\) users to control.\nThe way we sample users into the experiment and allocate them to treatment has implications for our statistical analysis, so let’s have a look at the details.\nOnline experiments usually use the following sampling approach to determine whether a user is part of an experiment:\n\nCreate a hash string unique to each user such as &lt;user_id&gt;&lt;experiment_id&gt;&lt;market&gt;.\nFeed the hash string into a hash algorithm (often MD5) and receive a hash value.\nUse the hash value to determine whether a user is part of the experiment. Say we allocate 10% of traffic to the experiment. We then include the user only if their hash value falls within the bottom (or top) 10% of possible hash values.\n\nWith this approach, the probability that a user is sampled into the experiment is independent of the sampling decisions of all other users.\nEach of our \\(N\\) users is part of the experiment experiment with probability \\(n/N\\) and we write \\(R_i = 1\\) if user \\(i\\) is part of the experiment sample and \\(R_i = 0\\) if they aren’t. Note that I take \\(n\\) as given here. For each user in the super population, being part of the experiment sample is a Bernoulli trial with \\(R_i \\sim \\text{Bernoulli}(n/N)\\) and, hence, \\(\\mathbb{E}[{R_i}] = n/N\\) and \\(\\mathbb{V}(R_i) = n/N(1-n/N)\\). I assume here that we know \\(n\\). The reason for doing so is twofold. First, it makes the math easier as it spares us from modeling \\(n\\) as a Binomial random variable. Second, by the time we analyse the data, we do know \\(n\\).\nFor each user in the super population, being part of the experiment sample is a Bernoulli trial, as is being part of the treatment group for each user in the experiment sample. \\[\n\\begin{align}\nR_i \\sim \\text{Bernoulli}(p) \\quad &\\text{with} \\quad \\mathbb{E}[R_i] = p \\\\\nW_i \\sim \\text{Bernoulli}(q) \\quad &\\text{with} \\quad \\mathbb{E}[W_i] = q\n\\end{align}\n\\]\nThe total number of users in the sample is \\(n = \\sum_{i = 1}^{N}R_i\\).\nEach of the \\(n\\) users in our sample is allocated to the treatment condition with probability \\(q\\), and we write \\(W_i = 1\\) if user \\(i\\) is in the treatment group and \\(W_i = 0\\) if they are in the control group.\nThe total number of users in the treatment group is \\(n_t = \\sum_{i = 1}^{N} W_i\\), and the total number of users in the control group is \\(n_c = \\sum_{i = 1}^{N} (1 - W_i)\\).\nGiven this setup, we have: \\[\n\\begin{align}\n\\mathbb{E}[R_i] &= p \\\\\n\\mathbb{E}[W_i] &= q \\\\\n\\mathbb{E}[n] &= Np \\\\\n\\mathbb{E}[n_t] &= nq \\\\\n\\mathbb{E}[n_c] &= n(1 - q) \\\\\n\\end{align}\n\\]\n(we will use this in the unbiasedness proof below)\nCompare this to a setup where treatment allocation is-non independent: This is easiest to see in contrast to experiments where neither of these decisions are independent, as is often the case in lab and field experiments in social science, but also medical experiments. There, we typically recruit a pre-determined number of units into our experiment, so that the inclusion of any given unit lowers the probability of inclusion for all others. Similarly, for treatment assignment, we would often use a completely randomised assignment, whereby we ensure that exactly \\(N_t\\) units end up in the treatment group. This, again means that assigning any given unit to treatment lowers the probability of receiving the treatment for all other units.\n\nSuper population Estimator\n\nA natural estimator is …\nWe can write our treatment effect estimator, \\(\\hat{\\tau}\\) in terms of the super-population as\n\n\\[\n\\hat{\\tau} = \\frac{1}{n_t}\\sum_{i=1}^{N} R_i W_i Y_i - \\frac{1}{n_c}\\sum_{i=1}^{N} R_i (1 - W_i) Y_i\n\\]\nUnbiasedness of \\(\\hat{\\tau}\\)\n\nUse derivation in wager2024causal page 6, which is very transparent!\nFirst, note that using ?eq-yi we can write:\n\n\\[\n\\begin{align}\n\\frac{1}{n_t}\\sum_{i=1}^{N} R_i W_i Y_i &= \\frac{1}{n_t}\\sum_{i=1}^{N} R_i W_i Y_i(1) \\\\\n\\frac{1}{n_c}\\sum_{i=1}^{N} R_i (1 - W_i) Y_i &= \\frac{1}{n_t}\\sum_{i=1}^{N} R_i(1 - W_i) Y_i(0)\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nDerivation\n\\[\n\\begin{align}\n\\frac{1}{n_t}\\sum_{i=1}^{N} R_i W_i Y_i\n&= \\frac{1}{n_t}\\sum_{i=1}^{N} R_i W_i \\Bigl(W_i Y_i(1) + (1 - W_i) Y_i(0)\\Bigr) \\\\\n&= \\frac{1}{n_t}\\sum_{i=1}^{N} \\Bigl(R_i W_i W_i Y_i(1) + R_i W_i (1 - W_i) Y_i(0)\\Bigr) \\\\\n&= \\frac{1}{n_t}\\sum_{i=1}^{N} R_i W_i W_i Y_i(1) \\\\\n&= \\frac{1}{n_t}\\sum_{i=1}^{N} R_i W_i Y_i(1) \\\\\n\\frac{1}{n_c}\\sum_{i=1}^{N} R_i (1 - W_i) Y_i\n&= \\frac{1}{n_t}\\sum_{i=1}^{N} R_i (1 - W_i) \\Bigl(W_i Y_i(1) + (1 - W_i) Y_i(0)\\Bigr) \\\\\n&= \\frac{1}{n_t}\\sum_{i=1}^{N} \\Bigl(R_i(1 - W_i) W_i Y_i(1) + R_i(1 - W_i) (1 - W_i) Y_i(0)\\Bigr) \\\\\n&= \\frac{1}{n_t}\\sum_{i=1}^{N} R_i(1 - W_i) (1 - W_i) Y_i(0) \\\\\n&= \\frac{1}{n_t}\\sum_{i=1}^{N} R_i(1 - W_i) Y_i(0)\n\\end{align}\n\\]\n\n\n\n\nWe can now show that \\(\\hat{\\tau}\\) in unbiased, that \\(\\mathbb{E}[\\hat{\\tau}] = \\tau\\).\nWe have two sources of randomness, one due to random sampling and one due to random allocation to treatment. Using the law of iterated expectations, we can write:\n\n\\[\n\\mathbb{E}[\\hat{\\tau}] = \\mathbb{E}_{sp}[\\mathbb{E}_W{\\hat{\\tau}|R}]].\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nLaw of iterated expectations\n\nThat is, we can first take the expectation over the randomisation distribution while talking the vector of sampling allocation indicators, \\(R\\), as given, and then take the expectation over the sampling distribution. \\[\n\\begin{align}\n...\n\\end{align}\n\\]\n\n\n\n\n\nIn both of these steps, we implicitly also condition on the vectors of potential outcomes in the super population, \\(Y_{sp}(0), Y_{sp}(1)\\), which we consider fixed and take as given. I don’t condition on these explicitly to keep the notation light.1\nTODO: condition on \\(Y\\), as a shorthand. Adapt notation below. Also, consider using bf for vectors and matrices for clarity. Probably do it!\nThe inner expectation is equal to:\n\n\\[\n\\begin{align}\n\\EW{\\hat{\\tau} | R}\n&= \\EW{\\frac{1}{n_t}\\sum_{i=1}^{N} R_i W_i Y_i - \\frac{1}{n_c}\\sum_{i=1}^{N} R_i (1 - W_i) Y_i \\&gt;\\Bigg|\\&gt; R} \\vs\n&= \\EW{\\frac{1}{n_t}\\sum_{i=1}^{N} R_i W_i Y_i(1) - \\frac{1}{n_c}\\sum_{i=1}^{N} R_i (1 - W_i) Y_i(0) \\&gt;\\Bigg|\\&gt; R} \\vs\n&= \\EW{\\sum_{i=1}^{N} R_i \\Biggl(\\frac{W_i Y_i(1)}{n_t} - \\frac{(1 - W_i) Y_i(0)}{n_c}\\Biggr)\\&gt;\\Bigg|\\&gt; R} \\vs\n&= \\sum_{i=1}^{N} R_i \\EW{\\frac{W_i Y_i(1)}{n_t} - \\frac{(1 - W_i) Y_i(0)}{n_c}} \\vs\n&= \\sum_{i=1}^{N} R_i \\Biggl(\\frac{\\EW{W_i} Y_i(1)}{\\EW{n_t}} - \\frac{(1 - \\EW{W_i}) Y_i(0)}{\\EW{n_c}}\\Biggr) \\vs\n&= \\sum_{i=1}^{N} R_i \\Biggl(\\frac{q Y_i(1)}{Nq} - \\frac{(1 - q) Y_i(0)}{N(1-q)}\\Biggr) \\vs\n&= \\sum_{i=1}^{N} R_i \\Biggl(\\frac{Y_i(1)}{N} - \\frac{Y_i(0)}{N}\\Biggr) \\vs\n&= \\frac{1}{N}\\sum_{i=1}^{N} R_i \\bigl(Y_i(1) - Y_i(0)\\bigr) \\vs\n&= \\tau_{fs}\n\\end{align}\n\\]\n\nThe outer expectation is equal to:\n\n\\[\n\\begin{align}\n\\mathbb{E}[\\hat{\\tau}]\n&= \\Esp{\\EW{\\hat{\\tau}|R}} \\\\\n&= \\Esp{\\tau_{fs}} \\\\\n&= \\Esp{\\frac{1}{N}\\sum_{i=1}^{N} R_i \\bigl(Y_i(1) - Y_i(0)\\bigr)} \\\\\n&= \\frac{1}{\\Esp{N}}\\sum_{i=1}^{N} \\Esp{R_i} \\bigl(Y_i(1) - Y_i(0)\\bigr) \\\\\n&= \\frac{1}{Np}\\sum_{i=1}^{N} p \\bigl(Y_i(1) - Y_i(0)\\bigr) \\\\\n&= \\frac{1}{N}\\sum_{i=1}^{N}\\bigl(Y_i(1) - Y_i(0)\\bigr) \\\\\n&= \\tau\n\\end{align}\n\\]\nVariance of \\(\\hat{\\tau}\\)\n\nUsing the law of total variance2, we can write \\(\\V{\\hat{\\tau}}\\) as\n\n\\[\n\\begin{align}\n\\V{\\hat{\\tau}}\n&= \\Esp{\\VW{\\hat{\\tau} | R}} + \\Vsp{\\EW{\\hat{\\tau} | R}}\n\\end{align}\n\\]\n\n\n\n\nImbens, Guido W, and Donald B Rubin. 2015. Causal Inference in Statistics, Social, and Biomedical Sciences. Cambridge University Press.\n\n\nLarsen, Nicholas, Jonathan Stallrich, Srijan Sengupta, Alex Deng, Ron Kohavi, and Nathaniel T Stevens. 2023. “Statistical Challenges in Online Controlled Experiments: A Review of a/b Testing Methodology.” The American Statistician, 1–15.\n\n\nNordin, Mattias, and Mårten Schultzberg. 2024. “Precision-Based Designs for Sequential Randomized Experiments.” arXiv Preprint arXiv:2405.03487.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Experiment setup</span>"
    ]
  },
  {
    "objectID": "chapters/experiment_setup.html#footnotes",
    "href": "chapters/experiment_setup.html#footnotes",
    "title": "18  Experiment setup",
    "section": "",
    "text": "If we were to explicitly condition on potential outcomes, we’d get: \\[\n  \\begin{align}\n  \\mathbb{E}[\\hat{\\tau}]\n  &=\\Esp{\\EW{\\hat{\\tau}|R, Y_{sp}(0), Y_{sp}(1)} | Y_{sp}(0), Y_{sp}(1)}\n  \\end{align}\n  \\]↩︎\nIn general, the Law of total variance states that: \\[\n  \\begin{align}\n  \\V{Y} = \\mathbb{E}[\\V{Y|X}] + \\V{\\mathbb{E}[Y|X}]\n  \\end{align}\n  \\] In our case here, conditioning on \\(R\\) means that we take the expectation or variance over the randomisation distribution, which I make explicit with the subscript \\(W\\). The unconditional expectation or variance is taken over the randomisation distribution, which I make explicit using the subscript \\(sp\\). As in the unbiasedness proof above, we are also implicitly conditioning on potential outcomes and I omit making this explicit to keep the notation lighter. Making the conditioning explicit would mean we apply the law to a conditional variance, for which the logic would still hold, and we’d write: \\[\n  \\begin{align}\n  \\V{\\hat{\\tau} | Y_{sp}(0), Y_{sp}(1)}\n  &= \\Esp{\\VW{\\hat{\\tau} | R, Y_{sp}(0), Y_{sp}(1)} | Y_{sp}(0), Y_{sp}(1)} \\\\\n  &+ \\Vsp{\\EW{\\hat{\\tau} | R, Y_{sp}(0), Y_{sp}(1)} | Y_{sp}(0), Y_{sp}(1)}\n  \\end{align}\n  \\]↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Experiment setup</span>"
    ]
  },
  {
    "objectID": "chapters/common_assumptions.html",
    "href": "chapters/common_assumptions.html",
    "title": "19  Excludability",
    "section": "",
    "text": "Observational causal inference requires additional assumptions to identify causal effects. This section briefly explains what they are and how they relate to the assumptions required in experiments.\n\nAnother key assumption, related to no hidden treatment variation, is that assignment to treatment affects outcomes only through the effect of the administration of the treatment – being part of the treatment group does not have an effect on outcomes other than through the treatment itself.\nThis could be violated if treatment units were somehow treated differently from control units (e.g. data collection was different)\nThe assumption is called “excludability” because it assumes that we can exclude from the potential outcome definition separate indicators for treatment assignment and administration. Instead, throughout, we use the indicator \\(w_i\\), which captures whether unit \\(i\\) was allocated to treatment, and assume that this perfectly corresponds to having been administered the treatment.\n\n\n19.0.1 Ignorability\nStates that if treatment assignment is independent of potential outcomes conditional on covariates and observed outcomes, then the assignment mechanism can be ignored for the recovery of causal effects (instead of having to be modeled). Note: assignment doesn’t have to be random for ignorability to hold, just to behave as if it were random given covariates and observed outcomes.\nSection 3.2 in rubin1978bayesian actually suggests that for assignment mechanism to be ignorable, recording mechanism simply needs to record everything upon which assignment depends, which, I suppose, can always be captured by X and Yobs, so that the below notation is comprehensive but may not always be necessary in the sense that in some cases, we only need recording of X or yobs if assignment doesn’t rely on the other of the two. For instance: “play-the-winner” example in paper relies on yobs only, whereas “balance patients characteristics” relies on X only.\nThis is needed to ignore the assignment mechanism in Bayesian posterior inference (see rubin1978bayesian for details).\nFormally: \\[\nP(W|X, Y_i(1), Y_i(0)) = P(W|X, Y_i^{obs}),\n\\] where \\[\nY_i^{obs} = W_iY_i(1) + (1 - W_i)Y_i(0).\n\\]\n\n\n19.0.2 Unconfoundedness\nAka (strong ignorability). Treatment is independent of potential outcomes given covariates. In practice, the following are used interchangeably to refer to this: - Ignorability - Unconfoundedness - Selection on observables - No unmeasured confounding\nThis is stronger than ignorability: it implies ignorability, but the reverse is not true.\nFormally: \\[\nP(W|X, Y_i(1), Y_i(0)) = P(W|X)\n\\]\nNote:\nAs Rubin points out on page 43 in rubin1978bayesian, if we rely on simple random sampling and then conduct a completely randomised experiment, we have\n\\[\nP(W|X, Y_i(1), Y_i(0)) = P(W),\n\\] which implies both ignorability and confoundedness.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Excludability</span>"
    ]
  },
  {
    "objectID": "chapters/lemmas.html",
    "href": "chapters/lemmas.html",
    "title": "20  Lemmas",
    "section": "",
    "text": "20.1 Lemma 1\nGiven that \\(W_i \\in \\{0, 1\\} \\sim \\text{Bernoulli}(q)\\), and given that we take potential outcomes \\(\\mathbf{Y(w)} = (\\mathbf{Y(1)}, \\mathbf{Y(0)})\\) and sample sizes \\(\\mathbf{n} = (n, n_t, n_c)\\) as given, we have: \\[\n\\mathbb{E}[W_i\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}]\n=\n\\frac{n_t}{n}\n\\]\nand\n\\[\n\\mathbb{E}[1-W_i\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}]\n=\n\\frac{n_c}{n}.\n\\] Proof:\nGiven that \\(W_i \\in \\{0, 1\\} \\sim \\text{Bernoulli}(q)\\) we have: \\[\nP(W_i = 1 \\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)})\n=\n\\frac{n_t}{n}.\n\\]Hence: $$ \\[\\begin{align}\n\\mathbb{E}&[W_i\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}]\n\\\\[5pt]\n\n&=\n1 \\times P(W_i = 1 \\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)})\n+ 0 \\times P(W_i = 0 \\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)})\n\\\\[5pt]\n\n&=\nP(W_i = 1 \\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)})\n\\\\[5pt]\n\n&= \\frac{n_t}{n}\n\\end{align}\\] \\[\nand\n\\] \\[\\begin{align}\n\\mathbb{E}&[(1-W_i)\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}]\n\\\\[5pt]\n\n&=\n1 - \\mathbb{E}[W_i\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}]\n\\\\[5pt]\n\n&=\n1 - \\frac{n_t}{n}\n\\\\[5pt]\n\n&=\n\\frac{n - n_t}{n}\n\\\\[5pt]\n\n&=\n\\frac{n_c}{n}\n\\qquad\\square\n\\end{align}\\] $$",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lemmas</span>"
    ]
  },
  {
    "objectID": "chapters/lemmas.html#sec-lemma2",
    "href": "chapters/lemmas.html#sec-lemma2",
    "title": "20  Lemmas",
    "section": "20.2 Lemma 2",
    "text": "20.2 Lemma 2\nGiven that \\(W_i \\sim \\text{Bernoulli} \\left( \\frac{n_t}{n} \\right)\\), we have:\n\\(\\mathbb{V}(W_i^2) = \\mathbb{V}(W_i)\\).\nProof:\nBernoulli distributed random variables take on either values 0 or 1, so \\(W_i \\in \\{0, 1\\}\\), which implies that: \\[\nW_i^2 =\n\\begin{cases}\n1 &\n\\text{if } W_i = 1\\\\[5pt]\n0 &\n\\text{if } W_i = 0\\\\[5pt]\n\\end{cases}\n\\qquad \\implies\nW_i^2 = W_i.\n\\]\nHence:\n\\(\\mathbb{V}(W_i^2) = \\mathbb{V}(W_i)\\qquad\\square\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lemmas</span>"
    ]
  },
  {
    "objectID": "chapters/lemmas.html#sec-lemma3",
    "href": "chapters/lemmas.html#sec-lemma3",
    "title": "20  Lemmas",
    "section": "20.3 Lemma 3",
    "text": "20.3 Lemma 3\n\\[\n\\mathbb{V}(W_i) = \\frac{n_t n_c}{n^2}, \\qquad\n\\text{Cov}(W_i, W_j) = -\\frac{n_t n_c}{n^2(n-1)}\n\\] Proof:\nGiven that \\[\nW_i \\sim \\text{Bern} \\left( \\frac{n_t}{n} \\right),\n\\]\n\\[\n\\begin{align}\n\\mathbb{V}(W_i) &= \\left(\\frac{n_t}{n}\\right) \\left(1-\\frac{n_t}{n}\\right) \\\\[5pt]\n\\mathbb{V}(W_i) &= \\left(\\frac{n_t}{n}\\right) \\left(\\frac{n-n_t}{n}\\right) \\\\[5pt]\n\\mathbb{V}(W_i) &= \\left(\\frac{n_t}{n}\\right) \\left(\\frac{n_c}{n}\\right) \\\\[5pt]\n\\mathbb{V}(W_i) &= \\frac{n_tn_c}{n^2} \\\\[5pt]\n\\end{align}\n\\]\nFrom the basic result that \\(\\mathbb{V}(X + Y) = \\mathbb{V}(X) + \\mathbb{V}(Y) + 2\\text{Cov}(X, Y)\\), and the fact that symmetry implies that the variances and covariances of all \\(W_i\\)s are the same, we get:\n$$ \\[\\begin{align}\n\\mathbb{V}\\left(\\sum_{i=1}^{n}W_i\\right)\n&= \\sum_{i=1}^{n}\\mathbb{V}(W_i) + 2\\sum_{i&lt;j}^{n}\\text{Cov}(W_i, W_j)\n& \\\\[5pt]\n\n\\mathbb{V}\\left(\\sum_{i=1}^{n}W_i\\right)\n&= n\\mathbb{V}(W_i) + 2\\frac{n(n-1)}{2}\\text{Cov}(W_i, W_j)\n& \\text{symmetry} \\\\[5pt]\n\n\\mathbb{V}\\left(n_t\\right)\n&= n\\mathbb{V}(W_i) + 2\\frac{n(n-1)}{2}\\text{Cov}(W_i, W_j)\n& \\text{Def of }n_t \\\\[5pt]\n\n0 &= n\\mathbb{V}(W_i) + 2\\frac{n(n-1)}{2}\\text{Cov}(W_i, W_j)\n& n_t\\text{ is constant} \\\\[5pt]\n\n0 &= n\\left(\\frac{n_tn_c}{n^2}\\right) + 2\\frac{n(n-1)}{2}\\text{Cov}(W_i, W_j)\n& \\text{Result for } \\mathbb{V}(W_i) \\\\[5pt]\n\n0 &= \\left(\\frac{n_tn_c}{n}\\right) + n(n-1)\\text{Cov}(W_i, W_j)\n& \\text{} \\\\[5pt]\n\n\\text{Cov}(W_i, W_j) &= -\\frac{n_tn_c}{n^2(n-1)}& \\text{}\n\\qquad\\square\n\\end{align}\\] $$",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lemmas</span>"
    ]
  },
  {
    "objectID": "chapters/lemmas.html#sec-lemma4",
    "href": "chapters/lemmas.html#sec-lemma4",
    "title": "20  Lemmas",
    "section": "20.4 Lemma 4",
    "text": "20.4 Lemma 4\n\\[\n-\\sum_{i=1}^{n}\\sum_{j \\neq i}^{n}\n(Y_i^+ - \\overline{Y}^+)(Y_j^+ - \\overline{Y}^+)\n= \\sum_{i=1}^{n}(Y_i^+ - \\overline{Y}^+)^2\n\\] Proof:\nThe sum of demeaned variables is zero. Hence, \\(\\sum_{i=1}^{n}(Y_i^+ - \\overline{Y}^+) = 0\\), which implies that:\n$$ \\[\\begin{align}\n0\n&= \\sum_{i=1}^{n}(Y_i^+ - \\overline{Y}^+)\n\\sum_{j=1}^{n}(Y_j^+ - \\overline{Y}^+)\n\\\\[5pt]\n\n&= \\sum_{i=1}^{n}\\sum_{j=1}^{n}\n(Y_i^+ - \\overline{Y}^+)(Y_j^+ - \\overline{Y}^+)\n\\\\[5pt]\n\n&= \\sum_{i=1}^{n}(Y_i^+ - \\overline{Y}^+)^2\n+\\sum_{i=1}^{n}\\sum_{j \\neq i}^{n}\n(Y_i^+ - \\overline{Y}^+)(Y_j^+ - \\overline{Y}^+)\n\\\\[5pt]\n\n-\\sum_{i=1}^{n}\\sum_{j \\neq i}^{n}\n(Y_i^+ - \\overline{Y}^+)(Y_j^+ - \\overline{Y}^+)\n&= \\sum_{i=1}^{n}(Y_i^+ - \\overline{Y}^+)^2\n\\end{align}\\] $$ ## Lemma 5 {#sec-lemma5}\n\\[\n2S_{0,1} = S^2_1 + S^2_0 - S^2_{\\tau_i}\n\\] Proof:\n$$ \\[\\begin{align}\nS_{\\tau_i}^2\n\n&= \\frac{1}{n-1}\\sum_{i=1}^{n}\n\\left(\nY_i(1) - Y_i(0)\n- \\left(\\overline{Y}(1) - \\overline{Y}(0)\\right)\n\\right)^2\n\\\\[5pt]\n\n&= \\frac{1}{n-1}\\sum_{i=1}^{n}\n\\left(\n\\left(Y_i(1) - \\overline{Y}(1)\\right)\n- \\left(Y_i(0) - \\overline{Y}(0)\\right)\n\\right)^2\n\\\\[5pt]\n\n&= \\frac{1}{n-1}\\sum_{i=1}^{n}\n\\left(\n\\left(Y_i(1) - \\overline{Y}(1)\\right)^2\n+ \\left(Y_i(0) - \\overline{Y}(0)\\right)^2\n- 2\\left(Y_i(1) - \\overline{Y}(1)\\right)\\left(Y_i(0) - \\overline{Y}(0)\\right)\n\\right)\n\\\\[5pt]\n\n&=\n\\frac{1}{n-1}\\sum_{i=1}^{n}\n\\left(Y_i(1) - \\overline{Y}(1)\\right)^2\n+ \\frac{1}{n-1}\\sum_{i=1}^{n}\n\\left(Y_i(0) - \\overline{Y}(0)\\right)^2\n- 2\\frac{1}{n-1}\\sum_{i=1}^{n}\n\\left(Y_i(1) - \\overline{Y}(1)\\right)\\left(Y_i(0) - \\overline{Y}(0)\\right)\n\\\\[5pt]\n\n&=\nS^2_1 + S^2_0 - 2S_{0, 1}\n\\\\[5pt]\n\n2S_{0, 1} &= S^2_1 + S^2_0 - S^2_{\\tau_i}\n\\end{align}\\] $$",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Lemmas</span>"
    ]
  },
  {
    "objectID": "chapters/outcomes_and_estimands.html",
    "href": "chapters/outcomes_and_estimands.html",
    "title": "21  Causal estimands",
    "section": "",
    "text": "21.1 Quantile treatment effects",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Causal estimands</span>"
    ]
  },
  {
    "objectID": "chapters/outcomes_and_estimands.html#quantile-treatment-effects",
    "href": "chapters/outcomes_and_estimands.html#quantile-treatment-effects",
    "title": "21  Causal estimands",
    "section": "",
    "text": "Diff in quantiles between T and C observed outcome distributions, not quantiles of \\(Y_i(1) - Y_i(0)\\).\nsee 4.3 in athey2017econometrics",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Causal estimands</span>"
    ]
  },
  {
    "objectID": "chapters/outcomes_and_estimands.html#treatment-effects",
    "href": "chapters/outcomes_and_estimands.html#treatment-effects",
    "title": "21  Causal estimands",
    "section": "21.2 Treatment effects",
    "text": "21.2 Treatment effects\nATE is the average difference in potential outcomes of all subjects. E[y1 - y0]\nATE is a weighted average of ATET and average treatment effect of untreated.\nATE is also a weighted average of the compliers, always-takers, and never-takers.\nATE = ATET if treatment effects are homogenous (if they are not, then experiment measures ATET).\nATE = ITT under homogenous effects or under perfect compliance.\nATET (or TOT) is the average difference in potential outcomes of treated subjects. E[y1 - y0 | D=1] (D is treatment)\nATET = LATE in an experiment with no always-takers. This is relevant because there are many cases of onesided non-compliance (if participants who are randomised to treatment do not take it up but nobody who wasn’t randomised to treatment has access).\nITT is the average difference in potential outcomes of those assigned to treatment (a weighted average between those who accepted treatment and those who didn’t).\nLATE is the average difference in potential outcomes of compliers.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Causal estimands</span>"
    ]
  },
  {
    "objectID": "chapters/culture.html",
    "href": "chapters/culture.html",
    "title": "6  Culture",
    "section": "",
    "text": "Thomke (2020) on how to build an experimentation culture, based on Booking.com\nBojinov, Chen, and Liu (2020) section 3 on how to scale quasi-experiments, based on LinkedIn\n\n\n\n\n\nBojinov, Iavor, Albert Chen, and Min Liu. 2020. “The Importance of Being Causal.” Harvard Data Science Review 2 (3): 6.\n\n\nThomke, Stefan. 2020. “Building a Culture of Experimentation.” Harvard Business Review 98 (2): 40–47.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Culture</span>"
    ]
  },
  {
    "objectID": "chapters/principles.html",
    "href": "chapters/principles.html",
    "title": "8  Guiding principles",
    "section": "",
    "text": "Remember Twyman’s Law: if a result looks interesting it is probably a mistake – don’t invent a story, investigate!",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Guiding principles</span>"
    ]
  }
]