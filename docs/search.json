[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Online experiments",
    "section": "",
    "text": "Welcome\nThe aim of this book is to collect all I know about online experiments. I do this mainly for personal reference because I forget stuff. But if you find the notes helpful, find any errors, or have any suggestions, please get in touch by writing to fa.gunzinger@gmail.com.\nMy aim is to capture the material I need as concisely as I can. There are many excellent resources out there that explain concepts more elaborately (e.g. here, here).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html",
    "href": "chapters/stats_of_online_experiments.html",
    "title": "1  The stats of online experiments",
    "section": "",
    "text": "1.1 Setup\nWe study a sample of \\(n\\) units, indexed by \\(i = 1, \\dots, n\\), to learn about the effect of a binary treatment on these units.1 The sample of units might be all visitors to an e-commerce app and the treatment a new UX feature. The treatment is “binary” because we only consider two treatment conditions: a unit either experiences the active treatment and is exposed to the new feature or experiences the control treatment and is exposed to the status-quo. We often refer to the two treatment conditions simply as “treatment” and “control”.\nEach unit has two potential outcomes: \\(Y_i(1)\\) is the outcome for unit \\(i\\) if they are in treatment and \\(Y_i(0)\\) is the outcome if they are in control. To simplify notation, we collect all unit-level potential outcomes in the \\(n \\times 1\\) vectors \\(\\mathbf{Y(1)}\\) and \\(\\mathbf{Y(0)}\\). These outcomes are “potential outcomes” because before the start of the experiment, each unit could be exposed to either treatment condition so that they can potentially experience either outcome. Once the experiment has started and units are assigned to treatment, only one of the two outcomes will be observed.\nThe causal effect of the treatment for unit \\(i\\) is the difference between the two potential outcomes:2 \\[\n\\tau_i = Y_i(1) - Y_i(0).\n\\] Because a unit can only ever be in either treatment or control, we can only ever observe one of the two potential outcomes, which means that directly observing unit-level treatment effects is impossible. This is the fundamental problem of causal inference (Holland 1986).\nAn experiment is one solution to the fundamental problem:3 randomly assigning units from a population to either treatment or control allows us to estimate average (unit-level) treatment effects. In the words of Holland (1986, 947):4\nHence, instead of trying to observe unit-level causal effects, the quantity of interest – the estimand – in an experiment is an average across a sample of units. In particular, we are usually interested in the effect of a universal policy, a comparison between a state of the world where everyone is exposed to the treatment and one where nobody is. While we can capture the difference between these two states of the world in many different ways, we typically focus on the difference in the averages of all these unit-level causal effects over the entire sample:\n\\[\n\\begin{align}\n\\tau\n= \\frac{1}{n}\\sum_{i=1}^n \\left(Y_i(1) - Y_i(0)\\right)\n= \\frac{1}{n}\\sum_{i=1}^n Y_i(1) - \\frac{1}{n}\\sum_{i=1}^nY_i(0).\n\\end{align}\n\\tag{1.1}\\]\nThis is the estimand, the statistical quantity we are trying to estimate in our experiment.\nRunning an experiment with our \\(n\\) units means that we randomly assign some units to treatment and some to control. We use the binary treatment indicator \\(W_i \\in \\{0, 1\\}\\) to indicate treatment exposure for unit \\(i\\) and write \\(W_i = 1\\) if they are in treatment and \\(W_i = 0\\) if they are in control. We collect all unit-level treatment indicators in the \\(n \\times 1\\) vector \\(\\mathbf{W} = (W_1, W_2, \\dots, W_n)'\\). At the end of the experiment, we have \\(n_t = \\sum_{i=1}^n W_i\\) units in treatment and the remaining \\(n_c = \\sum_{i=1}^n (1-W_i)\\) units in control. For each unit, we observe outcome \\(Y_i\\).\nTo estimate \\(\\tau\\), we use the observed difference in means between the treatment and control units:5\n\\[\n\\begin{align}\n\\hat{\\tau}^{\\text{dm}}\n=\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i - \\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\end{align}\n\\]\nThis is our estimator, the algorithm we use to produce estimates of the estimand.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#setup",
    "href": "chapters/stats_of_online_experiments.html#setup",
    "title": "1  The stats of online experiments",
    "section": "",
    "text": "“The important point is that [an experiment] replaces the impossible-to-observe causal effect of [a treatment] on a specific unit with the possible-to-estimate average causal effect of [the treatment] over a population of units.”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#assignment-mechanism",
    "href": "chapters/stats_of_online_experiments.html#assignment-mechanism",
    "title": "1  The stats of online experiments",
    "section": "1.2 Assignment mechanism",
    "text": "1.2 Assignment mechanism\nThe procedure we use the allocate units to treatment conditions is the assignment mechanism. In online experiments, we typically assign units to treatment conditions dynamically as they visit our site and use an assignment mechanism where the assignment of each unit is determined by a process that is equivalent to a coin-toss, such that \\(P(W_i) = q\\), where \\(q \\in [0, 1]\\). Throughout, I’ll focus on the most common case where \\(q=\\frac{1}{2}\\), so that we have:\n\\[\nP(W_i = 1) = P(W_i = 0) = \\frac{1}{2}.\n\\] Because of their coin-toss-like nature assignments follow a Bernoulli distribution and the type of experiment is called a Bernoulli Randomised Experiment. Formally, we have: \\[\n\\begin{align}\nW_i &\\sim \\text{Bernoulli}(1/2) \\\\\n\\mathbb{E}[{W_i}] &= 1/2 \\\\\n\\end{align}\n\\] and \\[\n\\begin{align}\nn_t &\\sim \\text{Binomial}(n, 1/2) \\\\\n\\mathbb{E}[{n_t}] &= n(1/2).\n\\end{align}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#analysis",
    "href": "chapters/stats_of_online_experiments.html#analysis",
    "title": "1  The stats of online experiments",
    "section": "1.3 Analysis",
    "text": "1.3 Analysis\nThere are different approaches we could take to formally analyse our experiment.\nDecisions:\n\nWe could either take a superpopulation or fixed sample perspective. Because for all but the largest companies, most online experiments are eventually run on the entire population of interest, I focus on the latter. This means that the goal of our experiment is to estimate the average treatment effect of the treatment on our \\(n\\) units, rather than using the estimate for our \\(n\\) units to infer the average treatment effect on a larger population from which the \\(n\\) units are drawn. I thus use a fully design-based approach (see ?sec-experiment-setup for details)\nWe can analyse the Bernoulli Randomised Experiment treating \\(n_t\\) as a Binomial random variable or taking it as given. Because by the time of the analysis it is, in fact, given, I follow the latter approach. This approach also has the advantage of considerably simplifying the math.\n\nImplications:\n\nOur sample of \\(n\\) units and the associated potential outcomes \\(\\mathbf{Y(1)}\\) and \\(\\mathbf{Y(0)}\\) are fixed (because units are non-random but determined by sample I have). I refer to the potential outcomes collectively as \\(\\mathbf{Y(w)} = (\\mathbf{Y(1)}, \\mathbf{Y(0)})\\).\nOnce randomisation is complete the number of units in treatment and control, \\(n_t\\) and \\(n_c\\) are given. I refer to them collectively as \\(\\mathbf{n} = (n_t, n_c)\\).\nThe assignment mechanism is also such that units treatment assignment is independent of the treatment assignment of all other units.\n\nIn the next two sections we show that \\(\\hat{\\tau}^{\\text{dm}}\\) is an unbiased estimator of \\(\\tau\\) and calculate its variance. (My approach is based on Ding (2023). For an alternative, see Appendix 6.B. in Imbens and Rubin (2015).)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#unbiasedness",
    "href": "chapters/stats_of_online_experiments.html#unbiasedness",
    "title": "1  The stats of online experiments",
    "section": "1.4 Unbiasedness",
    "text": "1.4 Unbiasedness\nAn estimator is unbiased if its expected value equals the estimand. To show that the difference in means estimator is unbiased we thus have to show that:\n$$ \\[\\begin{align}\n\\mathbb{E}\\left[\n\\hat{\\tau}^{\\text{dm}}\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\n&=\n\\tau.\n\\end{align}\\] $$ Given our definitions above this is means showing that\n$$ \\[\\begin{align}\n\\mathbb{E}\\left[\n\\hat{\\tau}^{\\text{dm}}\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\n&=\n\\mathbb{E}\\left[\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i - \\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\\\\[5pt]\n\n&=\n\\mathbb{E}\\left[\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n- \\mathbb{E}\\left[\n\\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\\\\[5pt]\n\n&\n\\enspace\n\\overset{!}{=}\n\\enspace\n\\frac{1}{n}\\sum_{i=1}^nY_i(1) - \\frac{1}{n}\\sum_{i=1}^nY_i(0)\n\\\\[5pt]\n\n&=\n\\tau\n\\end{align}\\] $$\nThere are two pieces we need for this:\n\nLink observed to potential outcomes so that \\(Y_i = Y_i(W_i)\\). This requires the Stable Unit Treatment Value Assumption (SUTVA).\nLink treatment group averages to sample averages so that \\(\\mathbb{E}\\left[\\frac{1}{n_t}\\sum_{W_i=w}Y_i(w)\\right] = \\frac{1}{n}\\sum_{i=1}^{n}Y_i(w)\\). This requires randomisation.\n\nLet’s tackle them in turn.\n\n1.4.1 SUTVA links observed outcomes to potential outcomes\nWe want to learn something about unit-level differences in potential outcomes \\[\n\\tau_i = Y_i(1) - Y_i(0),\n\\] where a potential outcome is a unit-indexed function of the treatment level, with the treatment levels in our case simply being “treatment” and “control”, captured by the assignment indicator \\(W_i \\in {0, 1}\\).\nNow say we run an experiment and unit \\(i\\) is allocated to treatment. The \\(i\\)-th element of the assignment vector \\(\\mathbf{W}\\) of that experiment is thus \\(W_i = 1\\), and we refer to this assignment vector as \\(\\mathbf{W}^{(i=1)}\\).\nWe know we can’t observe both \\(Y_i(0)\\) and \\(Y_i(1)\\), but given that \\(i\\) is in treatment we can observe the potential outcome under treatment, \\(Y_i(1)\\), which will help us estimate \\(\\hat{\\tau}\\). So we need\n\\[\nY_i = Y_i(1).\n\\]\nWhat we directly observe, however, is \\[\nY_i = Y_i\\left(\\mathbf{W}^{(i=1)}\\right).\n\\] In words: the outcome we observe for unit \\(i\\) in our experiment is not the potential outcome for \\(i\\) under treatment, but the potential outcome for \\(i\\) under the specific assignment vector of our experiment, \\(\\mathbf{W}^{(i=1)}\\). What does this mean concretely? It means that the observed outcome is a function not only of \\(i\\)’s treatment assignment but of:\n\nThe assignment of all units in the experiment\nThe precise form of the assigned treatment level received by \\(i\\)\nThe way in which said assigned treatment level is administered to \\(i\\)\n\nWe need: \\[\nY_i = Y_i\\left(\\mathbf{W}^{(i=1)}\\right) \\overset{!}{=} Y_i(1).\n\\] The only way to make progress is to assume what we need: that potential outcomes for unit \\(i\\) are a function only of the treatment level unit \\(i\\) itself receives and independent of (i) treatment assignment of other units and (ii) the form and administration of the treatment level. (i) above is referred to as “no interference” in the literature, (ii) as “no hidden variations of treatment”.\nThat assumption is SUTVA, the Stable Unit Treatment Value Assumption. It ensures that the potential outcomes for each unit and each treatment level are well-defined functions of the unit index and the treatment level only – that for a given unit and treatment level, the potential outcome is well-defined and, thus, “stable” (Rubin 1980).\nNote that (ii) does not mean that a treatment level has to take the same form for all units, even though it is often misinterpreted to mean that. What we need is that \\(Y_i(W_i)\\) is a clearly defined function for all \\(i\\) and all possible treatment levels \\(W_i \\in \\{0, 1\\}\\). For this to be the case, it must be the case that there are no different forms of possible treatments, the potential outcome for each for is the same so that the differences are irrelevant, or that the experienced form is random so that the expected outcome across all units remains stable.\nIn the context of our e-commerce app experiment, the non-interference part of SUTVA means that potential outcomes for all customers are independent of treatment assignment of all other customers – very much including family members and friends and the no-hidden-variation part means that there their precise experience is pinned down by their mobile device and remains stable over time: there are no accidental server-side bugs that creates different background colours and the it is either clear whether a customer uses an Android or iOS app (or the experience is identical). But, again, SUTVA does not require that the active treatment looks exactly the same on iOS and Android apps just that for each unit in our experiment, their experience if they are part of the active treatment is pinned down.\nWhy is this important? We want to know what happened if we rolled out our policy to everyone compared to if we didn’t roll it out to anyone. To have any hope of estimating this we can’t have treatment level’s vary over time or depending on circumstances, but need them to be pinned down for each unit. (In the context of Tech, this would mean that the experience of a feature for a given user is pinned down by, say, the size of their phone screen and the app version they use, which, by and large, is plausible.)\nSUTVA is a strong assumption and can be violated in a number of ways. I’ll discuss these, together with solutions, in ?sec-threats-to-validity.\nIf SUTVA holds we have:\n\\[\nY_i = Y_i(W_i) = \\begin{cases}\n   Y_i(1) & \\text{if } W_i = 1 \\\\\n   Y_i(0)       & \\text{if } W_i = 0,\n  \\end{cases}\n\\] or, more compactly: \\[\nY_i = W_iY_i(1) + (1 - W_i)Y_i(0).\n\\]\nThis is the link between observed and potential outcomes we need, and makes clear that we observe \\(Y_i(1)\\) for units in treatment and \\(Y_i(0)\\) for units in control.\nIn our unbiasedness proof, we now have $$ \\[\\begin{align}\n\\mathbb{E}\\left[\n\\hat{\\tau}^{\\text{dm}}\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\n&=\n\\mathbb{E}\\left[\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i - \\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\\\\[5pt]\n\n&=\n\\mathbb{E}\\left[\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n- \\mathbb{E}\\left[\n\\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\\\\[5pt]\n\n&\\text{SUTVA}\n\\\\[5pt]\n\n&=\n\\mathbb{E}\\left[\\frac{1}{n_t}\\sum_{W_i=1}Y_i(1)\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\\right]\n- \\mathbb{E}\\left[\\frac{1}{n_c}\\sum_{W_i=0}Y_i(0)\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\\right]\n\\\\[5pt]\n\n&\n\\enspace\n\\overset{!}{=}\n\\enspace\n\\frac{1}{n}\\sum_{i=1}^nY_i(1) - \\frac{1}{n}\\sum_{i=1}^nY_i(0)\n\\\\[5pt]\n\n&=\n\\tau\n\\end{align}\\] $$\nWhat remains is to show that \\(\\mathbb{E}\\left[\\frac{1}{n_t}\\sum_{W_i=w}Y_i(w)\\right] = \\frac{1}{n}\\sum_{i=1}^{n}Y_i(w)\\). This requires randomisation.\n\n\n1.4.2 Randomisation links treatment groups to the population\nSteps:\n\nWe use the definition of \\(W_i\\) to write \\(\\sum_{W_i=1}Y_i(1)\\) as \\(\\sum_{i=1}^{n} W_iY_i(1)\\) and similarly for control units.\nWe use the linearity of \\(\\mathbb{E}\\) to move \\(\\mathbb{E}\\) inside the summation, where the only random element is \\(W_i\\).\nUse Lemma 1 (randomisation) to replace expectations with expected values.\n\n$$ \\[\\begin{align}\n\\mathbb{E}\\left[\n\\hat{\\tau}^{\\text{dm}}\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\n&=\n\\mathbb{E}\\left[\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i - \\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\\\\[5pt]\n\n&=\n\\mathbb{E}\\left[\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n- \\mathbb{E}\\left[\n\\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\\\\[5pt]\n\n&\\text{SUTVA}\n\\\\[5pt]\n\n&=\n\\mathbb{E}\\left[\\frac{1}{n_t}\\sum_{W_i=1}Y_i(1)\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\\right]\n- \\mathbb{E}\\left[\\frac{1}{n_c}\\sum_{W_i=0}Y_i(0)\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\\right]\n\\\\[5pt]\n\n&=\n\\mathbb{E}\\left[\n\\frac{1}{n_t}\\sum_{i=1}^{n}W_iY_i(1)\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n-\n\\mathbb{E}\\left[\n\\frac{1}{n_c}\\sum_{i=1}^{n}(1-W_i)Y_i(0)\n\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}\n\\right]\n\\\\[5pt]\n\n&=\n\\frac{1}{n_t}\\sum_{i=1}^{n}\n\\mathbb{E}[W_i\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}]\nY_i(1)\n-\n\\frac{1}{n_c}\\sum_{i=1}^{n}\n\\mathbb{E}[1-W_i\\&gt;|\\&gt;\\mathbf{n}, \\mathbf{Y(w)}]\nY_i(0)\n\\\\[5pt]\n\n&\\text{Randomisation (Lemma 1)}\n\\\\[5pt]\n\n&=\n\\frac{1}{n_t}\\sum_{i=1}^{n}\n\\left(\\frac{n_t}{n}\\right)\nY_i(1)\n-\n\\frac{1}{n_c}\\sum_{i=1}^{n}\n\\left(\\frac{n_c}{n}\\right)\nY_i(0)\n\\\\[5pt]\n\n&=\n\\frac{1}{n}\\sum_{i=1}^nY_i(1) - \\frac{1}{n}\\sum_{i=1}^nY_i(0)\n\\\\[5pt]\n\n&=\n\\tau\n\\end{align}\\] $$",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#variance",
    "href": "chapters/stats_of_online_experiments.html#variance",
    "title": "1  The stats of online experiments",
    "section": "1.5 Variance",
    "text": "1.5 Variance\nFor the variance calculation below we need a few more definitions. We can define sample means and variances of the potential outcomes as:\n\\[\n\\begin{align}\n\\overline{Y}(1) = \\frac{1}{n}\\sum_{i=1}^n Y_i(1),\n\\qquad\nS_1^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(Y_i(1) - \\overline{Y}(1)\\right)^2\n\\\\[5pt]\n\\overline{Y}(0) = \\frac{1}{n}\\sum_{i=1}^n Y_i(0),\n\\qquad\nS_0^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(Y_i(0) - \\overline{Y}(0)\\right)^2\n\\\\[5pt]\n\\end{align}\n\\]\nWe have already defined the sample average treatment effect in Equation 1.1. I rewrite it here for convenience and expand the definition using the above expressions:\n\\[\n\\begin{align}\n\\tau\n&= \\frac{1}{n}\\sum_{i=1}^n \\tau_i\n\\\\[5pt]&= \\frac{1}{n}\\sum_{i=1}^n \\left(Y_i(1) - Y_i(0)\\right)\n\\\\[5pt]&= \\frac{1}{n}\\sum_{i=1}^n Y_i(1) - \\frac{1}{n}\\sum_{i=1}^nY_i(0)\n\\\\[5pt]&= \\overline{Y}(1) - \\overline{Y}(0).\n\\end{align}\n\\]\nThe variance of the individual-level causal effects is:\n\\[\n\\begin{align}\nS_{\\tau_i}^2\n&= \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(Y_i(1) - Y_i(0)\n- \\left(\\overline{Y}(1) - \\overline{Y}(0)\\right)\\right)^2\n\\\\[5pt]\n&= \\frac{1}{n-1}\\sum_{i=1}^{n}\\left(\\tau_i - \\tau\\right)^2 \\\\[5pt]\n\\end{align}\n\\] The covariance of potential outcomes is: \\[\n\\begin{align}\nS_{0, 1} &= \\frac{1}{n-1}\\sum_{i=1}^{n}\n\\left(Y_i(1) - \\overline{Y}(1)\\right)\n\\left(Y_i(0) - \\overline{Y}(0)\\right)\n\\end{align}\n\\]\nAll lemmas referred to below are here.\nWe can then calculate the variance as (I do not explicitly condition on \\(\\mathbf{n}\\) and \\(\\mathbf{Y(w)}\\) here to keep the notation lighter):\n$$ \\[\\begin{align}\n\n\\mathbb{V}\\left(\n\\hat{\\tau}^{\\text{dm}}\n\\right)\n\n&=\n\\mathbb{V}\\left(\n\\frac{1}{n_t}\\sum_{W_i=1}Y_i - \\frac{1}{n_c}\\sum_{W_i=0}Y_i\n\\right)\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\n\\frac{1}{n_t}\\sum_{i=1}^n W_iY_i - \\frac{1}{n_c}\\sum_{i=1}^n (1-W_i)Y_i\n\\right)\n\\\\[5pt]\n\n&\\text{SUTVA}\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\n\\frac{1}{n_t}\\sum_{i=1}^n W_iY_i(1) - \\frac{1}{n_c}\\sum_{i=1}^n (1-W_i)Y_i(0)\n\\right)\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\n\\frac{1}{n_t}\\sum_{i=1}^n W_iY_i(1)\n- \\frac{1}{n_c}\\sum_{i=1}^n Y_i(0)\n+ \\frac{1}{n_c}\\sum_{i=1}^n W_iY_i(0)\n\\right)\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\n\\sum_{i=1}^n W_i\\frac{Y_i(1)}{n_t}\n- \\sum_{i=1}^n \\frac{Y_i(0)}{n_c}\n+ \\sum_{i=1}^n W_i\\frac{Y_i(0)}{n_c}\n\\right)\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\n\\sum_{i=1}^n W_i \\left(\\frac{Y_i(1)}{n_t} + \\frac{Y_i(0)}{n_c}\\right)\n- \\sum_{i=1}^n \\frac{Y_i(0)}{n_c}\n\\right)\n\\\\[5pt]\n\n&\\text{Dropping constant term}\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\n\\sum_{i=1}^n W_i \\left(\\frac{Y_i(1)}{n_t} + \\frac{Y_i(0)}{n_c}\\right)\n\\right)\n\\\\[5pt]\n\n&\\text{Demeaning (leaves variance unchanged)}\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\\sum_{i=1}^n W_i \\left(\\frac{Y_i(1)}{n_t} + \\frac{Y_i(0)}{n_c} - \\left(\\frac{\\overline{Y}(1)}{n_t} - \\frac{\\overline{Y}(0)}{n_c}\\right)\n\\right)\\right)\n&\\text{}\n\\\\[5pt]\n\n&\\text{Using shorthands } Y_i^+ = Y_i(1)/n_t + Y_i(0)/n_c \\text{ and } \\overline{Y}^+ = \\overline{Y}(1)/n_t - \\overline{Y}(0)/n_c\n\\\\[5pt]\n\n&=\n\\mathbb{V}\\left(\\sum_{i=1}^n W_i \\left(Y_i^+ - \\overline{Y}^+\n\\right)\\right)\n&\\text{}\n\\\\[5pt]\n\n&\\text{Rewriting variance in terms of covariance}\n\\\\[5pt]\n\n&=\n\\text{Cov}\\left(\n\\sum_{i=1}^n W_i \\left(Y_i^+ - \\overline{Y}^+\\right),\n\\sum_{j=1}^n W_j \\left(Y_j^+ - \\overline{Y}^+\\right)\n\\right)\n&\\text{}\n\\\\[5pt]\n\n&=\n\\sum_{i=1}^n \\sum_{j=1}^n\n\\text{Cov}\\left(\nW_i \\left(Y_i^+ - \\overline{Y}^+\\right),\nW_j \\left(Y_j^+ - \\overline{Y}^+\\right)\n\\right)\n&\\text{}\n\\\\[5pt]\n\n&=\n\\sum_{i=1}^n \\sum_{j=1}^n\n\\text{Cov}\\left(W_i, W_j \\right)\n\\left(Y_i^+ - \\overline{Y}^+\\right)\n\\left(Y_j^+ - \\overline{Y}^+\\right)\n&\\text{}\n\\\\[5pt]\n\n&=\n\\sum_{i=1}^n\n\\mathbb{V}\\left(W_i^2\\right)\n\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n+\n\\sum_{i=1}^n \\sum_{j \\neq i}\n\\text{Cov}\\left(W_i, W_j \\right)\n\\left(Y_i^+ - \\overline{Y}^+\\right)\n\\left(Y_j^+ - \\overline{Y}^+\\right)\n&\\text{}\n\\\\[5pt]\n\n&\\text{Lemma 2}\n\\\\[5pt]\n\n&=\n\\sum_{i=1}^n\n\\mathbb{V}\\left(W_i\\right)\n\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n+\n\\sum_{i=1}^n \\sum_{j \\neq i}\n\\text{Cov}\\left(W_i, W_j \\right)\n\\left(Y_i^+ - \\overline{Y}^+\\right)\n\\left(Y_j^+ - \\overline{Y}^+\\right)\n&\\text{}\n\\\\[5pt]\n\n&\\text{Lemma 3}\n\\\\[5pt]\n\n&=\n\\sum_{i=1}^{n}\\left(\\frac{n_tn_c}{n^2}\\right)\n\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n- \\sum_{i=1}^{n}\\sum_{j \\neq i}\\left(\\frac{n_tn_c}{n^2(n-1)}\\right)\n\\left(Y_i^+ - \\overline{Y}^+\\right)\\left(Y_j^+ - \\overline{Y}^+\\right)\n\\\\[5pt]\n\n&=\n\\left(\\frac{n_tn_c}{n^2}\\right)\n\\sum_{i=1}^{n}\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n- \\left(\\frac{n_tn_c}{n^2(n-1)}\\right)\\sum_{i=1}^{n}\\sum_{j \\neq i}\n\\left(Y_i^+ - \\overline{Y}^+\\right)\\left(Y_j^+ - \\overline{Y}^+\\right)\n\\\\[5pt]\n\n&\\text{Lemma 4}\n\\\\[5pt]\n\n&=\n\\left(\\frac{n_tn_c}{n^2}\\right)\n\\sum_{i=1}^{n}\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n+ \\left(\\frac{n_tn_c}{n^2(n-1)}\\right)\\sum_{i=1}^{n}\n\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n\\\\[5pt]\n\n&=\n\\left(\\frac{n_tn_c}{n^2} + \\frac{n_tn_c}{n^2(n-1)}\\right)\n\\sum_{i=1}^{n}\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n&\\\\[5pt]\n\n&=\n\\frac{n_tn_c(n-1) + n_tn_c}{n^2(n-1)}\n\\sum_{i=1}^{n}\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n&\\\\[5pt]\n\n&=\n\\frac{nn_tn_c - n_tn_c + n_tn_c}{n^2(n-1)}\n\\sum_{i=1}^{n}\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n&\\\\[5pt]\n\n&=\n\\frac{n_tn_c}{n(n-1)}\n\\sum_{i=1}^{n}\\left(Y_i^+ - \\overline{Y}^+\\right)^2\n&\\\\[5pt]\n\n&\\text{Reverting to full notation and expanding square term}\n\\\\[5pt]\n\n&=\n\\frac{n_tn_c}{n(n-1)}\n\\sum_{i=1}^{n}\\left(\\frac{Y_i(1)}{n_t} + \\frac{Y_i(0)}{n_c}\n- \\frac{\\overline{Y}(1)}{n_t} - \\frac{\\overline{Y}(0)}{n_c}\\right)^2\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{n_tn_c}{n(n-1)}\n\\sum_{i=1}^{n}\\left(\n\\left(\\frac{Y_i(1)}{n_t} - \\frac{\\overline{Y}(1)}{n_t}\\right)\n+ \\left(\\frac{Y_i(0)}{n_c} - \\frac{\\overline{Y}(0)}{n_c}\\right)\n\\right)^2\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{n_tn_c}{n(n-1)}\n\\sum_{i=1}^{n}\\left(\n\\frac{1}{n_t}\\left(Y_i(1) - \\overline{Y}(1)\\right)\n+ \\frac{1}{n_c}\\left(Y_i(0) - \\overline{Y}(0)\\right)\n\\right)^2\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{n_tn_c}{n(n-1)}\\left[\n\\sum_{i=1}^{n}\\left(\n\\frac{1}{n_t^2}\\left(Y_i(1) - \\overline{Y}(1)\\right)^2\n+ \\frac{1}{n_c^2}\\left(Y_i(0) - \\overline{Y}(0)\\right)^2\n+ \\frac{2}{n_t n_c}\\left(Y_i(1) - \\overline{Y}(1)\\right)\\left(Y_i(0) - \\overline{Y}(0)\\right)\n\\right)\n\\right]\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{n_tn_c}{n(n-1)}\\left[\n\\frac{1}{n_t^2}\\sum_{i=1}^{n}\\left(Y_i(1) - \\overline{Y}(1)\\right)^2\n+ \\frac{1}{n_c^2}\\sum_{i=1}^{n}\\left(Y_i(0) - \\overline{Y}(0)\\right)^2\n+ \\frac{2}{n_t n_c}\\sum_{i=1}^{n}\\left(Y_i(1) - \\overline{Y}(1)\\right)\\left(Y_i(0) - \\overline{Y}(0)\\right)\n\\right]\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{n_c}{n n_t}\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(Y_i(1) - \\overline{Y}(1)\\right)^2\n+ \\frac{n_t}{n n_c}\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(Y_i(0) - \\overline{Y}(0)\\right)^2\n+ \\frac{2}{n}\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(Y_i(1) - \\overline{Y}(1)\\right)\\left(Y_i(0) - \\overline{Y}(0)\\right)\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{n_c}{n n_t}S_1^2\n+ \\frac{n_t}{n n_c}S_0^2\n+ \\frac{1}{n}2S_{0,1}\n&\\text{}\n\\\\[5pt]\n\n&\\text{Lemma 5}\n\\\\[5pt]\n\n&=\n\\frac{n_c}{n n_t}S_1^2\n+ \\frac{n_t}{n n_c}S_0^2\n+ \\frac{1}{n}\\left(S_1^2 + S_0^2 - S_{\\tau_i}^2\\right)\n&\\text{}\n\\\\[5pt]\n\n&=\n\\left(\\frac{n_c}{n n_t} + \\frac{1}{n}\\right)S_1^2\n+ \\left(\\frac{n_t}{n n_c} + \\frac{1}{n}\\right) S_0^2\n- \\frac{S_{\\tau_i}^2}{n}\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{n_c + n_t}{n n_t} S_1^2\n+ \\frac{n_t + n_c}{n n_c} S_0^2\n- \\frac{S_{\\tau_i}^2}{n}\n&\\text{}\n\\\\[5pt]\n\n&=\n\\frac{S_1^2}{n_t}\n+ \\frac{S_0^2}{n_c}\n- \\frac{S_{\\tau_i}^2}{n}\n&\\text{}\n\\\\[5pt]\n\\end{align}\\] $${#eq-var}\nThis is the sampling variance of \\(\\hat{\\tau}^{\\text{dm}}\\). It’s a theoretical quantity we cannot directly observe. However, we can observe treatment group means:\n\\[\n\\begin{align}\n\\overline{Y}_t = \\frac{1}{n_t}\\sum_{i=1}^n W_iY_i\n\\qquad\n\\overline{Y}_c = \\frac{1}{n_c}\\sum_{i=1}^n (1-W_i)Y_i\n\\end{align}\n\\] and treatment group variances:\n\\[\n\\begin{align}\ns_t^2 = \\frac{1}{n_t-1}\\sum_{i=1}^{n}W_i\\left(Y_i - \\overline{Y}_t\\right)^2\n\\qquad\ns_c^2 = \\frac{1}{n_c-1}\\sum_{i=1}^{n}(1-W_i)\\left(Y_i - \\overline{Y}_c\\right)^2.\n\\end{align}\n\\] It can be shown that the observed treatment group variances \\(s_t^2\\) and \\(s_c^2\\) are unbiased estimators of the sample variances \\(S_1^2\\) and \\(S_0^2\\) (see, for instance, Appendix A in Chapter 6 of Imbens and Rubin (2015)). The last term in ?eq-var, \\(S_{\\tau_i}^2\\), is the variance of unit-level treatment effects, which is impossible to observe.\nAs a result, the most widely used estimator in practice is: \\[\n\\hat{\\mathbb{V}}\n= \\frac{s_t^2}{n_t} + \\frac{s_c^2}{n_c}.\n\\] In our context, the main advantages of this estimator are:\n\nIf treatment effects are constant across units, then this is an unbiased estimator of the true sampling variance since in this case, \\(S^2_{\\tau_i} = 0\\).\nIf treatment effects are not constant, then this is a conservative estimator of the sampling variance (since \\(S_{\\tau_i}^2\\) is non-negative).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#standard-error",
    "href": "chapters/stats_of_online_experiments.html#standard-error",
    "title": "1  The stats of online experiments",
    "section": "1.6 Standard error",
    "text": "1.6 Standard error\nThe standard error of an estimator is simply the square root of its sampling variance. From ?eq-var we thus have:\n\\[\n\\widehat{SE}\n= \\sqrt{\\frac{s_t^2}{n_t} + \\frac{s_c^2}{n_c}}.\n\\tag{1.2}\\]\nBecause in online experiments sample sizes are large and treatment effects are usually small, it is sometimes convenient to assume equal sample sizes, so that the sample size for each variant is \\(n_t = n_c = n_v\\), and equal variances, so that \\(s_t^2 = s_c^2 = s^2\\). The common variance \\(s^2\\) is estimated by “pooling” the treatment group variances to create a degrees-of-freedom-weighted estimator of the form: \\[\ns^2 = \\frac{(n_t - 1) s_t^2 + (n_c - 1) s_c^2}{n_t + n_c - 2}.\n\\] Substituting in Equation 1.2 we then have: \\[\n\\widehat{SE}^{\\text{equal}}\n= \\sqrt{\\frac{s^2}{n_v} + \\frac{s^2}{n_v}}\n= \\sqrt{\\frac{2s^2}{n_v}}.\n\\tag{1.3}\\]\nFinally, for the purpose of experiment design it is sometimes useful to express the standard error in terms of the proportion of units allocated to the treatment group. Hence, instead of assuming equal sample sizes, we use \\(p\\) to denote that proportion and \\(n\\) to denote total sample size, while maintaining the assumption of equal variance. Again substituting in Equation 1.2 we can then write: \\[\n\\widehat{SE}^{\\text{prop}}\n= \\sqrt{\\frac{s^2}{pn} + \\frac{s^2}{(1-p)n}}\n= \\sqrt{\\frac{s^2}{np(1-p)}}.\n\\tag{1.4}\\]\nFor \\(p=0.5\\), this formulation is equivalent to Equation 1.3 as expected.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#qa",
    "href": "chapters/stats_of_online_experiments.html#qa",
    "title": "1  The stats of online experiments",
    "section": "1.7 Q&A",
    "text": "1.7 Q&A\n\n1.7.0.0.1 Question 1\nWhy do we need potential outcomes at all? Can’t we interpret the difference from a simple comparison of averages as the causal effect?\n\n\n1.7.0.0.2 Question 2\nWhy do randomised trials not require the excludability assumption in order to lead to unbiased results?\n\n\n1.7.0.0.3 Answer 1\nWhy potential outcomes? - Clarifies what precisely we are trying to estimate: average individual treatment effect - Makes explicit the assumptions we need to make to do so: - SUTVA: What we need is to be able to write Y-i = WY1 + (1-W)Y0. For this we need i) independence from other’s assignment, and ii) clearly defined meaning of Wi =1 and Wi = 0, because if they are not clearly defined then Y1/Y0 might not be stable. SUTVA handles both of these. - Randomisation: to make sure that EY0 for W=1 equals EY0 W=0 Material - ding2023first footnote 2 in chapter 4 and\n\nWhat is definition of causal effect in suggested comparison?\nWhat is source of randomisation?\nDiscuss textbook iid approach and why it’s not a good model for our purpose.\nShow that in practice, variance is the same\n\nIn the classic two-sample problem, observations in the treatment group {y1s} and control group {y0s} are assumed to be IID draws from two separate distributions. Treatment observations are assumed to be IID draws from a distribution with mean \\(\\mu_t\\) and variance \\(\\sigma_t^2\\) and similar for control, and the variance of the difference in means estimator is given by:\n\\[\n\\mathbb{V}(\\hat{\\tau}) = \\frac{\\sigma_t^2}{n_t} + \\frac{\\sigma_c^2}{n_c}.\n\\] That is, there is no third term for the variance of the individual-level potential outcomes.\nIn contrast, Rubin points out that for proper causal inference, {y1, y0} pairs are from the same distribution but we observe only one item of the pair.\nDifferences: - Sampling based vs randomisation based variation: makes sense given that in IID case we are assumed to sample from population, whereas in FS case we are assumed to have all units, but randomise which item of the PO pair is observed. - Hence: the variance in IID is taken over the randomness of the outcomes because uncertainty is sampling based, whereas in the potential outcomes framework, where potential outcomes are fixed, the variance is taken over the randomisation distribution. - As a result: there is no correlation between two groups in IID case (covar = 0) and hence no third term, whereas in FS case there is – why precisely? Because there is correlation between y1s and y2s – if there isn’t, then the third term vanishes. See ding derivation. However, ultimately it’s because there is heterogeneity in individual-level treatment effects. Why is that? Is that the same as PO correlation at individual level? - Weird, though, that Ding lemmas seem to be based on IID case!\n\n\n1.7.0.0.4 Answer 2\n…",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#references",
    "href": "chapters/stats_of_online_experiments.html#references",
    "title": "1  The stats of online experiments",
    "section": "1.8 References",
    "text": "1.8 References\n\n\n\n\nDing, Peng. 2023. “A First Course in Causal Inference.” https://arxiv.org/abs/2305.18793.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association 81 (396): 945–60.\n\n\nImbens, Guido W, and Donald B Rubin. 2015. Causal Inference in Statistics, Social, and Biomedical Sciences. Cambridge University Press.\n\n\nRubin, Donald B. 1980. “Randomization Analysis of Experimental Data: The Fisher Randomization Test Comment.” Journal of the American Statistical Association 75 (371): 591–93.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/stats_of_online_experiments.html#footnotes",
    "href": "chapters/stats_of_online_experiments.html#footnotes",
    "title": "1  The stats of online experiments",
    "section": "",
    "text": "Our \\(n\\) units are not a sample of a larger population that we hope to learn about but the entire population of units of interest. We thus use a finite sample rather than a super-population approach. For a discussion on the difference between these approaches see ?sec-experiment-setup.↩︎\nIn principle, the unit-level level causal effect can be any comparison between the potential outcomes, such as the difference \\(Y_i(1) - Y_i(0)\\) or the ratio \\(Y_i(1)/Y_i(0)\\). In online experiments, we usually focus on the difference.↩︎\nHolland (1986) discusses two solutions to the Fundamental Problem: one is the statistical solution, which relies on estimating average treatment effects across a large population of units while the other is the scientific solution, which uses homogeneity or invariance assumptions. The scientific solution works as follows: say we have one measurement of a units outcome under treatment from today and another measurement of their outcome under control from yesterday. If we are prepared to assume that control measurements are homogenous and invariant to time – that yesterday’s control measurement equals the control measurement we would have taken today – then we can calculate the individual level causal effect by comparing the two measurements taken at different points in time. Our assumption is untestable, of course, but in lab experiments it is sometimes possible to make a strong case that it is plausible. It is also the approach we informally use in daily life, whenever we conclude that taking Paracetamol helps against headaches or that going to sleep early makes us feel better the next morning.↩︎\nI have taken a slight shortcut here by treating experiments as being synonymous with the statistical solution (see previous footnote) even though observational studies can serve the same purpose (albeit with additional assumptions). I do this because my focus here is on experiments. See, for instance, Imbens and Rubin (2015) for an extensive discussion of experimental and observational approaches.↩︎\nTo denote the sum over all units in a given treatment group \\(w\\) I use \\(\\sum_{W_i=w}\\) as a shorthand for \\(\\sum_{i:W_i=w}\\) to keep the notation compact.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The stats of online experiments</span>"
    ]
  },
  {
    "objectID": "chapters/power.html",
    "href": "chapters/power.html",
    "title": "2  Power",
    "section": "",
    "text": "2.1 Required sample size\nThe required sample size is determined by four factors:\nIn the context of online experiments, we usually fix the significance level and desired power, calculate the estimate the outcome variable’s standard deviation from historical data, fix the minimal detectable effect, and then calculate required sample size. Given these inputs, and assuming equal sample sizes and variances for treatment and control variants, that required sample size per variant is given by: \\[\n\\begin{align}\nn_v = 2(z_{\\alpha/2} + z_{1 - \\beta})^2\\frac{s^2}{\\Delta^2},\n\\end{align}\n\\tag{2.1}\\]\nThe power formula can be intimidating and confusing, all the more so since there are different and sometimes incorrect versions presented in different articles. Here, I want to derive the formula to demystify it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#required-sample-size",
    "href": "chapters/power.html#required-sample-size",
    "title": "2  Power",
    "section": "",
    "text": "The probability of making a Type I error, denoted by \\(\\alpha\\), corresponds to the significance level of the test and has an associated with the upper-tail critical value \\(z_{\\alpha/2}\\) in a two-sided test.\nThe probability of making a Type II error, denoted by \\(\\beta\\), determines the power of the test, \\(1-\\beta\\), and has associated critical value given by \\(z_{1 - \\beta}\\).\nThe standard deviation of the outcome variable, \\(s\\).\nThe minimal detectable effect size, \\(\\Delta\\).\n\n\n\n\n2.1.1 Bloom approach\n\nBloom (1995) introduces the concept of MDE to measure and compare power and provides a useful heuristic approach to perform sample size calculations.\n\n\n\n\ntitle\n\n\n\nIn above figure, which is taken from Duflo, Glennerster, and Kremer (2007), the left hand curve is the sampling distribution of the estimator under \\(H_0\\), where the true effect size is 0, and the right hand curve its sampling distribution under \\(H_A\\), where the true effect size is \\(\\Delta\\). Because in online experiments sample sizes are usually large, these sampling distributions are well approximated by a standard normal distribution.\nFor a given significance level \\(\\alpha\\), the critical value \\(z_{a/2}\\) in a two-sided test is the point in the \\(H_0\\) distribution that has \\(\\alpha/2\\) of the probability mass to its right. We can also think of that critical critical value as a distance between the center of the distribution and the critical value.\nFor a given level of power, \\(1-\\beta\\), the critical value \\(z_{1-\\beta}\\) is the point in the \\(H_A\\) distribution that has \\(1-\\beta\\) of the probability mass to its right. It, too, can be thought of as a distance.\n\nlh curve …this is sampling dist of tee, know shape from sampling theory reject h0 if value larger than za rhs is sampling distr under ha what is zk? now derive bloom formula…\nBloom (1995) introduces the notion of the MDE as a useful way to quantify power. In the process, he also uses an intuitive way to derive the power formula based on an illustration of a typical hypothesis-testing scenario.\n\n\n\n\n\n\nFigure 2.1: Source: Duflo, Glennerster, and Kremer (2007), based on Bloom (1995).\n\n\n\nLet’s start by understanding Figure 2.1, which visualises the setup of a one-sided hypothesis test where the true effect equals 0 under the null hypothesis and some positive constant \\(\\te\\) under the alternative hypothesis. Note that the curves are not the standard normal distribution, but the sampling distribution of our estimator \\(\\tee\\). This means that the standard deviation of the curves is given by the standard error of \\(\\tee\\), which is \\(\\se\\). Under the assumption of a homogenous treatment effect, the standard error is identical under \\(\\hn\\) and \\(\\ha\\), which is why the two curves have the same shape (see ?sec-experiment-stats for details).\nthe distribution will be the same under both the null and the alternative hypothesis, with the center of each distribution given by our hypothesised value of \\(\\te\\) – zero under \\(\\hn\\) and a positive constant under \\(\\ha\\).\nWe reject \\(\\hn\\) if \\(\\tee\\) is to the right of the critical value \\(\\za\\). Also, for a given level of power \\(\\beta\\),\nLargely based on (duflo2007randomization?)\nPower basics\n\\[\nn = \\frac{(f(\\alpha) + f(\\beta))}{\\text{Sample allocation}}\\frac{\\sigma}{\\delta}\n\\]\n\nIn the simplest possible, we randomly draw a sample of size \\(N\\) from an identical population, so that our observations can be assumed to be i.i.d, and we allocate a fraction \\(P\\) of our sample to treatment. We can then estiamte the treatment effect using the OLS regression\n\n\\[ y = \\alpha + \\beta T + \\epsilon\\]\n\nwhere the standard error of \\(\\beta\\) is given by \\(\\sqrt{\\frac{1}{P(1-P)}\\frac{\\sigma^2}{N}}\\).\nstd error derivation (from standard variance result of two independent samples, using population fractions):\n\n\\[\nstd = \\sqrt{\\frac{\\sigma^2}{N_t} + \\frac{\\sigma^2}{N_c}} = \\sqrt{\\frac{\\sigma^2}{PN} + \\frac{\\sigma^2}{(1-P)N}} = ... = \\sqrt{\\frac{1}{P(1-P)}\\frac{\\sigma^2}{N}}\n\\]\n\nThe distribution on the left hand side below shows the distribution of our effect size estimator \\(\\hat{\\beta}\\) if the null hypothesis is true.\nWe reject the null hypothesis if the estimated effect size is larger than the critical value \\(t_{\\alpha}\\), determined by the significance level \\(\\alpha\\). Hence, for this to happen we need \\(\\hat{\\beta} &gt; t_{\\alpha} * SE(\\hat{\\beta})\\) (follows from rearranging the t-test formula).\nOn the right is the distribution of \\(\\hat{\\beta}\\) if the true effect size is \\(\\beta\\).\nThe power of the test for a true effect size of \\(\\beta\\) is the area under this curve that falls to the right of \\(t_{\\alpha}\\). This is the probability that we reject the null hypothesis given that it is false.\nHence, to attain a power of \\(\\kappa\\) it must be that \\(\\beta &gt; (t_a + t_{1-\\kappa}) * SE(\\hat{\\beta})\\), where \\(t_{1-\\kappa}\\) is the value from a t-distribution that has \\(1-\\kappa\\) of its probability mass to the left (for \\(\\kappa = 0.8\\), \\(t_{1-\\kappa} = 0.84\\)).\nThis means that the minimum detectable effect (\\(\\delta\\)) is given by:\n\n\\[ \\delta = (t_a + tq_{1-\\kappa}) * \\sqrt{\\frac{1}{P(1-P)}\\frac{\\sigma^2}{N}} \\]\n\nRearranding for the minimum required sample size we get:\n\n\\[ N =  \\frac{(t_a + t_{1-\\kappa})^2}{P(1-P)}\\left(\\frac{\\sigma}{\\delta}\\right)^2 \\]\n\nSo that the required sample size is inversely proportional to the minimal effect size we wish to detect. This makes sense, it means that the smaller an effect we want to detect, the larger the samle size we need. In particular, given that \\(N \\propto \\delta^{-2}\\), to detect an effect of half the size we need a sample four times the size.\nSE(\\(\\beta\\)) also includes measurement error, so this is also a determinant of power.\n\n\n\n2.1.2 Two-equations approach\n\n\n2.1.3 First-principles approach\nPower is the probability that we reject the null hypothesis if there exists a true effect of size \\(\\Delta\\).\nWe thus have: \\[\n\\begin{align}\n&H_0: \\tau = 0 \\\\[5 pt]\n&H_A: \\tau = \\Delta.\n\\end{align}\n\\]\nWe test the null hypothesis by constructing the test statistic \\[\nZ =\n\\frac{\\hat{\\tau}^{\\text{dm}}}\n{\\widehat{SE}},\n\\]\nand reject \\(H_0\\) if if falls into the rejection region beyond the critical value \\(z_{\\alpha/2}\\). Because the standard normal distribution is symmetric, for a two-sided test we thus reject \\(Z\\) if $$ \\[\\begin{align}\n|Z| &&gt; z_{\\alpha/2} \\\\[5pt]\n\n\\left|\\frac{\\hat{\\tau}^{\\text{dm}}}{\\widehat{SE}}\\right|\n&&gt; z_{\\alpha/2} \\\\[5pt]\n\n\\left|\\hat{\\tau}^{\\text{dm}}\\right|\n&&gt; z_{\\alpha/2}\\widehat{SE}.\n\\end{align}\\] $$\nThe power \\(1-\\beta\\) of the test given that \\(\\tau = \\Delta\\) is the probability that the test statistic \\(Z\\) falls into the rejection region, which is: \\[\n1 - \\beta = P\\left[\n\\left|\\hat{\\tau}^{\\text{dm}}\\right|\n&gt; z_{\\alpha/2}\\widehat{SE}\n\\&gt;\\middle|\\&gt; H_A\n\\right].\n\\]\nThe test statistic falling into the lower or upper rejection region are mutually exclusive events, so the above is equal to \\[\n1 - \\beta\n= P\\left[\n\\hat{\\tau}^{\\text{dm}}\n&gt; z_{\\alpha/2}\\widehat{SE}\\&gt;\\middle|\\&gt; H_A\n\\right]\n+ P\\left[\n\\hat{\\tau}^{\\text{dm}}\n&lt; -z_{\\alpha/2}\\widehat{SE}\\&gt;\\middle|\\&gt; H_A\n\\right]\n\\]\nWe can calculate these probabilities by standardising, which gives us:\n$$ \\[\\begin{align}\n1 - \\beta\n&= P\\left[\n\\frac{\\hat{\\tau}^{\\text{dm}} - \\Delta}{\\widehat{SE}}\n&gt;\n\\frac{z_{\\alpha/2}\\widehat{SE} - \\Delta}{\\widehat{SE}}\n\\right]\n\n+ P\\left[\n\\frac{\\hat{\\tau}^{\\text{dm}} - \\Delta}{\\widehat{SE}}\n&lt;\n\\frac{-z_{\\alpha/2}\\widehat{SE} - \\Delta}{\\widehat{SE}}\n\\right]\n\n\\\\[5pt]\n\n&=\nP\\left[Z &gt; \\frac{z_{\\alpha/2}\\widehat{SE} - \\Delta}{\\widehat{SE}}\n\\right]\n\n+ P\\left[Z &lt; \\frac{-z_{\\alpha/2}\\widehat{SE} - \\Delta}{\\widehat{SE}}\n\\right]\n\n\\\\[5pt]\n\n&=\nP\\left[Z &gt; z_{\\alpha/2} - \\frac{\\Delta}{\\widehat{SE}}\n\\right]\n\n+ P\\left[Z &lt; - z_{\\alpha/2} - \\frac{\\Delta}{\\widehat{SE}}\n\\right].\n\\end{align}\\] $$\nUsing the standard normal CDF, \\(\\Phi(z)\\), we get:\n\\[\n\\begin{align}\n1 - \\beta\n=1 - \\Phi\\left(z_{\\alpha/2} - \\frac{\\Delta}{\\widehat{SE}}\\right)\n+ \\Phi\\left(- z_{\\alpha/2} - \\frac{\\Delta}{\\widehat{SE}}\\right).\n\\end{align}\n\\]\nThe probability that we reject the null hypothesis for the wrong reason, because the test statistic falls below the lower critical value for a true positive effect or above the upper critical value for a true negative effect – sometimes called a Type III error –, is very small. Hence, as the true effect size deviates from zero, one of the two terms in the expression above becomes vanishingly small and can be ignored. For the rest of this chapter, I assume we have a true positive effect and omit the second of the two terms above. We thus have:\n\\[\n\\begin{align}\n1 - \\beta\n=\n1 - \\Phi\\left(z_{\\alpha/2} - \\frac{\\Delta}{\\widehat{SE}}\\right)\n\\end{align}\n\\]\nUsing the symmetry of the standard normal distribution, which implies that \\(1 - \\Phi(k) = \\Phi(-k)\\), we can simplify this to\n\\[\n\\begin{align}\n1 - \\beta\n=\n\\Phi\\left(\\frac{\\Delta}{\\widehat{SE}} - z_{\\alpha/2}\\right).\n\\end{align}\n\\tag{2.2}\\]\nNext, remember that \\(\\Phi(z)\\) takes z-values and returns probabilities (the probability that a standard normal variable is less than a given z value), so its inverse, \\(\\Phi^{-1}(p)\\), takes probabilities and returns z-values (the \\(z\\) value with \\(p\\) probability mass to its left). Hence, \\(\\Phi^{-1}(1-\\beta)\\) refers to the upper-tail critical value of the standard normal distribution that has \\(1-\\beta\\) probability mass to its right, and which we defined above as \\(z_{1-\\beta}\\). Using this, we get:\n$$ \\[\\begin{align}\n\n\\Phi^{-1}(1 - \\beta)\n&=\n\\Phi^{-1}\\left(\n\\Phi\\left(\\frac{\\Delta}{\\widehat{SE}} - z_{\\alpha/2}\\right)\n\\right) \\\\[5pt]\n\nz_{1-\\beta}\n&=\n\\frac{\\Delta}{\\widehat{SE}} - z_{\\alpha/2} \\\\[5pt]\n\n\\Delta\n&= \\widehat{SE}\\left(z_{\\alpha/2} + z_{1-\\beta}\\right).\n\\end{align}\\] $${#eq-mde}\nThis last line is in itself useful because it shows how the MDE is determined by the standard error and our choice of Type I and Type II probabilities.\nDepending on the context, we can plug in any of the standard error versions we defined earlier. To arrive at the above version, we use Equation 1.3, which gives us:\n$$ \\[\\begin{align}\n\n\\Delta\n&= \\widehat{SE}\\left(z_{\\alpha/2} + z_{1-\\beta}\\right) \\\\[5pt]\n\n\\Delta\n&= \\sqrt{\\frac{2s^2}{n_v}}\\left(z_{\\alpha/2} + z_{1-\\beta}\\right) \\\\[5pt]\n\n\\Delta^2\n&= \\frac{2s^2}{n_v}\\left(z_{\\alpha/2} + z_{1-\\beta}\\right)^2 \\\\[5pt]\n\nn_v\n&= 2\\left(z_{\\alpha/2} + z_{1-\\beta}\\right)^2\\frac{s^2}{\\Delta^2}\n\\end{align}\\] $$\nIf, instead of using Equation 1.3 we use the standard error expressed in terms of sample proportions from Equation 1.4, we get: \\[\n\\begin{align}\nn &= \\frac{(z_{\\alpha/2} + z_{1-\\beta})^2}{p(1-p)} \\frac{s^2}{\\Delta^2},\n\\end{align}\n\\] where the left-hand side, \\(n\\) now refers to the total sample size in the experiment rather than the sample size per variant.\nFinally, if we do not assume equal variance then we have: $$ \\[\\begin{align}\n\n\\Delta\n&= \\widehat{SE}\\left(z_{\\alpha/2} + z_{1-\\beta}\\right) \\\\[5pt]\n\n\\Delta\n&= = \\sqrt{\\frac{s_t^2}{n_t} + \\frac{s_c^2}{n_c}}\\left(z_{\\alpha/2} + z_{1-\\beta}\\right) \\\\[5pt]\n\n\\Delta^2\n&= \\frac{s_t^2}{n_t} + \\frac{s_c^2}{n_c}\\left(z_{\\alpha/2} + z_{1-\\beta}\\right)^2 \\\\[5pt]\n\nn_v\n&= 2\\left(z_{\\alpha/2} + z_{1-\\beta}\\right)^2\\frac{s^2}{\\Delta^2}\n\\end{align}\\] $$\n\n\n2.1.4 Starting from Type I and Type II error conditions\nUse List, Sadoff, and Wagner (2011)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#relative-effects",
    "href": "chapters/power.html#relative-effects",
    "title": "2  Power",
    "section": "2.2 Relative effects",
    "text": "2.2 Relative effects\nSee zhou2023all ## Correlated data\nSee zhou2023all, hesterberg2024power",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#effective-sample-size-of-test",
    "href": "chapters/power.html#effective-sample-size-of-test",
    "title": "2  Power",
    "section": "2.3 Effective sample size of test",
    "text": "2.3 Effective sample size of test\n\n2.3.1 Effective Sample Size in a Two-Sample Test (Equal Variances Assumed)\nWhen comparing two groups — treatment (size \\(N_T\\)) and control (size \\(N_C\\)) — and assuming equal variances, the effective sample size for estimating the variance of the difference in means is given by the harmonic mean:\n\\[\nN_{\\text{eff}} = \\frac{1}{\\frac{1}{N_T} + \\frac{1}{N_C}}\n\\]\nThis arises because the variance of the difference in means is:\n\\[\n\\text{Var}(\\bar{Y}_T - \\bar{Y}_C) = \\sigma^2 \\left( \\frac{1}{N_T} + \\frac{1}{N_C} \\right)\n\\]\nIf we treat this as equivalent to the variance under a single sample of size \\(N_{\\text{eff}}\\), then:\n\\[\n\\text{Var}(\\text{difference}) = \\frac{2\\sigma^2}{N_{\\text{eff}}}\n\\]\nMatching both sides gives the harmonic mean as the effective sample size.\n\n\n2.3.2 Interpretation\n\nThe harmonic mean weights smaller group sizes more heavily.\nIt reflects the information content for estimating differences — imbalanced samples reduce power.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#rule-of-thumb",
    "href": "chapters/power.html#rule-of-thumb",
    "title": "2  Power",
    "section": "2.4 Rule of thumb",
    "text": "2.4 Rule of thumb\nBlog post on 16 or 32 power confusion: - Reliably looking posts who get it wrong: (https://towardsdatascience.com/probing-into-minimum-sample-size-formula-derivation-and-usage-8db9a556280b — starts with the wrong std error with N for total instead of variant sample size), there is also Kohavi book or paper that gets it wrong\n\nThere is another way to express the variance, which has led to massive confusion.\nI’m pretty sure its the 1/N vs 1/(N/2) error that accounts for the wrong result, and nobody seems to derive this from first principles to check.\nIs original wrong? Check in book – access through WBS.\n\nPopular experiment textbooks and countless sources on the internet often refer to the rule-of thumb for the total sample size calculation that is given by:\n\\[\n\\N \\approx \\frac{32\\vpe}{\\tee^2}.\n\\]\nUsing formula ?eq-sample-size we can see that the rule of thumb straightforwardly results from using the default parameters.\nAssuming equal sample size, so that \\(P=0.5\\) gives us\n\\[\nN = 4 (z_{1 - \\beta} + z_{\\alpha/2})^2\\left(\\frac{\\sev}{\\te}\\right)^2.\n\\]\nSetting the false positive rate to 5% and the false negative rate at 20% for a two-sided hypothesis test, as we commonly do, we get\n\\[\n\\begin{align}\nN &= 4 (0.84 + 1.96)^2\\left(\\frac{\\sev}{\\te}\\right)^2 \\\\\n&\\approx \\left(\\frac{32\\sev}{\\te}\\right)^2\n\\end{align}\n\\]\nGive also per variant, as this is more useful to calculate sample size for experiments with n arms.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#how-to-choose-key-parameters",
    "href": "chapters/power.html#how-to-choose-key-parameters",
    "title": "2  Power",
    "section": "2.5 How to choose key parameters",
    "text": "2.5 How to choose key parameters\n\n2.5.1 MDE\n\nWhat are you balancing here? The size of the effect you are able to identify and the time it takes to do it.\nAll else equal, the smaller a change you want to be able to detect, the longer it will take for the experiment to run because you need more sample size.\nThe relevant question to ask here is “what counts as a practically relevant change?”\nTo answer that, consider:\n\nMaturity of service (the more mature, the smaller a change can be expected)\nSize of service (the larger, the smaller a change still generates a lot of revenue)\nCost of change that need ot be covered\n\nCost of fully building out feature for launch (can be 0 when fully built out for experiment or high if we use painted door)\nCost of maintaining new code (new code has higher bugs, may increase code complexity and maintenance)\nOther costs: e.g. does CPU utilization increase?\n\n\n\n\n\n2.5.2 Significance level\n\nWhat are you balancing here? The probabilities of making a type I and type II error.\nThe higher significance level, the less likely we are to implement useless features (to make a Type I error) but the more likely we are to no implement useful features (to make a Type II error).\nHence, gotta balance cost of implementing useless feature and cost of not implementing useful feature.\nThings that play into this:\n\nHow long will feature be in effect (less long lowers risk of implementing)?\nHow widely will it be deployed (less widely lowers risk of implementing)?\nHow many users will see it / where in the funnel is it (later in funnel lowers risk of implementation)\n\nWhat to do in practice:\n\nStart from baseline values (\\(alpha = 0.05\\))\nAdjust depending on balance of risks\n\n\n\n\n2.5.3 Power\n\nWhat are you balancing here? The risk of making a Type II error and the time you have to wait for your results.\nAll else equal, the higher a level or power you want, the longer you’ll have to run the experiment to accumulate the requried sample size.\nFactors to consider:\n\nHow costly is it to not implement a useful feature.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#measuring-power",
    "href": "chapters/power.html#measuring-power",
    "title": "2  Power",
    "section": "2.6 Measuring power",
    "text": "2.6 Measuring power\n\nCohen (1977) proposes estimated effect size / standard deviation of outcome. This is useful to compare effects across studies and domains.\nBloom (1985) proposes MDE, useful for within study/domain comparisons. More directly interpretable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#what-determines-power",
    "href": "chapters/power.html#what-determines-power",
    "title": "2  Power",
    "section": "2.7 What determines power",
    "text": "2.7 What determines power\n\nSignificance level\nEffect size\nStandard error\n\nSample size\nVariant allocation proportion\nMetric variance",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#how-to-increase-power",
    "href": "chapters/power.html#how-to-increase-power",
    "title": "2  Power",
    "section": "2.8 How to increase power",
    "text": "2.8 How to increase power\n\nfor framing on how to increase power, Integrate larsen2023statistical section 2\nPower can be increased trivially by lowering the significance level, which we often don’t want to do, or by increasing sample size, which we’re often trying to avoid.\nIncrease effect size\n\nEnsure that only users who are exposed to the change are in the data to avoid dilution of the effect\n\nOptimally allocate variance proportions\n\nUsually equal for highest power\nShow why with many treatment variants, higher share in control is better\n\nReduce metric variance\n\nChoose metric with low variance\n\nIndicator variables\nAvoid count variables which have have increasing variance as experiment duration progresses\n\nUse variance reduction technique\nTrim outliers\nOnly include triggered users\n\nUse a one-sided test\nEffect of one-sided testing on required sample size.\nIn general: \\[\n  N =  \\frac{(t_a + t_{1-\\kappa})^2}{P(1-P)}\\left(\\frac{\\sigma}{\\delta}\\right)^2\n  \\]\nFor \\(\\alpha = 0.05\\), we have \\(t_{\\alpha}^{ts} = 1.96\\) and \\(t_{\\alpha}^{os} = 1.65\\), while for \\(\\kappa = 0.8\\) we have \\(t_{1 - \\kappa} = 0.84\\). Hence: \\[\n  \\frac{N^{os}}{N^{ts}} = \\frac{ (1.64 + 0.84)^2}{(1.96 + 0.84)^2} = \\frac{6.2}{7.84} = 0.79\n  \\]\nHence, for given levels of power and significance, a one-sided test requires about 21 percent fewer observations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#problems-with-low-power",
    "href": "chapters/power.html#problems-with-low-power",
    "title": "2  Power",
    "section": "2.9 Problems with low power",
    "text": "2.9 Problems with low power\n\nTruth inflation: underpowered studies only find a significant effect it the effect size is larger than the true effect size, leading to inflated claims of effect sizes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#power-in-online-experiments",
    "href": "chapters/power.html#power-in-online-experiments",
    "title": "2  Power",
    "section": "2.10 Power in online experiments",
    "text": "2.10 Power in online experiments\n\nKohavi et al. (2014) point out (in rule 7) that while general advice suggets that the CLT provides a good approximation for n larger than 30, the large skew in online metrics often requires many moer users. They recomment 355 * (skewness coefficient)^2.\n\nThe theory for power calculation was developed for metrics with fixed values such as hight or weight.\n\n(During an experiment, the treatment will still change the metric values, but in practice we often make the sensible assumption that the treatment effect will be small, so that the variance between treatment and control are the same. This, in turn, then justifies use of pre-experiment data under the assumption that pre-experiment and experiment data will be very similar.)\nIn online experiments, we often experiment with metrics that are only defined for a specific period of time (e.g. conversion during a 1-month period starting on 15 March 2024).\nThis makes power calculation more complicated.\nWhen calculating required sample size (and/or experiment duration) for an experiment we want to make sure that a Z-test performed at the end of the experiment with the required number of unique units has a certain pre-specified level of power.\nTo calculate that required sample size we input a baseline metric value and the metric standard deviation.\nWith metrics defined only for specific periods, how to calculate these two values is not straightforward.\nLet’s see what we generally do when a metric value is fixed.\nActually, writing this and thinking of an example for the above makes me think that this might be an issue inherent to causal inference analysis.\nAn example where we don’t have the problem is if we want to compare the height of Londoners to the height of Berliners. Here, we’d do the following:\n\nDraw a random sample of Londoners and measure mean and variance of their heights.\nDo the same for a random sample of Berliners.\nPerform a Z-test and calculate its power.\n\nWriting the above makes me realise that the issue is inherent in ex-ante power calculations:\n\nEven in the Londoners and Berliners height example above we’d run into the same problem if we wanted to calculate, before taking any samples, how many samples we’d have to take to have our Z-test be adequately powered.\n\nThe problem arises once we rearrange the power formula from\n\n\\[\n1 - \\beta = f(\\sigma^2, \\delta, P, z_\\alpha, z_{1-\\beta})\n\\] to \\[\nN = \\frac{z_\\alpha + z_{1-\\beta}}{P(1-P)}\\frac{\\sigma^2}{\\delta^2}\n\\] - Because we would use the first version at the time we perform the analysis when we have all the required inputs, whereas we perform the second one before the analysis when we have to estimate \\(\\sigma\\) and \\(\\delta\\).\n\nThe core of the problem is that for many online experiment metrics baseline metric mean and variance change depending on (1) the size of the sample they are calculated from and (2) the period of time and period location they are calculated based on.\n\nis always the case, even in the Londoners and Berliners example above. It’s inherent in performing power calculations.\n\n\nis an additional complications in many online experiments. The two components are period length and period location (i.e. do we measure period of length \\(t\\) in January of September).\n\nOutside of periods that are non-representative because of seasonality reasons, ignoring period location should usually not be a big problem.\nHowever, period duration might make a difference.\nSo, the core problem is that in online experiments, in addition to approximating the sample we calculate metrics based on we also have to approximate the time period.\nHow big a difference does calculating means and stds based on different time periods make? The difference can be substantial. The below table shows means and std for order visit conversion for a set of UK users based on different period lengths.\n\n![[order-conversion-visit-different-periods.png|300]]\n\nRequired sample size is directly proportional to the sample variance, which means that using the variance based on one week instead of 1 month of data would increase required sample size by a factor of \\(\\frac{0.36^2}{0.31^2} = 1.35\\).\nHow hard is it to decently estimate an appropriate time-period? There are two parts we have to estimate: required number of unique units, and how long it takes to gather data from these many units.\nThe required number of units is determined by:\n\nMetric value mean (for experiment-period-length long period)\nMetric value variance (for experiment-period-length long period)\n\nTo amount of time it takes us to gather data for the required number of units depends on traffic to the precise point of the user-funnel/app where the bucketing for the experiment takes place.\n\nSolution: - To ensure that analysis is correctly powered, calculate power every day and stop once adequately powered. - If you want ex-ante guidance, use sensible approximations.\n\nFirst best: to know when analysis is sufficiently powered, calculate power daily and stop experiment when required level of power reached. This ensures that we have both (1) sample we use for analysis and (2) period used for analysis.\nSecond best, if we performed power using data from, say, the first 7-days of the experiment period, we would have a subset of (1) and could intelligently estimate (2) because the observed traffic would take into account the bucketing point and we could estimate future traffic based on it (e.g. we can estimate unique user visits based on unique visits during first week, with different traffic being the result of different bucketing points, but we can estimate path for all bucketing points).\nThird best, if we want to perform power calculation before the experiment starts (i.e. because we want to provide duration estimate during experiment config), we could use data from recent history (e.g. calculated monthly), and calculated separately for each metric, market, and based on other relevant dimensions such as different time period. Though, here, taking into account bucketing points might be challenging and hard to scale. So think about good approximations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#best-practices",
    "href": "chapters/power.html#best-practices",
    "title": "2  Power",
    "section": "2.11 Best practices",
    "text": "2.11 Best practices\n\nWhen aiming to estimate a precise effect size rather than just being interested in statistical significance, use assurance instead of power: instead of choosing a sample size to attain a given level of power, choose sample size so that confidence interval will be suitably narrow 99 percent of the time (Sample-Size Planning for More Accurate Statistical Power: A Method Adjusting Sample Effect Sizes for Publication Bias and Uncertainty and Understanding the new statistics.)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#experiment-duration",
    "href": "chapters/power.html#experiment-duration",
    "title": "2  Power",
    "section": "2.12 Experiment duration",
    "text": "2.12 Experiment duration\n\nWe usually care about power because it determines experiment runtime.\nThere we walk about how to translate required N into runtime.\nSimon Johnson – Four Customer Characteristics That Should Change Your Experiment Runtime",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#useful-resources",
    "href": "chapters/power.html#useful-resources",
    "title": "2  Power",
    "section": "2.13 Useful resources",
    "text": "2.13 Useful resources\n\nLarsen et al. (2023) for general overview\nZhou, Lu, and Shallah (2023) for comprehensive overview of how to calculate power\nBojinov, Simchi-Levi, and Zhao (2023), section 5, for simulation results for switchbacks and generally good approach to simulation to emulate\nReich et al. (2012) power calcs for cluster-randomised experiments\nPower Analysis for Experiments with Clustered Data, Ratio Metrics, and Regression for Covariate Adjustment\nStatsig sample size calculation formula",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  },
  {
    "objectID": "chapters/power.html#qa",
    "href": "chapters/power.html#qa",
    "title": "2  Power",
    "section": "2.14 Q&A",
    "text": "2.14 Q&A\nQuestions:\n\nLonger experiment duration generally increases power. Can you think of a scenario where this is not the case?\nAn online shopping site ranks products according to their average rating. Why might this be suboptimal? What could the site do instead?\n\nAnswers:\n\nWhen using a cumulative metric such as number of likes, the variance of which will increase the longer the experiment runs, which will increase the standard error of our treatment effect estimate and lower our power. Remember that \\(SE(\\hat{\\tau}) = \\sqrt{\\frac{1}{P(1-P)}\\frac{\\sigma^2}{N}}\\). So, whether this happens depends on what happens to \\(\\frac{\\sigma^2}{N}\\), as experiment duration increases. A decrease in power is plausible – likely, even! – because \\(N\\) will increase in a concave fashion over the course of the experiment duration (some users keep coming back), while \\(\\sigma^2\\) is likely to grow faster than linearly, which causes the ratio to increase and power to decrease.\nThe approach is suboptimal because products with few ratings will have much more variance than products with many ratings, and their average rating is thus less reliable. The problem is akin to small US states having the highest and lowest rates of kidney cancer, or small schools having highest and lowest average pupil performance. Fundamentally, it’s a problem of low power – the sample size is too low to reliably detect a true effect. The solution is to use a shrinkage method: use a weighted average of the product average rating and some global product rating, with the weight of the product average rating being proportional to the number of ratings. This way, products with few ratings will be average, while products with many ratings will reflect their own rating.\n\n\n\n\n\nBloom, Howard S. 1995. “Minimum Detectable Effects: A Simple Way to Report the Statistical Power of Experimental Designs.” Evaluation Review 19 (5): 547–56.\n\n\nBojinov, Iavor, David Simchi-Levi, and Jinglong Zhao. 2023. “Design and Analysis of Switchback Experiments.” Management Science 69 (7): 3759–77.\n\n\nDuflo, Esther, Rachel Glennerster, and Michael Kremer. 2007. “Using Randomization in Development Economics Research: A Toolkit.” Handbook of Development Economics 4: 3895–3962.\n\n\nKohavi, Ron, Alex Deng, Roger Longbotham, and Ya Xu. 2014. “Seven Rules of Thumb for Web Site Experimenters.” In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1857–66.\n\n\nLarsen, Nicholas, Jonathan Stallrich, Srijan Sengupta, Alex Deng, Ron Kohavi, and Nathaniel T Stevens. 2023. “Statistical Challenges in Online Controlled Experiments: A Review of a/b Testing Methodology.” The American Statistician, 1–15.\n\n\nList, John A, Sally Sadoff, and Mathis Wagner. 2011. “So You Want to Run an Experiment, Now What? Some Simple Rules of Thumb for Optimal Experimental Design.” Experimental Economics 14: 439–57.\n\n\nReich, Nicholas G, Jessica A Myers, Daniel Obeng, Aaron M Milstone, and Trish M Perl. 2012. “Empirical Power and Sample Size Calculations for Cluster-Randomized and Cluster-Randomized Crossover Studies.” PloS One 7 (4): e35564.\n\n\nZhou, Jing, Jiannan Lu, and Anas Shallah. 2023. “All about Sample-Size Calculations for a/b Testing: Novel Extensions and Practical Guide.” arXiv Preprint arXiv:2305.16459.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Power</span>"
    ]
  }
]